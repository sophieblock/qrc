{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import pickle\n",
    "from sympy import symbols, MatrixSymbol, lambdify, Matrix, pprint\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sympy import symbols, MatrixSymbol, lambdify\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import scipy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "from qiskit.circuit.library import *\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import *\n",
    "import autograd\n",
    "from pennylane.wires import Wires\n",
    "import matplotlib.cm as cm\n",
    "import base64\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the first dataset\n",
    "a = f'/Users/sophieblock/QRCCapstone/QFIM_traced_trainable_global/analog_model_theorem23_take2/Nc_1/GHZ_state/Nr_1/trotter_step_1/1.0K/data.pickle'\n",
    "with open(a, 'rb') as f:\n",
    "    df_analog = pickle.load(f)\n",
    "print(df_analog['fixed_params0']['test0']['fixed_params'])\n",
    "print(df_analog['fixed_params0']['test0']['trainable_params'])\n",
    "df_analog['fixed_params0']['test0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_analog['fixed_params0']['test0']['qfim_eigvals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10/0/U1_6/reservoirs_2/trotter_step_9/bath_False/data_run_0.pickle\n",
    "# /Users/sophieblock/QRCCapstone/QFIM_traced_trainable_global/analog_model_new/Nc_2/GHZ_state/Nr_1/trotter_step_8/1.0K/data.pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import base64\n",
    "import jax.numpy as jnp\n",
    "\n",
    "gate_idx = 1\n",
    "trot = 1\n",
    "N_r = 1\n",
    "N_ctrl = 1\n",
    "\n",
    "# Load the first dataset\n",
    "a = f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_1/reservoirs_1/trotter_step_1/10_training_states/fixed_params0/test15/U1_2/data_run_2.pickle'\n",
    "with open(a, 'rb') as f:\n",
    "    df_analog = pickle.load(f)\n",
    "\n",
    "# Load the second dataset\n",
    "b =f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_1/reservoirs_1/trotter_step_1/10_training_states/fixed_params0/test15/U1_2/test15_.pickle'\n",
    "with open(b, 'rb') as f:\n",
    "    df_digital = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Load the third dataset\n",
    "c = f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_1/reservoirs_1/trotter_step_1/10_training_states/fixed_params0/test9/U1_2/test9_.pickle'\n",
    "with open(c, 'rb') as f:\n",
    "    df_c = pickle.load(f)\n",
    "\n",
    "# Load the fourth dataset\n",
    "d =f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_1/reservoirs_1/trotter_step_1/10_training_states/fixed_params0/test116/U1_2/test116_.pickle'\n",
    "with open(d, 'rb') as f:\n",
    "    df_d = pickle.load(f)\n",
    "\n",
    "# Extract data for the first dataset\n",
    "costs_analog = [float(i) for i in df_analog['costs'][0]]\n",
    "\n",
    "grads_per_epoch_analog = [np.array(i) for i in df_analog['grads_per_epoch'][0]]\n",
    "testing_results_analog = df_analog['testing_results'][0]\n",
    "fidelity_analog = 1 - df_analog['avg_fidelity'][0]\n",
    "opt_lr_analog = df_analog['opt_lr'][0]\n",
    "\n",
    "# Extract data for the second dataset\n",
    "costs_digital = [float(i) for i in df_digital['costs'][0]]\n",
    "grads_per_epoch_digital = [np.array(i) for i in df_digital['grads_per_epoch'][0]]\n",
    "testing_results_digital = df_digital['testing_results'][0]\n",
    "fidelity_digital = 1 - df_digital['avg_fidelity'][0]\n",
    "opt_lr_digital = df_digital['opt_lr'][0]\n",
    "\n",
    "# Extract data for the third dataset\n",
    "costs_c = [float(i) for i in df_c['costs'][0]]\n",
    "grads_per_epoch_c = [np.array(i) for i in df_c['grads_per_epoch'][0]]\n",
    "testing_results_c = df_c['testing_results'][0]\n",
    "fidelity_c = 1 - df_c['avg_fidelity'][0]\n",
    "opt_lr_c = df_c['opt_lr'][0]\n",
    "print(f\"data_run_3__.pickle params: {df_c['init_params'][0]}\")\n",
    "print(f\"ts: {df_c['training_states'][0][0]}\")\n",
    "# # Extract data for the fourth dataset\n",
    "costs_d = [float(i) for i in df_d['costs'][0]]\n",
    "grads_per_epoch_d = [np.array(i) for i in df_d['grads_per_epoch'][0]]\n",
    "testing_results_d = df_d['testing_results'][0]\n",
    "fidelity_d = 1 - df_d['avg_fidelity'][0]\n",
    "opt_lr_d = df_d['opt_lr'][0]\n",
    "print(f\"data_run_3.pickle ts: {df_d['init_params'][0]}\")\n",
    "print(f\"ts: {df_d['training_states'][0][0]}\")\n",
    "# Convert jax arrays to Python floats without rounding\n",
    "def calculate_iqr(data):\n",
    "    \"\"\"\n",
    "    Calculate the Interquartile Range (IQR) of the input data.\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data, 65) - np.percentile(data, 35)\n",
    "    return iqr\n",
    "\n",
    "# Convert jax arrays to Python floats without rounding\n",
    "mean_fidelity_analog = round(float(np.mean(testing_results_analog)), 5)\n",
    "std_fidelity_analog = round(float(np.std(testing_results_analog)), 5)\n",
    "\n",
    "\n",
    "mean_fidelity_digital = round(float(np.mean(testing_results_digital)), 5)\n",
    "std_fidelity_digital = round(float(np.std(testing_results_digital)), 5)\n",
    "\n",
    "\n",
    "mean_fidelity_c = round(float(np.mean(testing_results_c)), 5)\n",
    "std_fidelity_c = round(float(np.std(testing_results_c)), 5)\n",
    "\n",
    "\n",
    "mean_fidelity_d = round(float(np.mean(testing_results_d)), 5)\n",
    "std_fidelity_d = round(float(np.std(testing_results_d)), 5)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Fidelity (10): {mean_fidelity_analog} ± {std_fidelity_analog}\")\n",
    "\n",
    "print(f\"\\nAverage Fidelity (16): {mean_fidelity_digital} ± {std_fidelity_digital}\")\n",
    "\n",
    "print(f\"\\nAverage Fidelity (4): {mean_fidelity_c} ± {std_fidelity_c}\")\n",
    "\n",
    "print(f\"\\nAverage Fidelity (8): {mean_fidelity_d} ± {std_fidelity_d}\")\n",
    "\n",
    "# Plot histograms of testing results\n",
    "fig, ax = plt.subplots(5, 1, figsize=(12, 36))\n",
    "\n",
    "# Calculate average fidelity and standard deviation\n",
    "\n",
    "avg_fidelity_analog = np.mean(testing_results_analog)\n",
    "avg_fidelity_digital = np.mean(testing_results_digital)\n",
    "avg_fidelity_c = np.mean(testing_results_c)\n",
    "avg_fidelity_d = np.mean(testing_results_d)\n",
    "\n",
    "label_analog = f\"10: {avg_fidelity_analog:.5f}\"\n",
    "label_digital = f\"16: {avg_fidelity_digital:.5f}\"\n",
    "label_c = f\"4: {avg_fidelity_c:.5f}\"\n",
    "label_d = f\"8: {avg_fidelity_d:.5f}\"\n",
    "\n",
    "ax[0].hist([float(i) for i in testing_results_analog], bins=50, alpha=0.5,color='orange', label=label_analog)\n",
    "ax[0].hist([float(i) for i in testing_results_digital], bins=50, alpha=0.5,color='blue', label=label_digital)\n",
    "ax[0].hist([float(i) for i in testing_results_c], bins=50,color='green', alpha=0.5, label=label_c)\n",
    "ax[0].hist([float(i) for i in testing_results_d], bins=50,color='red', alpha=0.5, label=label_d)\n",
    "ax[0].legend(fontsize=20, loc=\"upper left\")\n",
    "ax[0].set_title('Test Results')\n",
    "ax[0].set_xlabel('Fidelity')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot costs per epoch\n",
    "ax[1].plot(range(1, len(costs_analog) + 1), costs_analog,color='orange', label=label_analog)\n",
    "ax[1].plot(range(1, len(costs_digital) + 1), costs_digital,color='blue', label=label_digital)\n",
    "ax[1].plot(range(1, len(costs_c) + 1), costs_c, color='green', label=label_c)\n",
    "ax[1].plot(range(1, len(costs_d) + 1), costs_d, color='red', label=label_d)\n",
    "ax[1].set_title('Costs per Epoch')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Cost')\n",
    "ax[1].legend(fontsize=20)\n",
    "\n",
    "# Plot variance of gradients per epoch\n",
    "var_analog = [np.var(grads) for grads in grads_per_epoch_analog]\n",
    "min_abs_analog = [np.min(np.abs(grads)) for grads in grads_per_epoch_analog]\n",
    "max_abs_analog = [np.max(np.abs(grads)) for grads in grads_per_epoch_analog]\n",
    "\n",
    "var_digital = [np.var(grads) for grads in grads_per_epoch_digital]\n",
    "min_abs_digital = [np.min(np.abs(grads)) for grads in grads_per_epoch_digital]\n",
    "max_abs_digital = [np.max(np.abs(grads)) for grads in grads_per_epoch_digital]\n",
    "\n",
    "var_c = [np.var(grads) for grads in grads_per_epoch_c]\n",
    "min_abs_c = [np.min(np.abs(grads)) for grads in grads_per_epoch_c]\n",
    "max_abs_c = [np.max(np.abs(grads)) for grads in grads_per_epoch_c]\n",
    "\n",
    "var_d = [np.var(grads) for grads in grads_per_epoch_d]\n",
    "min_abs_d = [np.min(np.abs(grads)) for grads in grads_per_epoch_d]\n",
    "max_abs_d = [np.max(np.abs(grads)) for grads in grads_per_epoch_d]\n",
    "\n",
    "ax[2].plot(range(1, len(var_analog) + 1), var_analog, label=label_analog, linestyle='-', color='orange')\n",
    "ax[2].plot(range(1, len(var_digital) + 1), var_digital, label=label_digital, linestyle='-', color='blue')\n",
    "ax[2].plot(range(1, len(var_c) + 1), var_c, label=label_c, linestyle='-', color='green')\n",
    "ax[2].plot(range(1, len(var_d) + 1), var_d, label=label_d, linestyle='-', color='red')\n",
    "ax[2].set_title('Variance of Gradients per Epoch', fontsize=25)\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Variance')\n",
    "ax[2].legend(fontsize=20)\n",
    "\n",
    "# Plot minimum absolute value of gradients per epoch\n",
    "ax[3].plot(range(1, len(min_abs_analog) + 1), min_abs_analog, linewidth=1.5, color='orange', label=label_analog, linestyle='-')\n",
    "ax[3].plot(range(1, len(min_abs_digital) + 1), min_abs_digital, linewidth=1.5, color='blue', label=label_digital, linestyle='-')\n",
    "ax[3].plot(range(1, len(min_abs_c) + 1), min_abs_c, linewidth=1, color='green', label=label_c, linestyle='-')\n",
    "ax[3].plot(range(1, len(min_abs_d) + 1), min_abs_d, linewidth=1, color='red', label=label_d, linestyle='-')\n",
    "ax[3].set_title('Minimum Absolute Value of Gradients per Epoch', fontsize=25)\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_yscale('log')\n",
    "ax[3].set_ylabel('Min Abs Gradient')\n",
    "ax[3].legend(fontsize=20)\n",
    "\n",
    "# Plot maximum absolute value of gradients per epoch\n",
    "ax[4].plot(range(1, len(max_abs_analog) + 1), max_abs_analog, linewidth=2, color='orange', label=label_analog, linestyle='-.')\n",
    "ax[4].plot(range(1, len(max_abs_digital) + 1), max_abs_digital, linewidth=2, color='blue', label=label_digital, linestyle='-.')\n",
    "ax[4].plot(range(1, len(max_abs_c) + 1), max_abs_c, linewidth=2, color='green', label=label_c, linestyle='-.')\n",
    "ax[4].plot(range(1, len(max_abs_d) + 1), max_abs_d, linewidth=2, color='red', label=label_d, linestyle='-.')\n",
    "ax[4].set_title('Maximum Absolute Value of Gradients per Epoch', fontsize=25)\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Max Abs Gradient')\n",
    "ax[4].legend(fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import base64\n",
    "import jax.numpy as jnp\n",
    "\n",
    "gate_idx = 4\n",
    "trot = 1\n",
    "N_r = 1\n",
    "N_ctrl = 1\n",
    "\n",
    "# Load the first dataset\n",
    "a = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_test/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_9.pickle'\n",
    "with open(a, 'rb') as f:\n",
    "    df_analog = pickle.load(f)\n",
    "\n",
    "# Load the second dataset\n",
    "b = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_test/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_19.pickle'\n",
    "with open(b, 'rb') as f:\n",
    "    df_digital = pickle.load(f)\n",
    "\n",
    "# Load the third dataset\n",
    "c = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_test/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_3.pickle'\n",
    "with open(c, 'rb') as f:\n",
    "    df_c = pickle.load(f)\n",
    "\n",
    "# Load the fourth dataset\n",
    "d = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_test/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_7.pickle'\n",
    "with open(d, 'rb') as f:\n",
    "    df_d = pickle.load(f)\n",
    "\n",
    "# Extract data for the first dataset\n",
    "costs_analog = [float(i) for i in df_analog['costs'][0]]\n",
    "\n",
    "grads_per_epoch_analog = [np.array(i) for i in df_analog['grads_per_epoch'][0]]\n",
    "testing_results_analog = df_analog['testing_results'][0]\n",
    "fidelity_analog = 1 - df_analog['avg_fidelity'][0]\n",
    "opt_lr_analog = df_analog['opt_lr'][0]\n",
    "\n",
    "# Extract data for the second dataset\n",
    "costs_digital = [float(i) for i in df_digital['costs'][0]]\n",
    "grads_per_epoch_digital = [np.array(i) for i in df_digital['grads_per_epoch'][0]]\n",
    "testing_results_digital = df_digital['testing_results'][0]\n",
    "fidelity_digital = 1 - df_digital['avg_fidelity'][0]\n",
    "opt_lr_digital = df_digital['opt_lr'][0]\n",
    "\n",
    "# Extract data for the third dataset\n",
    "costs_c = [float(i) for i in df_c['costs'][0]]\n",
    "grads_per_epoch_c = [np.array(i) for i in df_c['grads_per_epoch'][0]]\n",
    "testing_results_c = df_c['testing_results'][0]\n",
    "fidelity_c = 1 - df_c['avg_fidelity'][0]\n",
    "opt_lr_c = df_c['opt_lr'][0]\n",
    "print(f\"data_run_3__.pickle params: {df_c['init_params'][0]}\")\n",
    "print(f\"ts: {df_c['training_states'][0][0]}\")\n",
    "# # Extract data for the fourth dataset\n",
    "costs_d = [float(i) for i in df_d['costs'][0]]\n",
    "grads_per_epoch_d = [np.array(i) for i in df_d['grads_per_epoch'][0]]\n",
    "testing_results_d = df_d['testing_results'][0]\n",
    "fidelity_d = 1 - df_d['avg_fidelity'][0]\n",
    "opt_lr_d = df_d['opt_lr'][0]\n",
    "print(f\"data_run_3.pickle ts: {df_d['init_params'][0]}\")\n",
    "print(f\"ts: {df_d['training_states'][0][0]}\")\n",
    "# Convert jax arrays to Python floats without rounding\n",
    "def calculate_iqr(data):\n",
    "    \"\"\"\n",
    "    Calculate the Interquartile Range (IQR) of the input data.\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data, 65) - np.percentile(data, 35)\n",
    "    return iqr\n",
    "\n",
    "# Convert jax arrays to Python floats without rounding\n",
    "mean_fidelity_analog = round(float(np.mean(testing_results_analog)), 5)\n",
    "std_fidelity_analog = round(float(np.std(testing_results_analog)), 5)\n",
    "var_grad_analog = float(jnp.mean(df_analog['var_grad'][0]))\n",
    "iqr_var_grad_analog = calculate_iqr(jnp.array(df_analog['var_grad'][0]))\n",
    "min_grad_analog = float(jnp.min(df_analog['min_grad'][0]))\n",
    "max_grad_analog = float(jnp.max(df_analog['max_grad'][0]))\n",
    "\n",
    "mean_fidelity_digital = round(float(np.mean(testing_results_digital)), 5)\n",
    "std_fidelity_digital = round(float(np.std(testing_results_digital)), 5)\n",
    "var_grad_digital = float(jnp.mean(df_digital['var_grad'][0]))\n",
    "iqr_var_grad_digital = calculate_iqr(jnp.array(df_digital['var_grad'][0]))\n",
    "min_grad_digital = float(jnp.min(df_digital['min_grad'][0]))\n",
    "max_grad_digital = float(jnp.max(df_digital['max_grad'][0]))\n",
    "\n",
    "mean_fidelity_c = round(float(np.mean(testing_results_c)), 5)\n",
    "std_fidelity_c = round(float(np.std(testing_results_c)), 5)\n",
    "var_grad_c = float(jnp.mean(df_c['var_grad'][0]))\n",
    "iqr_var_grad_c = calculate_iqr(jnp.array(df_c['var_grad'][0]))\n",
    "min_grad_c = float(jnp.min(df_c['min_grad'][0]))\n",
    "max_grad_c = float(jnp.max(df_c['max_grad'][0]))\n",
    "\n",
    "mean_fidelity_d = round(float(np.mean(testing_results_d)), 5)\n",
    "std_fidelity_d = round(float(np.std(testing_results_d)), 5)\n",
    "var_grad_d = float(jnp.mean(df_d['var_grad'][0]))\n",
    "iqr_var_grad_d = calculate_iqr(jnp.array(df_d['var_grad'][0]))\n",
    "min_grad_d = float(jnp.min(df_d['min_grad'][0]))\n",
    "max_grad_d = float(jnp.max(df_d['max_grad'][0]))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Fidelity (10): {mean_fidelity_analog} ± {std_fidelity_analog}\")\n",
    "print(f\"Initial Variance (10): {var_grad_analog:.5e}, IQR: {iqr_var_grad_analog:.5e}, Min Gradient: {min_grad_analog:.5e}, Max Gradient: {max_grad_analog:.5e}\")\n",
    "\n",
    "print(f\"\\nAverage Fidelity (16): {mean_fidelity_digital} ± {std_fidelity_digital}\")\n",
    "print(f\"Initial Variance (16): {var_grad_digital:.5e}, IQR: {iqr_var_grad_digital:.5e}, Min Gradient: {min_grad_digital:.5e}, Max Gradient: {max_grad_digital:.5e}\")\n",
    "\n",
    "print(f\"\\nAverage Fidelity (4): {mean_fidelity_c} ± {std_fidelity_c}\")\n",
    "print(f\"Initial Variance (4): {var_grad_c:.5e}, IQR: {iqr_var_grad_c:.5e}, Min Gradient: {min_grad_c:.5e}, Max Gradient: {max_grad_c:.5e}\")\n",
    "\n",
    "print(f\"\\nAverage Fidelity (8): {mean_fidelity_d} ± {std_fidelity_d}\")\n",
    "print(f\"Initial Variance (8): {var_grad_d:.5e}, IQR: {iqr_var_grad_d:.5e}, Min Gradient: {min_grad_d:.5e}, Max Gradient: {max_grad_d:.5e}\")\n",
    "\n",
    "# Plot histograms of testing results\n",
    "fig, ax = plt.subplots(5, 1, figsize=(12, 36))\n",
    "\n",
    "# Calculate average fidelity and standard deviation\n",
    "\n",
    "avg_fidelity_analog = np.mean(testing_results_analog)\n",
    "avg_fidelity_digital = np.mean(testing_results_digital)\n",
    "avg_fidelity_c = np.mean(testing_results_c)\n",
    "avg_fidelity_d = np.mean(testing_results_d)\n",
    "\n",
    "label_analog = f\"10: {avg_fidelity_analog:.5f}\"\n",
    "label_digital = f\"16: {avg_fidelity_digital:.5f}\"\n",
    "label_c = f\"4: {avg_fidelity_c:.5f}\"\n",
    "label_d = f\"8: {avg_fidelity_d:.5f}\"\n",
    "\n",
    "ax[0].hist([float(i) for i in testing_results_analog], bins=50, alpha=0.5,color='orange', label=label_analog)\n",
    "ax[0].hist([float(i) for i in testing_results_digital], bins=50, alpha=0.5,color='blue', label=label_digital)\n",
    "ax[0].hist([float(i) for i in testing_results_c], bins=50,color='green', alpha=0.5, label=label_c)\n",
    "ax[0].hist([float(i) for i in testing_results_d], bins=50,color='red', alpha=0.5, label=label_d)\n",
    "ax[0].legend(fontsize=20, loc=\"upper left\")\n",
    "ax[0].set_title('Test Results')\n",
    "ax[0].set_xlabel('Fidelity')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot costs per epoch\n",
    "ax[1].plot(range(1, len(costs_analog) + 1), costs_analog,color='orange', label=label_analog)\n",
    "ax[1].plot(range(1, len(costs_digital) + 1), costs_digital,color='blue', label=label_digital)\n",
    "ax[1].plot(range(1, len(costs_c) + 1), costs_c, color='green', label=label_c)\n",
    "ax[1].plot(range(1, len(costs_d) + 1), costs_d, color='red', label=label_d)\n",
    "ax[1].set_title('Costs per Epoch')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Cost')\n",
    "ax[1].legend(fontsize=20)\n",
    "\n",
    "# Plot variance of gradients per epoch\n",
    "var_analog = [np.var(grads) for grads in grads_per_epoch_analog]\n",
    "min_abs_analog = [np.min(np.abs(grads)) for grads in grads_per_epoch_analog]\n",
    "max_abs_analog = [np.max(np.abs(grads)) for grads in grads_per_epoch_analog]\n",
    "\n",
    "var_digital = [np.var(grads) for grads in grads_per_epoch_digital]\n",
    "min_abs_digital = [np.min(np.abs(grads)) for grads in grads_per_epoch_digital]\n",
    "max_abs_digital = [np.max(np.abs(grads)) for grads in grads_per_epoch_digital]\n",
    "\n",
    "var_c = [np.var(grads) for grads in grads_per_epoch_c]\n",
    "min_abs_c = [np.min(np.abs(grads)) for grads in grads_per_epoch_c]\n",
    "max_abs_c = [np.max(np.abs(grads)) for grads in grads_per_epoch_c]\n",
    "\n",
    "var_d = [np.var(grads) for grads in grads_per_epoch_d]\n",
    "min_abs_d = [np.min(np.abs(grads)) for grads in grads_per_epoch_d]\n",
    "max_abs_d = [np.max(np.abs(grads)) for grads in grads_per_epoch_d]\n",
    "\n",
    "ax[2].plot(range(1, len(var_analog) + 1), var_analog, label=label_analog, linestyle='-', color='orange')\n",
    "ax[2].plot(range(1, len(var_digital) + 1), var_digital, label=label_digital, linestyle='-', color='blue')\n",
    "ax[2].plot(range(1, len(var_c) + 1), var_c, label=label_c, linestyle='-', color='green')\n",
    "ax[2].plot(range(1, len(var_d) + 1), var_d, label=label_d, linestyle='-', color='red')\n",
    "ax[2].set_title('Variance of Gradients per Epoch', fontsize=25)\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Variance')\n",
    "ax[2].legend(fontsize=20)\n",
    "\n",
    "# Plot minimum absolute value of gradients per epoch\n",
    "ax[3].plot(range(1, len(min_abs_analog) + 1), min_abs_analog, linewidth=1.5, color='orange', label=label_analog, linestyle='-')\n",
    "ax[3].plot(range(1, len(min_abs_digital) + 1), min_abs_digital, linewidth=1.5, color='blue', label=label_digital, linestyle='-')\n",
    "ax[3].plot(range(1, len(min_abs_c) + 1), min_abs_c, linewidth=1, color='green', label=label_c, linestyle='-')\n",
    "ax[3].plot(range(1, len(min_abs_d) + 1), min_abs_d, linewidth=1, color='red', label=label_d, linestyle='-')\n",
    "ax[3].set_title('Minimum Absolute Value of Gradients per Epoch', fontsize=25)\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_yscale('log')\n",
    "ax[3].set_ylabel('Min Abs Gradient')\n",
    "ax[3].legend(fontsize=20)\n",
    "\n",
    "# Plot maximum absolute value of gradients per epoch\n",
    "ax[4].plot(range(1, len(max_abs_analog) + 1), max_abs_analog, linewidth=2, color='orange', label=label_analog, linestyle='-.')\n",
    "ax[4].plot(range(1, len(max_abs_digital) + 1), max_abs_digital, linewidth=2, color='blue', label=label_digital, linestyle='-.')\n",
    "ax[4].plot(range(1, len(max_abs_c) + 1), max_abs_c, linewidth=2, color='green', label=label_c, linestyle='-.')\n",
    "ax[4].plot(range(1, len(max_abs_d) + 1), max_abs_d, linewidth=2, color='red', label=label_d, linestyle='-.')\n",
    "ax[4].set_title('Maximum Absolute Value of Gradients per Epoch', fontsize=25)\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Max Abs Gradient')\n",
    "ax[4].legend(fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_idx = 0\n",
    "trot = 2\n",
    "N_r = 1\n",
    "N_ctrl = 1\n",
    "selected_sets = [0]  # List of dataset indices\n",
    "\n",
    "def calculate_iqr(data):\n",
    "    \"\"\"\n",
    "    Calculate the Interquartile Range (IQR) of the input data.\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data, 85) - np.percentile(data, 15)\n",
    "    return iqr\n",
    "\n",
    "# Initialize the paths to the datasets (as a loop)\n",
    "data_files = [f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_optimized_by_cost3/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_{run_idx}.pickle'\n",
    "              for run_idx in selected_sets]\n",
    "def calculate_gradient_stats(gradients):\n",
    "    mean_grad = jnp.mean(gradients, axis=0)\n",
    "    mean_grad_squared = jnp.mean(gradients**2, axis=0)\n",
    "    var_grad = mean_grad_squared - mean_grad**2\n",
    "    return mean_grad, var_grad\n",
    "# Load the datasets and extract relevant data\n",
    "data_results = []\n",
    "for i,file_path in enumerate(data_files):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Extract necessary information from the dataset\n",
    "    costs = [float(i) for i in data['costs'][0]]\n",
    "    grads_per_epoch = [np.array(i) for i in data['grads_per_epoch'][0]]\n",
    "    init_grads = grads_per_epoch[0]\n",
    "    \n",
    "\n",
    "    # print(init_grads.shape, np.abs((init_grads)),init_grads)\n",
    "    testing_results = data['testing_results'][0]\n",
    "    fidelity = data['avg_fidelity'][0]\n",
    "    # avg_fidelity = np.mean(fidelity)\n",
    "    opt_lr = data['opt_lr'][0]\n",
    "    trainin_states = data['training_states'][0][0]\n",
    "    \n",
    "\n",
    "    \n",
    "    var_grad = [np.var(grads) for grads in grads_per_epoch]\n",
    "    min_grad = [np.min(np.abs(grads)) for grads in grads_per_epoch]\n",
    "    max_grad = [np.max(np.abs(grads)) for grads in grads_per_epoch]\n",
    "\n",
    "    iqr_var_grad = calculate_iqr(jnp.array(init_grads))\n",
    "    mean_fidelity = round(float(np.mean(testing_results)), 5)\n",
    "    std_fidelity = round(float(np.std(testing_results)), 5)\n",
    "    # Print the results\n",
    "    print(f\"\\nAverage Fidelity ({selected_sets[i]+1}): {mean_fidelity} ± {std_fidelity}\")\n",
    "    print(f\"Initial Variance ({selected_sets[i]+1}): {np.var(init_grads):.5e}, IQR: {iqr_var_grad:.5e}, Min Gradient: {np.min(np.abs(init_grads)):.5e}, Max Grad: {np.max(np.abs(init_grads)):.5e}\")\n",
    "\n",
    "    print(f\"From test: {np.mean(data['var_grad'][0]):.5e}, IQR: {calculate_iqr(data['var_grad'][0]):.5e}, Min Gradient: {data['min_grad'][0]:.5e}, Max Grad: {data['max_grad'][0]:.5e}\")\n",
    "\n",
    "    \n",
    "    mean_grad, var_grad_out = calculate_gradient_stats(init_grads)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    data_results.append({\n",
    "        'costs': costs,\n",
    "        'var_grad': var_grad,\n",
    "        'min_grad': min_grad,\n",
    "        'max_grad': max_grad,\n",
    "        'avg_fidelity':fidelity,\n",
    "        'testing_results': testing_results\n",
    "    })\n",
    "\n",
    "# Now plot the results\n",
    "fig, ax = plt.subplots(5, 1, figsize=(8, 20))\n",
    "\n",
    "# Loop over datasets and add each to the plots\n",
    "for idx, dataset in enumerate(data_results):\n",
    "    # Dynamically label the dataset for each selected set\n",
    "    avg_fidelity = dataset['avg_fidelity']\n",
    "    label = f'Set {selected_sets[idx]+1} [ {avg_fidelity:.5f}]'\n",
    "    \n",
    "    # Plot testing results (histograms)\n",
    "    ax[0].hist([float(i) for i in dataset['testing_results']], bins=50, alpha=0.5, label=f'{label}')\n",
    "    \n",
    "    # Plot costs per epoch\n",
    "    ax[1].plot(range(1, len(dataset['costs']) + 1), dataset['costs'], label=label)\n",
    "    \n",
    "    # Plot variance of gradients per epoch\n",
    "    ax[2].plot(range(1, len(dataset['var_grad']) + 1), dataset['var_grad'], label=label)\n",
    "    ax[2].set_yscale('log')\n",
    "    # Plot minimum absolute value of gradients per epoch\n",
    "    ax[3].plot(range(1, len(dataset['min_grad']) + 1), dataset['min_grad'],linewidth=1, label=label)\n",
    "    ax[3].set_yscale('log')\n",
    "    # Plot maximum absolute value of gradients per epoch\n",
    "    ax[4].plot(range(1, len(dataset['max_grad']) + 1), dataset['max_grad'], label=label)\n",
    "    ax[4].set_yscale('log')\n",
    "\n",
    "# Customize the plot titles, labels, and legends\n",
    "ax[0].set_title('Test Results (Fidelity)')\n",
    "ax[0].set_xlabel('Fidelity')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend(fontsize=12)\n",
    "\n",
    "ax[1].set_title('Costs per Epoch')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Cost')\n",
    "ax[1].legend(fontsize=12)\n",
    "\n",
    "ax[2].set_title('Variance of Gradients per Epoch')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Variance')\n",
    "ax[2].legend(fontsize=12)\n",
    "\n",
    "ax[3].set_title('Minimum Absolute Value of Gradients per Epoch')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('Min Abs Gradient')\n",
    "ax[3].set_yscale('log')  # Log scale as before\n",
    "ax[3].legend(fontsize=12)\n",
    "\n",
    "ax[4].set_title('Maximum Absolute Value of Gradients per Epoch')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Max Abs Gradient')\n",
    "ax[4].legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "gate_idx = \n",
    "trot = 3\n",
    "N_r = 1\n",
    "N_ctrl = 2\n",
    "selected_sets = [0,1,2,3,9]  # List of selected data run indices\n",
    "trainsize = 20 \n",
    "def calculate_iqr(data):\n",
    "    \"\"\"\n",
    "    Calculate the Interquartile Range (IQR) of the input data.\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    return iqr\n",
    "# Initialize the plot for costs per epoch\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Loop through the selected sets\n",
    "# for idx, run_idx in enumerate(selected_sets):\n",
    "for idx, run_idx in enumerate(selected_sets):\n",
    "    # Load the dataset\n",
    "    file_path =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}_optimized_by_cost/0/U{N_ctrl}_{run_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_1.pickle'\n",
    "    # file_path = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_test/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_{run_idx}.pickle'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Extract data\n",
    "    costs = [float(i) for i in data['costs'][0]]\n",
    "    grads_per_epoch = [np.array(i) for i in data['grads_per_epoch'][0]]\n",
    "    testing_results = data['testing_results'][0]\n",
    "    fidelity = 1 - data['avg_fidelity'][0]\n",
    "    opt_lr = data['opt_lr'][0]\n",
    "    \n",
    "    # Convert jax arrays to Python floats without rounding\n",
    "    mean_fidelity = round(float(np.mean(testing_results)), 5)\n",
    "    std_fidelity = round(float(np.std(testing_results)), 5)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"U {run_idx + 1}:\")\n",
    "    print(f\"Average Fidelity: {mean_fidelity} ± {std_fidelity}\")\n",
    "\n",
    "    \n",
    "    plt.hist(testing_results, bins=50, alpha=0.5,label = f'U{N_ctrl}_{run_idx}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "gate_idx = 1\n",
    "trot = 1\n",
    "N_r = 1\n",
    "N_ctrl = 1\n",
    "selected_sets = [3,9]  # List of selected data run indices\n",
    "trainsize = 20\n",
    "def calculate_iqr(data):\n",
    "    \"\"\"\n",
    "    Calculate the Interquartile Range (IQR) of the input data.\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    return iqr\n",
    "# Initialize the plot for costs per epoch\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Loop through the selected sets\n",
    "for idx, run_idx in enumerate(selected_sets):\n",
    "    # Load the dataset\n",
    "    file_path =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}_optimized_by_cost3/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "    file_path = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_test/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_{run_idx}.pickle'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Extract data\n",
    "    costs = [float(i) for i in data['costs'][0]]\n",
    "    grads_per_epoch = [np.array(i) for i in data['grads_per_epoch'][0]]\n",
    "    testing_results = data['testing_results'][0]\n",
    "    fidelity = 1 - data['avg_fidelity'][0]\n",
    "    opt_lr = data['opt_lr'][0]\n",
    "    \n",
    "    # Convert jax arrays to Python floats without rounding\n",
    "    mean_fidelity = round(float(np.mean(testing_results)), 5)\n",
    "    std_fidelity = round(float(np.std(testing_results)), 5)\n",
    "    var_grad = float(jnp.mean(data['var_grad'][0]))\n",
    "    iqr_var_grad = calculate_iqr(jnp.array(data['var_grad'][0]))\n",
    "    min_grad = float(jnp.min(data['min_grad'][0]))\n",
    "    max_grad = float(jnp.max(data['max_grad'][0]))\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Set {run_idx + 1} (data_run_{run_idx}):\")\n",
    "    print(f\"Average Fidelity: {mean_fidelity} ± {std_fidelity}\")\n",
    "    print(f\"Initial Variance: {var_grad:.5e}, IQR: {iqr_var_grad:.5e}, Min Gradient: {min_grad:.5e}, Max Gradient: {max_grad:.5e}\\n\")\n",
    "    # Plot the cost per epoch for this dataset\n",
    "    plt.plot(range(1, len(costs) + 1), costs, label=f'Set {run_idx + 1} (data_run_{run_idx})')\n",
    "    \n",
    "\n",
    "# Finalize the plot\n",
    "plt.title('Costs per Epoch for Selected Sets')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_epoch1000/U2_1/reservoirs_2/trotter_step_30/bath_False/data_run_1.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Note 'rb' here, which means read binary\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Note 'rb' here, which means read binary\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     dfb \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# results_dict = df['preopt_results']\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/qualtran_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_epoch1000/U2_1/reservoirs_2/trotter_step_30/bath_False/data_run_1.pickle'"
     ]
    }
   ],
   "source": [
    "gate_idx =4\n",
    "trot = 8\n",
    "N_r = 1\n",
    "N_ctrl = 2\n",
    "trainsize = 20\n",
    "temp = 1*10**-2.9\n",
    "# /Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_1/reservoirs_1/trotter_step_2/10_training_states_no_opt/fixed_params0/1.0K/test22\n",
    "a = f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_epoch1000/U2_1/reservoirs_2/trotter_step_30/bath_False/data_run_0.pickle'\n",
    "b =f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_epoch1000/U2_1/reservoirs_2/trotter_step_30/bath_False/data_run_1.pickle'\n",
    "\n",
    "# a =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}_optimized_by_cost_t1.0/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "with open(a, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "with open(b, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    dfb = pickle.load(f)\n",
    "# results_dict = df['preopt_results']\n",
    "print(df['epochs'])\n",
    "print(dfb['epochs'])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "# gate_idx =gate_idx + 1\n",
    "costs = [float(i) for i in df['costs'][0]]\n",
    "costsb = [float(i) for i in dfb['costs'][0]]\n",
    "# rocs = [float(i) for i in df['rocs'][0]]\n",
    "# print(df['selected_indices'][0])\n",
    "grads_per_epoch = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "testing_results = df['testing_results'][0]\n",
    "fidelity = df['avg_fidelity'][0]\n",
    "# print(df['stored_epoch'][0],df['a_marked'][0]*1e-3, (df['a_marked'][0]*temp)*1 )\n",
    "print(df['epochs'][0])\n",
    "# print(df['training_states'][0][0])\n",
    "# print(df['init_params'][0])\n",
    "label1 =f\"analog: {fidelity:6e}\"\n",
    "# print(df['init_params'][0][trot:])\n",
    "# results_dict = df['preopt_results']\n",
    "\n",
    "gate = df['Gate'][0]\n",
    "decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "#print(decoded_qobj)\n",
    "test_results = [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "ax.hist(test_results, bins=50, alpha=0.5,label = label1)\n",
    "\n",
    "test_resultsb = [float(i) for i in dfb['testing_results'][0]] \n",
    "ax.hist(test_resultsb, bins=50, alpha=0.5,label = 'b')\n",
    "print(f\"final_cost: {1-costs[-1]:.4f}. avg fidelity 1: {np.mean(test_results):3f} +- {np.std(test_results):3f}\\n\")\n",
    "print(test_results)\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "ax.plot(costsb)\n",
    "ax.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_idx =0\n",
    "trot = 28\n",
    "N_ctrl = 2\n",
    "trainsize = 20\n",
    "temp = 1*10**-2.9\n",
    "res_count = 1\n",
    "# /Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_1/reservoirs_1/trotter_step_2/10_training_states_no_opt/fixed_params0/1.0K/test22\n",
    "# a = f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_{N_ctrl}/reservoirs_{N_r}/trotter_step_{trot}/10_training_states_no_opt/fixed_params0/1.0K/test22/U{N_ctrl}_{gate_idx}/data_run_0.pickle'\n",
    "a = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_optimized_by_cost3/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "# a =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}_optimized_by_cost_t1.0/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "with open(a, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "# # results_dict = df['preopt_results']\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "# # gate_idx =gate_idx + 1\n",
    "costs = [float(i) for i in df['costs'][0]]\n",
    "# rocs = [float(i) for i in df['rocs'][0]]\n",
    "# # print(df['stored_epoch'][0],df['selected_indices'][0])\n",
    "# grads_per_epoch = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "test_results = df['testing_results'][0]\n",
    "fidelity = df['avg_fidelity'][0]\n",
    "# # print(df['stored_epoch'][0],df['backup_epoch'][0], (df['a_marked'][0]*temp)*1 )\n",
    "# # print(df['backup_epoch'][0])\n",
    "# # print(df['training_states'][0][0])\n",
    "# # print(df['init_params'][0])\n",
    "label1 =f\"analog: {fidelity:6e}\"\n",
    "params = df['init_params'][0]\n",
    "params = '[' + ', '.join([f\"{x:.16f}\" for x in params]) + ']'\n",
    "\n",
    "print(params)\n",
    "print(f\"final_cost: {1-costs[-1]:.4f}. avg fidelity 1: {fidelity:3f} +- {np.std(test_results):3f}\")\n",
    "# print(df['stored_epoch'][0],df['selected_indices'][0])\n",
    "print(f\"lr: {df['opt_lr'][0]:.5f}, train state 0: \",df['training_states'][0][0])\n",
    "# gate = df['Gate'][0]\n",
    "# decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "# #print(decoded_qobj)\n",
    "print(df.keys())\n",
    "test_results = [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "ax.hist(test_results, bins=50, alpha=0.5,label = label1)\n",
    "\n",
    "# add=0\n",
    "# if N_ctrl ==3:\n",
    "#     add = 5\n",
    "# /Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_20_optimized_by_cost3/0/U2_0/reservoirs_1/trotter_step_10/bath_False/data_run_0.pickle\n",
    "# b =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}_optimized_by_cost/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "b =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}/0/U{N_ctrl}_{gate_idx}/reservoirs_{res_count}/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "\n",
    "# b =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}_optimized_by_cost_test/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/A5_to_A7__.pickle'\n",
    "\n",
    "with open(b, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "# rocs2 = [float(i) for i in df['rocs'][0]]\n",
    "# print(df['init_params'][0][trot:])\n",
    "results_dict = df['preopt_results']\n",
    "# a_marked = df['a_marked'][0]\n",
    "costs2 = [float(i) for i in df['costs'][0]]\n",
    "testing_results2 = df['testing_results'][0]\n",
    "fidelity2 = df['avg_fidelity'][0]\n",
    "# print(df['stored_epoch'][0],df['selected_indices'][0])\n",
    "label2 = f\"test 53: {fidelity2:6e}\"\n",
    "gate = df['Gate'][0]\n",
    "decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "# results_dict = df['preopt_results']\n",
    "test_results2 = [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "ax.hist(test_results2, bins=50, alpha=0.5,label = label2)\n",
    "print(f\"\\nfinal_cost: {1-costs2[-1]:.4f}. avg fidelity 3: {fidelity2:3f} +- {np.std(test_results2):3f}\")\n",
    "print(df['stored_epoch'][0],df['selected_indices'][0])\n",
    "print(f\"lr: {df['opt_lr'][0]:.5f}\")\n",
    "print(df['init_params'][0])\n",
    "\n",
    "# c =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_20/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_1.pickle'\n",
    "# # c =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize+add}/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_2.pickle'\n",
    "\n",
    "# with open(c, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "#     df = pickle.load(f)\n",
    "\n",
    "# costs3 = [float(i) for i in df['costs'][0]]\n",
    "\n",
    "# fidelity3 = df['avg_fidelity'][0]\n",
    "# results_dict = df['preopt_results']\n",
    "# label3 =  f\"analog (trots: {trot}): {fidelity3:6e}\"\n",
    "\n",
    "# test_results3 =  [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "# # results_dict = df['preopt_results']\n",
    "# print(f\"\\nfinal_cost: {1-costs3[-1]:.4f}. avg fidelity 3: {np.mean(test_results3):3f} +- {np.std(test_results3):3f}\")\n",
    "# print(df['stored_epoch'][0],df['selected_indices'][0])\n",
    "# # print(f\"lr: {df['opt_lr'][0]:.5f}, replacement indices: \",df['replacement_indices'][0])\n",
    "# # print(df['init_params'][0])\n",
    "# ax.hist(test_results3, bins=50, alpha=0.5,label = label3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # /Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_20_all_results/0/U1_9/reservoirs_1/trotter_step_3/bath_False/data_run_3.pickle\n",
    "# d =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_{trainsize}/0/U{N_ctrl}_{gate_idx}/reservoirs_{res_count}/trotter_step_{trot}/bath_False/data_run_1.pickle'\n",
    "# with open(d, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "#     df = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# costs4 = [float(i) for i in df['costs'][0]]\n",
    "\n",
    "# fidelity4 = df['avg_fidelity'][0]\n",
    "\n",
    "# label4 =  f\"digital (trots: {trot}): {fidelity4:6e}\"\n",
    "\n",
    "# test_results4 = [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "\n",
    "# print(f\"\\nfinal_cost: {1-costs4[-1]:.4f}. avg fidelity 4: {np.mean(test_results4):3f} +- {np.std(test_results4):3f}\")\n",
    "\n",
    "# print(f\"lr: {df['opt_lr'][0]:.5f}, train state 0: \",df['training_states'][0][0])\n",
    "# print(df['stored_epoch'][0],df['selected_indices'][0])\n",
    "# # print(df['selected_indices'][0])\n",
    "# ax.hist(test_results4, bins=50, alpha=0.5,label = label4)\n",
    "\n",
    "\n",
    "\n",
    "# ax.legend(fontsize=20)\n",
    "\n",
    "# /analog_results_trainable_global/trainsize_20/0/U3_2/reservoirs_1/trotter_step_24/bath_False/data_run_1.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load the first dataset\n",
    "a = '/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_optimized_by_cost3/0/U2_0/reservoirs_1/trotter_step_8/bath_False/data_run_0.pickle'\n",
    "with open(a, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "test_results1 = [float(i) for i in df['testing_results'][0]]\n",
    "label1 = f\"analog: {df['avg_fidelity'][0]:6e}\"\n",
    "\n",
    "# Load the second dataset\n",
    "b = '/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_20/0/U2_0/reservoirs_1/trotter_step_8/bath_False/data_run_0.pickle'\n",
    "with open(b, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "test_results2 = [float(i) for i in df['testing_results'][0]]\n",
    "label2 = f\"test 53: {df['avg_fidelity'][0]:6e}\"\n",
    "\n",
    "# Load the third dataset\n",
    "c = '/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_20/0/U2_0/reservoirs_1/trotter_step_8/bath_False/data_run_1.pickle'\n",
    "with open(c, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "test_results3 = [float(i) for i in df['testing_results'][0]]\n",
    "label3 = f\"analog (trots: 8): {df['avg_fidelity'][0]:6e}\"\n",
    "\n",
    "# Plot using seaborn histplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.histplot(test_results1, bins=50, label=label1, color=\"blue\", stat='probability',alpha=0.5)\n",
    "sns.histplot(test_results2, bins=50, label=label2, color=\"green\",stat='probability', alpha=0.5)\n",
    "sns.histplot(test_results3, bins=50, label=label3, color=\"orange\",stat='probability', alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Fidelity Distributions from Testing Results')\n",
    "plt.xlabel('Fidelity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(costs)+1),costs, label = label1)\n",
    "linestyle =  '--'\n",
    "\n",
    "\n",
    "plt.plot(range(1,len(costs2)+1),costs2, label = label2)\n",
    "\n",
    "plt.plot(range(1,len(costs3)+1),costs3, label = label3, linestyle=linestyle)\n",
    "# plt.plot(range(1,len(costs4)+1),costs4, label = label4, linestyle=linestyle)\n",
    "plt.legend()\n",
    "\n",
    "# print(costs[1490:])\n",
    "# len(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b =1*(10**-2.55)\n",
    "\n",
    "print(b*3.42e-04)\n",
    "b = 1 * (10**-3)\n",
    "\n",
    "# b = 1**10-2.5\n",
    "print(b*3.42e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1400\n",
    "plt.plot(costs[i:])\n",
    "plt.plot(costs2[i:])\n",
    "costs[-4]-costs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh =9.638869624924435e-07\n",
    "thresh = 7.866838e-07\n",
    "for i,a in enumerate(rocs):\n",
    "    if a < 0.0 and np.abs(a) < thresh:\n",
    "        print(i, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rocs[11]*1e-3)\n",
    "# print(np.abs(rocs[50])*1e-2)\n",
    "# rocs[850]\n",
    "# print(rocs2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rocs[8]*1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy\n",
    "def calculate_gradient_stats_per_state(gradients,abs_grad=True):\n",
    "    \n",
    "    \"\"\"Calculate the mean and variance of the gradients for each state.\"\"\"\n",
    "    if abs_grad:\n",
    "        gradients = jnp.abs(gradients)\n",
    "    mean_grad = jnp.mean(gradients, axis=-1)  # Mean across parameters for each state\n",
    "    mean_grad_squared = jnp.mean(gradients ** 2, axis=-1)  # Mean squared gradients across parameters\n",
    "    var_grad = mean_grad_squared - mean_grad ** 2  # Variance per state\n",
    "    grad_norm = jnp.linalg.norm(gradients, axis=-1)  # Norm per state\n",
    "\n",
    "    return mean_grad, var_grad, grad_norm\n",
    "def analyze_results(results_dict, num_datasets):\n",
    "    \"\"\"\n",
    "    Analyze results dictionary to select the best datasets for initial training, replacement, and fine-tuning.\n",
    "    \"\"\"\n",
    "    # Lists to store metrics for normalization purposes\n",
    "    var_grad_means,var_var_grads = [], []\n",
    "    min_gradvars,max_gradvars = [], []\n",
    "    min_mean_grads,max_mean_grads = [], []\n",
    "    grad_norms, mean_grads = [],[]\n",
    "    min_norm_var_grads, max_norm_var_grads = [],[]\n",
    "    normalized_var_grad_norms = []\n",
    "\n",
    "    # Initialize tracking for scores and results\n",
    "    results = {}\n",
    "\n",
    "    # Loop through all datasets in the results_dict\n",
    "    for set_idx in range(num_datasets):\n",
    "        # Extract necessary statistics for the current dataset\n",
    "        # meangrad_norm, vargrad_norm, gradnorm_norm = calculate_gradient_stats_per_state(normalized_gradients_per_state)\n",
    "        result = results_dict[f'dataset_{set_idx}']\n",
    "        var_grad_mean = result[\"Mean(Var Grad)\"]\n",
    "        variance_of_variance_gradients_normalized =result[\"Var(Var Grad) [normalized]\"]\n",
    "        mean_grad = result[\"Mean(Mean Grad)\"]\n",
    "        min_grad, max_grad = result['Min Gradient'], result['Max Gradient']\n",
    "        min_vargrad, max_vargrad = result['min_var_grad_means'], result['max_var_grad_means']\n",
    "        min_gradvars.append(min_vargrad)\n",
    "        max_gradvars.append(max_vargrad)\n",
    "        min_mean_grads.append(min_grad)\n",
    "        max_mean_grads.append(max_grad)\n",
    "        var_var_grad = result[\"Var(Var Grad)\"]\n",
    "        grad_norm = result[\"Gradient Norm\"]\n",
    "        norm_var_grad_mean = result[\"Mean(Var Grad) [normalized]\"]\n",
    "\n",
    "        # Collect statistics for normalization purposes\n",
    "        var_grad_means.append(var_grad_mean)\n",
    "        var_var_grads.append(var_var_grad)\n",
    "        mean_grads.append(mean_grad)\n",
    "        grad_norms.append(grad_norm)\n",
    "        \n",
    "        normalized_var_grad_norms.append(np.float64(variance_of_variance_gradients_normalized))\n",
    "\n",
    "    # Compute min and max for normalization\n",
    "    min_var_var,max_var_var = min(var_var_grads), max(var_var_grads)\n",
    "    print(np.array(mean_grads).shape)\n",
    "    print(f\"Variance of the gradients: [min: {min(min_gradvars):.2e}), max: {max(max_gradvars):.2e}]\")\n",
    "    print(f\"Mean gradients: [min: {min(min_mean_grads):.2e}), max: {max(max_mean_grads):.2e}]\")\n",
    "    min_var_grad = min(min_gradvars)\n",
    "    max_var_grad = max(max_gradvars)\n",
    "    min_mean = min(min_mean_grads)\n",
    "    max_mean = max(max_mean_grads)\n",
    "    min_norm_var_grad = min(normalized_var_grad_norms*10)\n",
    "    max_norm_var_grad = max(normalized_var_grad_norms*10)\n",
    "    print(f\"normalized_var_grad_norms: {normalized_var_grad_norms}\")\n",
    "    log_var_grad_norms = [numpy.float64(np.log2(el)) for el in normalized_var_grad_norms]\n",
    "    print(f\"log_var_grad_norms: {log_var_grad_norms}\")\n",
    "    print(f\"min: {min_norm_var_grad:.2e}, max: {max_norm_var_grad:.2e}\")\n",
    "    # Function for normalizing metrics\n",
    "    def normalize_metric(value, min_val, max_val, epsilon=1e-6, upper_bound=0.999):\n",
    "        if max_val > min_val:\n",
    "            # Adjust to avoid exactly 0.0 and 1.0, and ensure upper bound doesn't reach 1.0\n",
    "            normalized_value = (value - min_val ) / (max_val - min_val + 2 )\n",
    "            return min(normalized_value, upper_bound)\n",
    "        else:\n",
    "            return 0.5  # Neutral value if min and max are the same\n",
    "        \n",
    "    to_print = [0,1,2,3,4,5,6,7,8,9]\n",
    "    alpha = 1.0\n",
    "    # Weights for each metric in the weighted sum approach\n",
    "\n",
    "    w1 = 0.8  # Weight for normalized variance of the gradient\n",
    "    w2 = .2# Weight for normalized gradient norm\n",
    "    w3 = 0.5 # Weight for normalized variance of the variance of the gradient\n",
    "\n",
    "    beta =0.5\n",
    "\n",
    "    max_scale = max(max_mean,max_var_grad)\n",
    "    min_scale = min(min_mean,min_var_grad)\n",
    "    print(f\"Scaled: [min: {min_scale:.2e}), max: {max_scale:.2e}]\")\n",
    "    # Now calculate scores and store them in the results\n",
    "    for set_idx in range(num_datasets):\n",
    "        i = set_idx\n",
    "        result = results_dict[f'dataset_{set_idx}']\n",
    "\n",
    "        # Extract necessary statistics for the current dataset\n",
    "        var_grad_mean = result[\"Mean(Var Grad)\"]\n",
    "        var_var_grad = result[\"Var(Var Grad)\"]\n",
    "        mean_grad = np.mean(result[\"Mean(Mean Grad)\"])\n",
    "        var_mean_grad = result[\"Var(Mean Grad)\"]\n",
    "        grad_norm = result[\"Gradient Norm\"]\n",
    "        X,y = result[\"dataset\"]\n",
    "        \n",
    "        mean_variance_of_gradient_normalized =result[\"Mean(Var Grad) [normalized]\"]\n",
    "        variance_of_variance_gradients_normalized =normalized_var_grad_norms[set_idx]\n",
    "        average_of_mean_gradients_normalized_abs =result[\"Mean(Mean Grad) [normalized]\"]\n",
    "        variance_of_mean_gradients_normalized =result[\"Var(Mean Grad) [normalized]\"]\n",
    "        normalized_grad_norm = result[\"Norm(Gradient Norm)\"]\n",
    "        \n",
    "        \n",
    "        # Normalize var_grad_mean and grad_norm\n",
    "        normalized_mean_variance_of_gradient = normalize_metric(var_grad_mean, min_scale,max_scale)\n",
    "        normalized_mean_gradient_score = normalize_metric(mean_grad,  min_scale,max_scale)\n",
    "        # normalized_mean_variance_of_gradient_normalized = normalize_metric(mean_variance_of_gradient_normalized, min_var_grad, max_var_grad)\n",
    "        normalized_var_var_grads = normalize_metric(var_var_grad,   min_var_var,max_var_var)\n",
    "        normalized_var_mean_gradient = normalize_metric(var_mean_grad, min_scale,max_scale)\n",
    "        variance_of_variance_gradients_normalized_scaled = normalize_metric(variance_of_variance_gradients_normalized, min_norm_var_grad,max_norm_var_grad)\n",
    "        # normalized_grad_norm_score = normalize_metric(grad_norm, 0, 1)\n",
    "        # normalized_mean_variance_of_gradient_normalized = normalize_metric(mean_of_variance_normalized_gradient, 0, 1)\n",
    "        # normalized_var_var_grads = normalize_metric(var_var_grad, 0, 1)\n",
    "        \n",
    "        # initial_score = (\n",
    "        #     w1 * (var_grad_mean)  # Reward high gradient variance\n",
    "        #     +w2 * (mean_grad)   # Reward high gradient norm\n",
    "        #     # +w2*(average_of_mean_gradients_normalized_abs)\n",
    "        #     -w3* (var_var_grad)  # Penalize high variance of variance of gradients\n",
    "        #      -beta * (var_mean_grad)  # Penalize high variance of mean gradients\n",
    "        # )\n",
    "        other_score = (\n",
    "            w1 * (normalized_mean_variance_of_gradient)  # Reward high gradient variance\n",
    "            +w2 * (normalized_mean_gradient_score)   # Reward high gradient norm\n",
    "            # +w2*(average_of_mean_gradients_normalized_abs)\n",
    "            -w3* (variance_of_variance_gradients_normalized)  # Penalize high variance of variance of gradients\n",
    "             -beta * (normalized_var_mean_gradient)  # Penalize high variance of mean gradients\n",
    "        )\n",
    "        initial_score = (\n",
    "            w1 * (normalized_mean_variance_of_gradient)  # Reward high gradient variance\n",
    "            +w2 * (normalized_mean_gradient_score)   # Reward high gradient norm\n",
    "            # +w2*(average_of_mean_gradients_normalized_abs)\n",
    "            -w3* (variance_of_variance_gradients_normalized_scaled)  # Penalize high variance of variance of gradients\n",
    "             -beta * (normalized_var_mean_gradient)  # Penalize high variance of mean gradients\n",
    "        )\n",
    "      \n",
    "        # Compute Replacement Score (focus on stable gradient variance but consider Var(Var Grad))\n",
    "        replacement_score = (\n",
    "            mean_variance_of_gradient_normalized * np.exp(-beta * (normalized_grad_norm - 1) ** 2) \n",
    "        )\n",
    "        # replacement_score = (normalized_grad_norm + mean_variance_of_gradient_normalized\n",
    "\n",
    "        # )\n",
    "\n",
    "        fine_tuning_score = 0.5 * normalized_grad_norm + 0.5 * mean_variance_of_gradient_normalized\n",
    "\n",
    "        # Store the scores in the results dictionary\n",
    "        results[f'dataset_{set_idx}'] = {\n",
    "            \"Initial Score\": initial_score,\n",
    "            \"Replacement Score\": replacement_score,\n",
    "            \n",
    "        }\n",
    "        if set_idx in to_print:\n",
    "            # Print the detailed summary statistics for each dataset\n",
    "            print(f\"(A{i}, b{i}) \")\n",
    "            # print(f\"    first state  : {X[0]}\")\n",
    "            # print(f\"    Var(Grad): {var_grad_mean:.2e}, Mean(Grad): {mean_grad:.2e} ->  Var(Grad)+Mean(Grad) = {var_grad_mean+mean_grad:.2e}\")\n",
    "            print(f\"    Raw Var(Grad): {var_grad_mean:.2e}\")\n",
    "            print(f\"    Var(Grad): {w1*normalized_mean_variance_of_gradient:.2e}, Mean(Grad): {w2*normalized_mean_gradient_score:.2e} ->  Var(Grad)+Mean(Grad) = {w1*normalized_mean_variance_of_gradient+w2*normalized_mean_gradient_score:.2e} [scaled]\")\n",
    "            # print(f\"    Var(Var): {variance_of_variance_gradients_normalized_scaled:.2e}, {variance_of_variance_gradients_normalized:.2e},  {normalized_var_var_grads:.2e}.  VarMean): {var_mean_grad:.2e}, {normalized_var_mean_gradient:.2e}\")\n",
    "            print(f\"    Var(Var) scaled: {variance_of_variance_gradients_normalized_scaled:.2e}, Var(Mean(Grad)): {normalized_var_mean_gradient:.2e} -> {-w3*variance_of_variance_gradients_normalized_scaled+ -beta*normalized_var_mean_gradient:.2e}\")\n",
    "            print(f\"    Initial Score: {initial_score:.3e}, other: {other_score:.3e}\\n\")\n",
    "\n",
    "            # print(f\"(A{i}, b{i}) \")\n",
    "            # print(f\"    Var(Grad): {var_grad_mean:.2e}, Normalized Var(Grad): {mean_variance_of_gradient_normalized:.2e}\")\n",
    "            # print(f\"    scaled vargrad: {normalized_mean_variance_of_gradient:.2e}, scaled meangrad: {normalized_mean_gradient_score:.2e}\")\n",
    "            # print(f\"    Mean(Grad): {mean_grad:.2e},  Mean(GradNorm): {average_of_mean_gradients_normalized_abs:.2e}\")\n",
    "            # print(f\"    Var(MeanGrad): {var_mean_grad:.2e},  VarMean(GradNorm): {variance_of_mean_gradients_normalized:.2e}\")\n",
    "            # # print(f\"    Gradient Norm: {grad_norm:.2e}, Normalized Grad Norm: {normalized_grad_norm_score:.2e}\")\n",
    "            # # print(f\"    first state  : {X[0]}\")\n",
    "            # # print(f\"    Var(NormGrad): {mean_variance_of_gradient_normalized:.2e}, Normalized Var(NormGrad): {normalized_grad_norm:.2e}\")\n",
    "            # print(f\"    Var(Var(Grad)): {var_var_grad:.2e}, Normalized  Var(Var(Grad)): {variance_of_variance_gradients_normalized:.2e}\")\n",
    "            \n",
    "            # print(f\"    Initial Score: {initial_score:.3e}, Replacement Score: {replacement_score:.3e}, other: {other_score:.3e}\\n\")\n",
    "\n",
    "\n",
    "    # Now select the best datasets for initial training, replacement, and fine-tuning\n",
    "    sorted_by_initial = sorted(results.items(), key=lambda x: x[1][\"Initial Score\"], reverse=True)\n",
    "    sorted_by_replacement = sorted(results.items(), key=lambda x: x[1][\"Replacement Score\"], reverse=True)\n",
    "\n",
    "    # print(sorted_by_replacement)\n",
    "    # Best for initial training\n",
    "    best_for_initial_training = sorted_by_initial[0]\n",
    "    \n",
    "    # Ensure the replacement is not the same as the initial\n",
    "    if sorted_by_replacement[0][0] == best_for_initial_training[0]:\n",
    "        best_for_replacement = sorted_by_replacement[1]  # Pick the second best\n",
    "    else:\n",
    "        best_for_replacement = sorted_by_replacement[0]\n",
    "\n",
    "\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Best Dataset for Initial Training: {best_for_initial_training[0]} with score: {best_for_initial_training[1]['Initial Score']:.2e}\")\n",
    "    print(f\"Best Dataset for Replacement: {best_for_replacement[0]} with score: {best_for_replacement[1]['Replacement Score']:.2e}\")\n",
    "\n",
    "    # Return the best datasets\n",
    "    return best_for_initial_training, best_for_replacement\n",
    "analyze_results(results_dict[0],num_datasets)\n",
    "\n",
    "# [-0.658#[ 0.21641777-0.05214687j  0.1271135 +0.6088834j  -0.41251817+0.46148774j  -0.0407903 -0.422778j  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results(results_dict[0],num_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg fidelity 1: 0.99174512347579 +- 0.0027397721582176553\n",
    "avg fidelity 3: 0.9912960633635521 +- 0.002645143835515139\n",
    "avg fidelity 3: 0.9983646594158782 +- 0.00047729121268886556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "a = 3*num_epochs // 4\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance of gradients = 1.337231e-01; Varince of mean gradient 1.186686e-02 IQR of Variance (25,75) = 8.437296e-02; min grad: 3.721845e-04; max grad: 5.292232e-01\n",
    "\n",
    "Variance of gradients = 1.493278e-01; Varince of mean gradient 8.839552e-03 IQR of Variance (25,75) = 8.805603e-02; min grad: 2.534003e-04; max grad: 3.528179e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "init_params:  [-0.88111204 -1.487593   -1.8190458  -0.01114495  0.76837647]\n",
      "final_cost: 6.172260e-01. avg fidelity 1: 6.810059e-01 +- 2.114353e-01\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U2_0/reservoirs_1/trotter_step_1/bath_False/testing_preopt/trainingset_A7_rep_A9.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcosts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. avg fidelity 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(test_results)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(test_results)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m b \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_ctrl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgate_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/reservoirs_1/trotter_step_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bath_False/testing_preopt/trainingset_A7_rep_A9.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Note 'rb' here, which means read binary\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     39\u001b[0m grads_per_epoch2 \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrads_per_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniforge3/envs/qualtran_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U2_0/reservoirs_1/trotter_step_1/bath_False/testing_preopt/trainingset_A7_rep_A9.pickle'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gate_idx =0\n",
    "trot = 1\n",
    "N_r = 1\n",
    "N_ctrl = 2\n",
    "#/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U2_3/reservoirs_1/trotter_step_9/bath_False/testing_preopt/trainingset_A4_rep_A7.pickle\n",
    "\n",
    "a = f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost3/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "with open(a, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "\n",
    "fig, axs = plt.subplots(3,1,figsize=(12, 16))\n",
    "ax = axs[0]\n",
    "# gate_idx =gate_idx + 1\n",
    "preopt_results = df['selected_indices'][0]\n",
    "print(preopt_results)\n",
    "costs = [i for i in df['costs'][0]]\n",
    "grads_per_epoch = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "max_grads_per_epoch = [max(arr) for arr in grads_per_epoch]\n",
    "testing_results = df['test_results'][0]\n",
    "fidelity = df['avg_fidelity'][0]\n",
    "# print(df['training_states'][0][0])\n",
    "print(\"init_params: \",df['init_params'][0])\n",
    "label1 =f\"A4_rep_A7\"\n",
    "\n",
    "gate = df['Gate'][0]\n",
    "decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "#print(decoded_qobj)\n",
    "test_results = [i for i in df['test_results'][0]]  # Add this line to read the test results if needed\n",
    "ax.hist(test_results, bins=50, alpha=0.5, density=True, label=label1)  # Normalize histogram\n",
    "\n",
    "print(f\"final_cost: {costs[-1]:4e}. avg fidelity 1: {np.mean(test_results):3e} +- {np.std(test_results):3e}\")\n",
    "\n",
    "b =f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/testing_preopt/trainingset_A7_rep_A9.pickle'\n",
    "\n",
    "with open(b, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "grads_per_epoch2 = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "max_grads_per_epoch2 = [max(arr) for arr in grads_per_epoch2]\n",
    "costs2 =  [i for i in df['costs'][0]]\n",
    "\n",
    "fidelity2 = df['avg_fidelity'][0]\n",
    "\n",
    "label2 =  f\"A7_rep_A9\"\n",
    "\n",
    "try:\n",
    "    test_results2 = [i for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "except KeyError:\n",
    "    test_results2 = [i for i in df['fidelities'][0]] \n",
    "\n",
    "print(f\"final_cost: {costs2[-1]:4e}. avg fidelity 2: {np.mean(test_results2):3e} +- {np.std(test_results2):3e}\")\n",
    "ax.hist(test_results2, bins=50, alpha=0.5, density=True, label = label2)\n",
    "\n",
    "\n",
    "c =f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/testing_preopt/trainingset_A4_rep_A9.pickle'\n",
    "\n",
    "with open(c, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "grads_per_epoch3 = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "max_grads_per_epoch3 = [max(arr) for arr in grads_per_epoch3]\n",
    "costs3 =  [i for i in df['costs'][0]]\n",
    "\n",
    "fidelity3 = df['avg_fidelity'][0]\n",
    "\n",
    "label3 =  f\"A4_rep_A9\"\n",
    "\n",
    "try:\n",
    "    test_results3 = [i for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "except KeyError:\n",
    "    test_results3 = [i for i in df['fidelities'][0]] \n",
    "\n",
    "print(f\"final_cost: {costs3[-1]:4e}. avg fidelity 3: {np.mean(test_results3):3e} +- {np.std(test_results3):3e}\")\n",
    "\n",
    "\n",
    "ax.hist(test_results3, bins=50, alpha=0.5, density=True, label = label3)\n",
    "\n",
    "\n",
    "# Define paths to the additional data files\n",
    "d = f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/testing_preopt/trainingset_A9_rep_A7.pickle'\n",
    "e = f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/testing_preopt/trainingset_A8_rep_A7.pickle'\n",
    "# Load and plot the fourth dataset\n",
    "with open(d, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "label4 = f\"A9_rep_A7\"\n",
    "costs4 = [i for i in df['costs'][0]]\n",
    "grads_per_epoch4 = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "max_grads_per_epoch4 = [max(arr) for arr in grads_per_epoch4]\n",
    "test_results4 = df['testing_results'][0]\n",
    "ax.hist(test_results4, bins=50, alpha=0.5, density=True,  label=label4)\n",
    "\n",
    "print(f\"final_cost: {costs4[-1]:4e}. avg fidelity 4: {np.mean(test_results4):3e} +- {np.std(test_results4):3e}\")\n",
    "\n",
    "# Load and plot the fifth dataset\n",
    "with open(e, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "label5 = f\"A8_rep_A7\"\n",
    "costs5 = [i for i in df['costs'][0]]\n",
    "grads_per_epoch5 = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "max_grads_per_epoch5 = [max(arr) for arr in grads_per_epoch5]\n",
    "test_results5 = df['testing_results'][0]\n",
    "ax.hist(test_results5, bins=50, alpha=0.5, density=True,  label=label5)\n",
    "\n",
    "print(f\"final_cost: {costs5[-1]:4e}. avg fidelity 5: {np.mean(test_results5):3e} +- {np.std(test_results5):3e}\")\n",
    "\n",
    "\n",
    "# ax.set_ylim([0.0,1.0])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# Plot costs for all datasets\n",
    "axs[1].plot(range(1, len(costs) + 1), costs, label=label1)\n",
    "axs[1].plot(range(1, len(costs2) + 1), costs2, label=label2)\n",
    "axs[1].plot(range(1, len(costs3) + 1), costs3, label=label3)\n",
    "axs[1].plot(range(1, len(costs4) + 1), costs4, label=label4)\n",
    "axs[1].plot(range(1, len(costs5) + 1), costs5, label=label5)\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot max gradient per epoch for all datasets\n",
    "axs[2].plot(range(1, len(max_grads_per_epoch) + 1), max_grads_per_epoch, label=label1)\n",
    "axs[2].plot(range(1, len(max_grads_per_epoch2) + 1), max_grads_per_epoch2, label=label2)\n",
    "axs[2].plot(range(1, len(max_grads_per_epoch3) + 1), max_grads_per_epoch3, label=label3)\n",
    "axs[2].plot(range(1, len(max_grads_per_epoch4) + 1), max_grads_per_epoch4, label=label4)\n",
    "axs[2].plot(range(1, len(max_grads_per_epoch5) + 1), max_grads_per_epoch5, label=label5)\n",
    "axs[2].legend()\n",
    "\n",
    "# plt.show()/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost3/U2_0/reservoirs_1/trotter_step_1/bath_False/data_run_0.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grads_per_epoch3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_idx =9\n",
    "trot = 15\n",
    "N_r = 1\n",
    "N_ctrl = 2\n",
    "#/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/U2_3/reservoirs_1/trotter_step_9/bath_False/testing_preopt/trainingset_A4_rep_A7.pickle\n",
    "# a = f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_{N_ctrl}/reservoirs_{N_r}/trotter_step_{trot}/10_training_states_no_opt/fixed_params0/1.0K/test22/U{N_ctrl}_{gate_idx}/data_run_0.pickle'\n",
    "a = f'/Users/sophieblock/QRCCapstone/digital_results_trainable_global/trainsize_20_optimized_by_cost/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/testing_preopt/data_run_0.pickle'\n",
    "with open(a, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "# gate_idx =gate_idx + 1\n",
    "costs = [i for i in df['costs'][0]]\n",
    "grads_per_epoch = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "testing_results = df['test_results'][0]\n",
    "fidelity = df['avg_fidelity'][0]\n",
    "# print(df['training_states'][0][0])\n",
    "print(\"init_params: \",df['init_params'][0])\n",
    "label1 =f\"test 22: {fidelity:6e}\"\n",
    "\n",
    "gate = df['Gate'][0]\n",
    "decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "#print(decoded_qobj)\n",
    "test_results = [i for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "ax.hist(test_results, bins=50, alpha=0.5,label = label1)\n",
    "print(f\"final_cost: {costs[-1]:4e}. avg fidelity 1: {np.mean(test_results):3e} +- {np.std(test_results):3e}\")\n",
    "\n",
    "trot = 10\n",
    "c =f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_20_optimized_by_cost/0/U{N_ctrl}_{gate_idx}/reservoirs_1/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "\n",
    "with open(c, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "costs3 =  [i for i in df['costs'][0]]\n",
    "\n",
    "fidelity3 = df['avg_fidelity'][0]\n",
    "\n",
    "label3 =  f\"digital (trots: {trot}): {fidelity3:6e}\"\n",
    "\n",
    "try:\n",
    "    test_results3 = [i for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "except KeyError:\n",
    "    test_results3 = [i for i in df['fidelities'][0]] \n",
    "\n",
    "print(f\"final_cost: {costs3[-1]:4e}. avg fidelity 3: {np.mean(test_results3):3e} +- {np.std(test_results3):3e}\")\n",
    "\n",
    "\n",
    "ax.hist(test_results3, bins=50, alpha=0.5,label = label3)\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(costs)+1),costs, label = label1)\n",
    "\n",
    "plt.plot(range(1,len(costs3)+1),costs3, label = label3)\n",
    "#plt.plot(range(1,len(costs3)+1),costs3)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [el for el in testing_results if el <0.0]\n",
    "print(len(temp))\n",
    "\n",
    "temp = [el for el in testing_results3 if el <0.0]\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [el for el in test_results if el <0.0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results3\n",
    "temp = [el for el in test_results3 if el <0.0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_idx = 1\n",
    "trot = 2\n",
    "N_r = 1\n",
    "N_ctrl = 1\n",
    "selected_sets = [0]  # List of dataset indices\n",
    "\n",
    "def calculate_iqr(data):\n",
    "    \"\"\"\n",
    "    Calculate the Interquartile Range (IQR) of the input data.\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data, 85) - np.percentile(data, 15)\n",
    "    return iqr\n",
    "tests = ['test22','test53','test31','test50', 'test94','test121','test44','test125']\n",
    "# Initialize the paths to the datasets (as a loop)\n",
    "data_files = [f'/Users/sophieblock/QRCCapstone/param_initialization/analog_results/Nc_{N_ctrl}/reservoirs_{N_r}/trotter_step_{trot}/10_training_states_no_opt/fixed_params0/1.0K/{test_key}/U{N_ctrl}_{gate_idx}/data_run_0.pickle'\n",
    "                for test_key in tests]\n",
    "\n",
    "def calculate_gradient_stats(gradients):\n",
    "    mean_grad = jnp.mean(gradients, axis=0)\n",
    "    mean_grad_squared = jnp.mean(gradients**2, axis=0)\n",
    "    var_grad = mean_grad_squared - mean_grad**2\n",
    "    return mean_grad, var_grad\n",
    "\n",
    "threshold = 10e-14\n",
    "# Load the datasets and extract relevant data\n",
    "data_results = {}\n",
    "for i,file_path in enumerate(data_files):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Extract necessary information from the dataset\n",
    "    costs = [float(i) for i in data['costs'][0]]\n",
    "    grads_per_epoch = [np.array(i) for i in data['grads_per_epoch'][0]]\n",
    "    init_grads = grads_per_epoch[0]\n",
    "    \n",
    "\n",
    "    # print(init_grads.shape, np.abs((init_grads)),init_grads)\n",
    "    testing_results = data['testing_results'][0]\n",
    "    fidelity = data['avg_fidelity'][0]\n",
    "    # avg_fidelity = np.mean(fidelity)\n",
    "    opt_lr = data['opt_lr'][0]\n",
    "    trainin_states = data['training_states'][0][0]\n",
    "    \n",
    "\n",
    "    var_nonzero_grads = []\n",
    "    for grads in grads_per_epoch:\n",
    "        \n",
    "\n",
    "        nz_grads = [res for res in grads if np.abs(res) > threshold]\n",
    "        # if tests[i] == 'test50':\n",
    "        #     print(nz_grads)\n",
    "        var_nonzero_grads.append(np.var(nz_grads))\n",
    "    var_grad = [np.var(grads) for grads in grads_per_epoch]\n",
    "    min_grad = [np.min(np.abs(grads)) for grads in grads_per_epoch]\n",
    "    max_grad = [np.max(np.abs(grads)) for grads in grads_per_epoch]\n",
    "\n",
    "    iqr_var_grad = calculate_iqr(jnp.array(init_grads))\n",
    "    mean_fidelity = round(float(np.mean(testing_results)), 5)\n",
    "    std_fidelity = round(float(np.std(testing_results)), 5)\n",
    "    # Print the results\n",
    "    print(f\"\\nAverage Fidelity ({tests[i]}): {mean_fidelity} ± {std_fidelity}\")\n",
    "    # print(f\"Initial Variance ({tests[i]}): {np.var(init_grads):.5e}, IQR: {iqr_var_grad:.5e}, Min Gradient: {np.min(np.abs(init_grads)):.5e}, Max Grad: {np.max(np.abs(init_grads)):.5e}\")\n",
    "\n",
    "    # print(f\"From test: {np.mean(data['var_grad'][0]):.5e}, IQR: {calculate_iqr(data['var_grad'][0]):.5e}, Min Gradient: {data['min_grad'][0]:.5e}, Max Grad: {data['max_grad'][0]:.5e}\")\n",
    "\n",
    "    \n",
    "    mean_grad, var_grad_out = calculate_gradient_stats(init_grads)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    data_results[tests[i]] = {\n",
    "        'costs': costs,\n",
    "        'var_grad': var_grad,\n",
    "        'var_nonzero_grads':var_nonzero_grads,\n",
    "        'min_grad': min_grad,\n",
    "        'max_grad': max_grad,\n",
    "        'avg_fidelity':fidelity,\n",
    "        'testing_results': testing_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now plot the results\n",
    "fig, ax = plt.subplots(5, 1, figsize=(8, 20))\n",
    "\n",
    "# Loop over datasets and add each to the plots\n",
    "for test_key, dataset in data_results.items():\n",
    "    # Dynamically label the dataset for each selected set\n",
    "    avg_fidelity = dataset['avg_fidelity']\n",
    "    label = f'{test_key} [ {avg_fidelity:.5f}]'\n",
    "    \n",
    "    # Plot testing results (histograms)\n",
    "    ax[0].hist([float(i) for i in dataset['testing_results']], bins=50, alpha=0.5, label=f'{label}')\n",
    "    \n",
    "    # Plot costs per epoch\n",
    "    ax[1].plot(range(1, len(dataset['costs']) + 1), dataset['costs'], label=label)\n",
    "    \n",
    "    # Plot variance of gradients per epoch\n",
    "    ax[2].plot(range(1, len(dataset['var_nonzero_grads']) + 1), dataset['var_nonzero_grads'], label=label)\n",
    "    ax[2].set_yscale('log')\n",
    "    # Plot minimum absolute value of gradients per epoch\n",
    "    ax[3].plot(range(1, len(dataset['min_grad']) + 1), dataset['min_grad'],linewidth=1, label=label)\n",
    "    ax[3].set_yscale('log')\n",
    "    # Plot maximum absolute value of gradients per epoch\n",
    "    ax[4].plot(range(1, len(dataset['max_grad']) + 1), dataset['max_grad'], label=label)\n",
    "    ax[4].set_yscale('log')\n",
    "\n",
    "# Customize the plot titles, labels, and legends\n",
    "ax[0].set_title('Test Results (Fidelity)')\n",
    "ax[0].set_xlabel('Fidelity')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend(fontsize=12)\n",
    "\n",
    "ax[1].set_title('Costs per Epoch')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Cost')\n",
    "ax[1].legend(fontsize=12)\n",
    "\n",
    "ax[2].set_title('Variance of Gradients per Epoch')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Variance')\n",
    "ax[2].legend(fontsize=12)\n",
    "\n",
    "ax[3].set_title('Minimum Absolute Value of Gradients per Epoch')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('Min Abs Gradient')\n",
    "ax[3].set_yscale('log')  # Log scale as before\n",
    "ax[3].legend(fontsize=12)\n",
    "\n",
    "ax[4].set_title('Maximum Absolute Value of Gradients per Epoch')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Max Abs Gradient')\n",
    "ax[4].legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_idx =1\n",
    "trot = 7\n",
    "N_r = 1\n",
    "N_ctrl = 2\n",
    "for gate_idx in range(0,20):\n",
    "    print(f\"U{N_ctrl}_{gate_idx}\")\n",
    "    a = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_optimized_by_cost3/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "    with open(a, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "    costs = [float(i) for i in df['costs'][0]]\n",
    "    grads_per_epoch = [np.array(i) for i in df['grads_per_epoch'][0]]\n",
    "    testing_results = df['testing_results'][0]\n",
    "    fidelity = 1 - df['avg_fidelity'][0]\n",
    "\n",
    "    label1 =f\"new lr: {df['opt_lr'][0]}\"\n",
    "    gate = df['Gate'][0]\n",
    "    decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "    #print(decoded_qobj)\n",
    "    test_results = [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "    ax.hist(test_results, bins=50, alpha=0.5,label = label1)\n",
    "    print(f\"avg fidelity 1: {np.mean(test_results)} +- {np.std(test_results)}\")\n",
    "\n",
    "    # b = f'/Users/sophieblock/QRCCapstone/analog_results_trainable_global/trainsize_10_optimized_by_cost/0/U{N_ctrl}_{gate_idx}/reservoirs_{N_r}/trotter_step_{trot}/bath_False/data_run_0.pickle'\n",
    "    # try:\n",
    "    #     with open(b, 'rb') as f:  # Note 'rb' here, which means read binary\n",
    "    #         df = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    #     costs2 = [float(i) for i in df['costs'][0]]\n",
    "    #     testing_results2 = df['testing_results'][0]\n",
    "    #     fidelity2 = 1 - df['avg_fidelity'][0]\n",
    "\n",
    "    #     label2 = f\"old lr: {df['opt_lr'][0]}\"\n",
    "    #     gate = df['Gate'][0]\n",
    "    #     decoded_qobj = pickle.loads(base64.b64decode(gate.encode('utf-8')))\n",
    "\n",
    "    #     test_results2 = [float(i) for i in df['testing_results'][0]]  # Add this line to read the test results if needed\n",
    "    #     ax.hist(test_results2, bins=50, alpha=0.5,label = label2)\n",
    "    #     print(f\"avg fidelity 3: {np.mean(test_results2)} +- {np.std(test_results2)}\")\n",
    "    # except FileNotFoundError:\n",
    "    #     pass\n",
    "    \n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "print(a[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
