{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "\n",
    "from jax import numpy as np\n",
    "from qiskit.circuit.library import *\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pennylane.wires import Wires\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import base64\n",
    "\n",
    "import optax\n",
    "from jax import config\n",
    "import os\n",
    "import jax\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad, vmap\n",
    "import pennylane.numpy as pnp\n",
    "#os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "has_jax = True\n",
    "diable_jit = True\n",
    "config.update('jax_disable_jit', diable_jit)\n",
    "#config.parse_flags_with_absl()\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "os.environ['JAX_TRACEBACK_FILTERING'] = 'off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import iqr\n",
    "def is_valid_pickle_file(file_path):\n",
    "    \"\"\"Check if a pickle file is valid.\"\"\"\n",
    "    try:\n",
    "        if file_path.exists() and file_path.stat().st_size > 0:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                try:\n",
    "                    pickle.load(f)\n",
    "                    return True\n",
    "                except EOFError:\n",
    "                    print(f\"File {file_path} is corrupted.\")\n",
    "                    return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "#print(\"all_gradients shape:\", all_gradients.shape)\n",
    "def calculate_gradient_stats(gradients):\n",
    "    mean_grad = jnp.mean(gradients, axis=0)\n",
    "    mean_grad_squared = jnp.mean(gradients**2, axis=0)\n",
    "    var_grad = mean_grad_squared - mean_grad**2\n",
    "    return mean_grad, var_grad\n",
    "\n",
    "def calculate_inv_qfim(eigen_vals, eigen_vecs, n_params):\n",
    "    cutoff_eigvals=10**-12\n",
    "    eigvals_inv=np.zeros(n_params)\n",
    "    #invert eigenvalues if they are above threshold, else set to zero\n",
    "    for i in range(n_params):\n",
    "        if(eigen_vals[i]<cutoff_eigvals):\n",
    "            eigvals_inv[i]=0 #inverted eigenvalues with cutoff of smallest eigenvalues set to zero\n",
    "        else:\n",
    "            eigvals_inv[i]=1/eigen_vals[i]\n",
    "    qfi_inv_matrix=np.dot(eigen_vecs,np.dot(np.diag(eigvals_inv),np.transpose(np.conjugate(eigen_vecs))))\n",
    "    return qfi_inv_matrix\n",
    "\n",
    "def calculate_gradient_variance(gradients):\n",
    "    grad_matrix = jnp.array(gradients)\n",
    "    mean_grad = jnp.mean(grad_matrix, axis=0)\n",
    "    var_grad = jnp.mean((grad_matrix - mean_grad) ** 2, axis=0)\n",
    "    return var_grad\n",
    "\n",
    "def process_data_combined(df, threshold, by_test, N_R, trot, print_bool, weight_median=0.5, weight_iqr=0.5):\n",
    "    \"\"\"Load and process data from a pickle file.\"\"\"\n",
    "    \n",
    "    max_eigvals = []\n",
    "    trace_eigvals = []\n",
    "    min_eigvals = []\n",
    "    var_eigval = []\n",
    "    ranks = []\n",
    "    var_log_eigval, norm_trace_eigvals = [],[]\n",
    "    ratios = []\n",
    "    counts = []\n",
    "    entropies = []\n",
    "    qfim_eigval_list = []\n",
    "    redundancies = []\n",
    "    for fixed_params_dict in df.keys():\n",
    "        for test in df[fixed_params_dict].keys():\n",
    "            qfim_eigvals = df[fixed_params_dict][test]['qfim_eigvals']\n",
    "            qfim_eigval_list.append(qfim_eigvals)\n",
    "            n_params = len(qfim_eigvals)\n",
    "            nonzero_eigvals = qfim_eigvals[qfim_eigvals > threshold]\n",
    "            counts.append(len(nonzero_eigvals))\n",
    "            num_nonzero_eigvals = len(nonzero_eigvals)\n",
    "            ranks.append(num_nonzero_eigvals)\n",
    "            ratios.append(num_nonzero_eigvals / n_params)\n",
    "            redundancies.append((n_params-num_nonzero_eigvals)/n_params)\n",
    "            var_eigval.append(np.var(nonzero_eigvals) if nonzero_eigvals.size > 0 else np.nan)\n",
    "            var_log_eigval.append(np.var(np.log10(nonzero_eigvals)) if nonzero_eigvals.size > 0 else np.nan)\n",
    "            trace_eigvals.append(np.sum(qfim_eigvals))\n",
    "            norm_trace_eigvals.append(np.sum(qfim_eigvals)/len(nonzero_eigvals))\n",
    "            max_eigvals.append(np.max(nonzero_eigvals) if nonzero_eigvals.size > 0 else np.nan)\n",
    "            min_eigvals.append(np.min(nonzero_eigvals) if nonzero_eigvals.size > 0 else np.nan)\n",
    "            # print(df[fixed_params_dict][test].keys())\n",
    "            entropies.append(df[fixed_params_dict][test]['entropies'])\n",
    "    \n",
    "    mean_trace = np.mean(trace_eigvals)\n",
    "    mean_norm_trace_eigvals = np.mean(norm_trace_eigvals)\n",
    "    mean_var_eigval = np.mean(var_eigval)\n",
    "    mean_var_log_eigval = np.mean(var_log_eigval)\n",
    "    mean_entropy = np.mean(entropies)\n",
    "    # Calculate the median and IQR\n",
    "    median_trace = np.median(trace_eigvals)\n",
    "    median_var_eigval = np.median(var_eigval)\n",
    "    median_var_log_eigval = np.median(var_log_eigval)\n",
    "    \n",
    "    iqr_trace = iqr(trace_eigvals, rng= (10,90))\n",
    "    iqr_var_eigval = iqr(var_eigval,  rng= (10,90))\n",
    "    iqr_var_log_eigval = iqr(var_log_eigval, rng= (10,90))\n",
    "    \n",
    "    weighted_avg_trace = weight_median * median_trace + weight_iqr * iqr_trace\n",
    "    weighted_avg_var_eigval = weight_median * median_var_eigval + weight_iqr * iqr_var_eigval\n",
    "    weighted_avg_var_log_eigval = weight_median * median_var_log_eigval + weight_iqr * iqr_var_log_eigval\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'mean_trace_eigvals': np.mean(trace_eigvals),\n",
    "        'mean_entropy':mean_entropy,\n",
    "        'quantum_dim': np.mean(ranks),\n",
    "        'ratios':np.mean(ratios),\n",
    "        'redundancies':np.mean(redundancies),\n",
    "        'mean_norm_trace_eigvals':mean_norm_trace_eigvals,\n",
    "        'median_trace_eigvals': median_trace,\n",
    "        'iqr_trace_eigvals': iqr_trace,\n",
    "        'weighted_avg_trace_eigvals': weighted_avg_trace,\n",
    "        'mean_var_eigval': np.mean(var_eigval),\n",
    "        'median_var_eigval': median_var_eigval,\n",
    "        'iqr_var_eigval': iqr_var_eigval,\n",
    "        'weighted_avg_var_eigval': weighted_avg_var_eigval,\n",
    "         'mean_var_log_eigval': np.mean(var_log_eigval),\n",
    "        'median_var_log_eigval': median_var_log_eigval,\n",
    "        'iqr_var_log_eigval': iqr_var_log_eigval,\n",
    "        'weighted_avg_var_log_eigval': weighted_avg_var_log_eigval,\n",
    "        'all_qfim_eigvals':qfim_eigval_list,\n",
    "    }\n",
    "\n",
    "threshold = 1e-14\n",
    "by_test = False\n",
    "N_ctrls = [1, 2]\n",
    "base_state = 'GHZ_state/1xK'\n",
    "base_path = Path('/Users/sophieblock/QRCCapstone/')\n",
    "model_type = 'gate_model_DQFIM'\n",
    "all_data = []\n",
    "num_states_sampled = 50\n",
    "\n",
    "# Function to check if a file is a valid pickle file (you might have this already)\n",
    "def is_valid_pickle_file(file_path):\n",
    "    return file_path.exists() and file_path.is_file()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Your variables\n",
    "threshold = 1e-14\n",
    "by_test = False\n",
    "N_ctrls = [1, 2]\n",
    "base_state = 'GHZ_state/1xK'  # You have a '1xK' directory in the structure\n",
    "base_path = Path('/Users/sophieblock/QRCCapstone/')\n",
    "model_type = 'gate_model_DQFIM'\n",
    "all_data = []\n",
    "num_states_sampled = 50\n",
    "\n",
    "# Function to check if a file is a valid pickle file\n",
    "def is_valid_pickle_file(file_path):\n",
    "    return file_path.exists() and file_path.is_file()\n",
    "\n",
    "# Function to check and skip hidden system files like .DS_Store\n",
    "def is_hidden_file(file_name):\n",
    "    return file_name.startswith('.')\n",
    "\n",
    "for N_ctrl in N_ctrls:\n",
    "    \n",
    "    # Model path includes the '1xK' folder after the 'L_{num_states_sampled}'\n",
    "    model_path = base_path / 'QFIM_traced_final_results' / model_type / f'Nc_{N_ctrl}' / f'L_{num_states_sampled}' / '1xK'\n",
    "    \n",
    "    # Print the model path being processed\n",
    "    print(f\"Processing model path: {model_path}\")\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Model path not found: {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    for Nr in sorted(os.listdir(model_path)):\n",
    "        if is_hidden_file(Nr):  # Skip hidden files\n",
    "            continue\n",
    "        Nr_path = model_path / Nr\n",
    "        if not Nr_path.is_dir():\n",
    "            print(f\"Nr path not a directory: {Nr_path}\")\n",
    "            continue\n",
    "\n",
    "        for trotter_step in sorted(os.listdir(Nr_path)):\n",
    "            if is_hidden_file(trotter_step):  # Skip hidden files\n",
    "                continue\n",
    "            trotter_step_path = Nr_path / trotter_step\n",
    "            if not trotter_step_path.is_dir():\n",
    "                print(f\"Trotter step path not a directory: {trotter_step_path}\")\n",
    "                continue\n",
    "\n",
    "            # Now correctly pointing to nested L_{num_states_sampled} inside each trotter_step\n",
    "            folder_gate = trotter_step_path / f'L_{num_states_sampled}'  \n",
    "\n",
    "            data_file = folder_gate / 'data_pi_range.pickle'\n",
    "            \n",
    "            # Check if the pickle file exists and is valid\n",
    "            if is_valid_pickle_file(data_file):\n",
    "                print(f\"Reading data from: {data_file}\")\n",
    "                with open(data_file, 'rb') as f:\n",
    "                    df = pickle.load(f)\n",
    "                    trotter_step_num = int(trotter_step.split('_')[-1])\n",
    "                    processed_data = process_data_combined(df, threshold, by_test, int(Nr.split('_')[-1]), trotter_step_num, False)\n",
    "                    processed_data.update({\n",
    "                        'N_ctrl': N_ctrl,\n",
    "                        'N_reserv': int(Nr.split('_')[-1]),\n",
    "                        'Trotter_Step': trotter_step_num\n",
    "                    })\n",
    "                    all_data.append(processed_data)\n",
    "            else:\n",
    "                print(f\"Data file not found or invalid: {data_file}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "if all_data:\n",
    "    df_all = pd.DataFrame(all_data)\n",
    "    print(df_all)\n",
    "else:\n",
    "    print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-12\n",
    "\n",
    "# Apply the condition where values below the threshold are set to 0, and then calculate the mean\n",
    "df_all['avg_qfim_eigvals'] = df_all['all_qfim_eigvals'].apply(\n",
    "    lambda x: np.mean(np.where(np.array(x) < threshold, 0, np.array(x)), axis=0)\n",
    ")\n",
    "# Choose a scaling factor alpha\n",
    "alpha = 0.75  # sqrt, adjust this between 0 and 1 depending on how much scaling you want\n",
    "\n",
    "# Apply the power law transformation\n",
    "df_all['power_scaled_avg_qfim_eigvals'] = df_all['avg_qfim_eigvals'].apply(\n",
    "    lambda avg_vals: np.array(avg_vals) ** alpha\n",
    ")\n",
    "df_all['power_scaled_avg_qfim_eigvals_nonzero'] = df_all['avg_qfim_eigvals'].apply(\n",
    "    lambda avg_vals: np.array([val for val in avg_vals if val > threshold]) ** alpha\n",
    ")\n",
    "# # Now calculate the variance of the power-scaled values and store it in a new column\n",
    "df_all['var_power_scaled_avg_qfim_eigvals'] = df_all['power_scaled_avg_qfim_eigvals'].apply(\n",
    "    lambda scaled_vals: np.var(scaled_vals)\n",
    ")\n",
    "df_all['var_power_scaled_avg_qfim_eigvals_nonzero'] = df_all['power_scaled_avg_qfim_eigvals_nonzero'].apply(\n",
    "    lambda scaled_vals: np.var(scaled_vals)\n",
    ")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res3 = df_all[df_all['N_reserv'] == 3]\n",
    "df_res3 = df_res3[df_res3['N_ctrl'] == 2]\n",
    "df_res3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "N_ctrl = 2\n",
    "\n",
    "metric_keys = ['weighted_avg_trace_eigvals', 'mean_entropy',  'redundancies','var_power_scaled_avg_qfim_eigvals', 'var_power_scaled_avg_qfim_eigvals_nonzero']\n",
    "metrics_info = {\n",
    "    'weighted_avg_trace_eigvals': {\n",
    "        'title': f'Weighted Avg ${{Tr}}[\\mathcal{{Q}}]$ [$N_C = {N_ctrl}$]',\n",
    "        'cbar': ''\n",
    "    },\n",
    "    'quantum_dim': {\n",
    "        'title': f'$G_C$ (Rank) [$N_C = {N_ctrl}$]',\n",
    "        'cbar': ''\n",
    "    },\n",
    "    'mean_entropy': {\n",
    "        'title': f'VN Entropy [$N_C = {N_ctrl}$]',\n",
    "        'cbar': ''\n",
    "    },\n",
    "    'redundancies': {\n",
    "        'title': f'$R$ (Redundancy)',\n",
    "        'cbar': ''\n",
    "    },\n",
    "    'var_power_scaled_avg_qfim_eigvals': {\n",
    "        'title': f'Variance of QFIM Eigenvalues Power Scaled [$N_C = {N_ctrl}$] (iqr)',\n",
    "        'cbar': ''\n",
    "    },\n",
    "    'var_power_scaled_avg_qfim_eigvals_nonzero': {\n",
    "        'title': f'Variance of QFIM Eigenvalues (Non-Zero) Power Scaled [$N_C = {N_ctrl}$]',\n",
    "        'cbar': ''\n",
    "    }\n",
    "}\n",
    "\n",
    "resies = [1,2,3]\n",
    "resies = [1, 2, 3,4,5,6]\n",
    "\n",
    "for metric_key in metric_keys:\n",
    "    # Retrieve title and color bar from the dictionary\n",
    "    metric_title = metrics_info[metric_key]['title']\n",
    "    metric_cbar = metrics_info[metric_key]['cbar']\n",
    "    # Filter the data for the current N_ctrl\n",
    "    df_filtered = df_all[df_all['N_ctrl'] == N_ctrl][['N_reserv', 'Trotter_Step', f'{metric_key}']]\n",
    "    \n",
    "    if N_ctrl == 1:\n",
    "        trots = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    elif N_ctrl == 2:\n",
    "        trots = [1, 3,4,5, 6,7, 8,9, 10, 12, 14, 16, 18, 20, 22, 24]\n",
    "        trots = np.arange(1, 45, 1)\n",
    "    else:\n",
    "        trots = np.arange(1, 45, 1)\n",
    "    \n",
    "    df_filtered = df_filtered[df_filtered['Trotter_Step'].isin(trots)]\n",
    "    # df_filtered = df_filtered[df_filtered['N_reserv'].isin(resies)]\n",
    "    \n",
    "    # Aggregate the data by Trotter_Step and N_reserv\n",
    "    df_heatmap = df_filtered.groupby(['Trotter_Step', 'N_reserv']).agg({metric_key: 'mean'}).reset_index()\n",
    "    \n",
    "    # Pivot the data to create a heatmap\n",
    "    heatmap_pivot = df_heatmap.pivot(index='Trotter_Step', columns='N_reserv', values=metric_key)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    # Disable the default color bar by setting `cbar=False`\n",
    "    sns.heatmap(heatmap_pivot, ax=ax, cmap='magma', annot=True, fmt=\".2f\", annot_kws={\"size\": 22,\"weight\":'bold',}, cbar=False)\n",
    "\n",
    "    # Create a divider for the existing axes instance\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.05)  # Adjust pad for proximity\n",
    "\n",
    "    # Create a manual color bar\n",
    "    cbar = fig.colorbar(ax.collections[0], cax=cax)\n",
    "    cbar.set_label(metric_cbar, rotation=0, labelpad=20, fontsize=22, weight=\"bold\")\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(metric_title, fontsize=30, pad=20)\n",
    "    ax.set_ylabel('$d$', labelpad=30, fontsize=28, rotation=0)\n",
    "    ax.set_xlabel('', fontsize=20)\n",
    "    ax.tick_params(axis='y', labelrotation=0, labelsize=18)\n",
    "    ax.tick_params(axis='x', labelsize=18)\n",
    "    ax.set_xticklabels([f'$N_R = {rc+1}$' for rc in range(len(ax.get_xticklabels()))], fontweight='bold', fontsize=28, rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jaxenv)",
   "language": "python",
   "name": "jaxenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
