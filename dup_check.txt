def process_new_files(base_path, gate_prefixes, reservoir_counts, trots, cached_data, processed_files, N_ctrl):
    """
    Process new files (pickles) that haven't been processed before, 
    or if the folder's file count differs from the cached 'num_data_runs',
    re-check all runs to find the best fidelity. Then update the cache 
    with the best result for that (N_ctrl, gate, reservoir_count, trotter_step).

    Parameters
    ----------
    base_path : str
        Root directory where the gate folders are located.
    gate_prefixes : list of str
        List of gate prefixes (e.g. ['U2'] for 2-qubit gates).
    reservoir_counts : list of int
        Which reservoir counts to consider.
    trots : list of int
        Which Trotter steps to consider.
    cached_data : dict
        Dictionary storing previously processed results in the structure:
        {
          N_ctrl: {
            gate: {
              reservoir_count: {
                trotter_step: [ { 'fidelity': ..., 'test_results': ..., ... }, ]
              }
            }
          }
        }
    processed_files : set
        A set of file paths that have already been processed.
    N_ctrl : int
        Number of control qubits for this run.

    Returns
    -------
    cached_data, processed_files : (dict, set)
        Updated cache dictionary and processed_files set.
    """
    for gate_prefix in gate_prefixes:
        # Find folder names that match the gate prefix
        for folder_name in sorted(os.listdir(base_path)):
            if folder_name.startswith(gate_prefix + "_"):
                gate = folder_name

                # Ensure N_ctrl is in cached_data
                if N_ctrl not in cached_data:
                    cached_data[N_ctrl] = {}

                # For each "bath_True" or "bath_False" subfolder
                for bath_status in ['bath_True', 'bath_False']:

                    # For each "reservoirs_*" subfolder
                    subfolder_path = os.path.join(base_path, gate)
                    if not os.path.exists(subfolder_path):
                        continue
                    for subfolder in sorted(os.listdir(subfolder_path), key=extract_last_number):
                        if 'reservoirs_' in subfolder:
                            reservoir_count = extract_last_number(subfolder)
                            if int(reservoir_count) not in reservoir_counts:
                                continue

                            # For each "trotter_step_*" subfolder
                            trotter_parent = os.path.join(subfolder_path, subfolder)
                            if not os.path.exists(trotter_parent):
                                continue
                            for trotter_folder in sorted(os.listdir(trotter_parent), key=extract_last_number):
                                if 'trotter_step_' in trotter_folder:
                                    trotter_step = extract_last_number(trotter_folder)
                                    if trotter_step not in trots:
                                        continue

                                    trotter_path = os.path.join(trotter_parent, trotter_folder, bath_status)
                                    if not os.path.exists(trotter_path):
                                        continue

                                    files_in_folder = [f for f in os.listdir(trotter_path) if not f.startswith('.')]
                                    print(f"{gate}-- N_c={N_ctrl}, N_R={reservoir_count}, T={trotter_step}, num files(runs)= {len(files_in_folder)}")

                                    # Make sure the nested dict structure exists in the cache
                                    if gate not in cached_data[N_ctrl]:
                                        cached_data[N_ctrl][gate] = {}
                                    if reservoir_count not in cached_data[N_ctrl][gate]:
                                        cached_data[N_ctrl][gate][reservoir_count] = {}
                                    if trotter_step not in cached_data[N_ctrl][gate][reservoir_count]:
                                        cached_data[N_ctrl][gate][reservoir_count][trotter_step] = []

                                    cached_trotter_data = cached_data[N_ctrl][gate][reservoir_count][trotter_step]

                                    # Get the previously cached best fidelity if available
                                    previous_best_data_point = None
                                    previous_best_fidelity = float('-inf')
                                    if len(cached_trotter_data) > 0:
                                        previous_best_data_point = cached_trotter_data[0]
                                        previous_best_fidelity = previous_best_data_point.get('fidelity', float('-inf'))

                                    # Decide if we must re-check everything:
                                    #  - if we see new files not in processed_files
                                    #  - or if the # of folder files != the cached 'num_data_runs'
                                    recheck_all = False
                                    if previous_best_data_point is not None:
                                        cached_num_runs = previous_best_data_point.get('num_data_runs', 0)
                                        if cached_num_runs != len(files_in_folder):
                                            recheck_all = True
                                    else:
                                        # If there's no cached data at all, we must check everything
                                        recheck_all = True

                                    # We'll store the absolute best result from scanning all valid files
                                    best_fidelity_overall = previous_best_fidelity
                                    best_data_point_overall = previous_best_data_point
                                    runs_found = 0

                                    if recheck_all:
                                        # We re-check all the files
                                        for file in files_in_folder:
                                            pickle_file = os.path.normpath(os.path.join(trotter_path, file))
                                            if not pickle_file.endswith('.pickle'):
                                                continue
                                            if is_valid_pickle_file(Path(pickle_file)):
                                                run_name = os.path.splitext(os.path.basename(file))[0]
                                                runs_found += 1
                                                costs, fidelity, num_params, test_results, grads_per_epoch, selected_indices = read_jax_file(pickle_file, gate)

                                                # If it's better than what we have so far, update
                                                if fidelity > best_fidelity_overall:
                                                    best_fidelity_overall = fidelity
                                                    best_data_point_overall = {
                                                        'costs': costs,
                                                        'gate': gate,
                                                        'fidelity': fidelity,
                                                        'test_results': test_results,
                                                        'param_count': num_params,
                                                        'run': run_name,
                                                        'num_data_runs': runs_found,
                                                        'grads_per_epoch': grads_per_epoch,
                                                        'selected_indices': selected_indices,
                                                        'file_path': pickle_file
                                                    }

                                                # Mark file as processed
                                                processed_files.add(pickle_file)

                                        # If we found at least something
                                        if best_data_point_overall:
                                            # Make sure we store the actual total runs found in the best_data_point
                                            # so that next time we see if the folder changed
                                            best_data_point_overall['num_data_runs'] = runs_found

                                            # Replace the single cached entry
                                            cached_data[N_ctrl][gate][reservoir_count][trotter_step] = [best_data_point_overall]

                                    else:
                                        # We do NOT recheck everything, but we still check any new files
                                        # to see if there's a better run
                                        runs_found = previous_best_data_point.get('num_data_runs', 0)
                                        new_files_found = False
                                        for file in files_in_folder:
                                            pickle_file = os.path.normpath(os.path.join(trotter_path, file))
                                            if not pickle_file.endswith('.pickle'):
                                                continue

                                            # If not processed yet, read it
                                            if pickle_file not in processed_files and is_valid_pickle_file(Path(pickle_file)):
                                                run_name = os.path.splitext(os.path.basename(file))[0]
                                                runs_found += 1
                                                new_files_found = True

                                                costs, fidelity, num_params, test_results, grads_per_epoch, selected_indices = read_jax_file(pickle_file, gate)
                                                if fidelity > best_fidelity_overall:
                                                    best_fidelity_overall = fidelity
                                                    best_data_point_overall = {
                                                        'costs': costs,
                                                        'gate': gate,
                                                        'fidelity': fidelity,
                                                        'test_results': test_results,
                                                        'param_count': num_params,
                                                        'run': run_name,
                                                        'num_data_runs': runs_found,
                                                        'grads_per_epoch': grads_per_epoch,
                                                        'selected_indices': selected_indices,
                                                        'file_path': pickle_file
                                                    }

                                                processed_files.add(pickle_file)

                                        # If we found any new files that improved results, update the cache
                                        if new_files_found and best_data_point_overall:
                                            best_data_point_overall['num_data_runs'] = runs_found
                                            cached_data[N_ctrl][gate][reservoir_count][trotter_step] = [best_data_point_overall]

    return cached_data, processed_files
