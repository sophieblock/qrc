{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import statistics\n",
    "import numpy as np\n",
    "import sympy\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import pickle\n",
    "from sympy import symbols, MatrixSymbol, lambdify, Matrix, pprint\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sympy import symbols, MatrixSymbol, lambdify\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import scipy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "from qiskit.circuit.library import *\n",
    "from qiskit import *\n",
    "import math\n",
    "from qiskit.quantum_info import *\n",
    "import autograd\n",
    "from pennylane.wires import Wires\n",
    "import matplotlib.cm as cm\n",
    "import base64\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from ANALYSIS_SPECIFIC_CONFIG import is_valid_pickle_file,spread_per_sample_vectorized,spread_pooling_vectorized, load_and_clean_pickle, extract_Nr, extract_trotter_step\n",
    "\n",
    "def process_data_dqfim(df, threshold, by_test, Nc, N_R, trot, print_bool=False):\n",
    "    \"\"\"\n",
    "    Create a list of dictionaries, each representing one (fixed_params_key, test_key) row.\n",
    "    Returns a list of row-dicts for direct DataFrame construction.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for fixed_params_key, test_dict in df.items():\n",
    "        for test_key, results in test_dict.items():\n",
    "            qfim_eigvals = results.get('qfim_eigvals', None)\n",
    "            qfim_mat = results.get('qfim',None)\n",
    "            # print(f\"qfim_mat.shape: {qfim_mat.shape}\")\n",
    "            entropies      = results.get('entropies', None)\n",
    "            input_states = results.get('L',None)\n",
    "            n_reserv = results.get('n_reserv',None)\n",
    "            # assert n_reserv == N_R, f'{N_R}. Actual: {n_reserv}'\n",
    "            T = results.get('time_steps',None)\n",
    "            # assert T == trot, f'{trot}. Actual: {T}'\n",
    "            row = {\n",
    "                \"N_ctrl\": Nc,\n",
    "                \"N_reserv\": n_reserv,\n",
    "                \"Trotter_Step\": T,\n",
    "                \"fixed_params_key\": fixed_params_key,\n",
    "                \"test_key\": test_key,\n",
    "                \"qfim_eigvals\": qfim_eigvals,\n",
    "                \"qfim_mat\":qfim_mat,\n",
    "                \"entropies\": entropies,\n",
    "                \"input_states\":input_states,\n",
    "                \"num_inputs\":len(input_states)\n",
    "                # Add any other fields you might need later\n",
    "            }\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "def build_df_expanded_DQFIM(base_path, sample_range, model_type, N_ctrls, K_str,datasize, threshold, by_test):\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    all_expanded_rows = []\n",
    "    processed_files = []\n",
    "    \n",
    "    # Iterate over each N_ctrl value\n",
    "    for N_ctrl in N_ctrls:\n",
    "        model_path = Path(base_path) / \"QFIM_global_results\" / f\"{model_type}_model_DQFIM\" / f\"Nc_{N_ctrl}\" / f\"sample_{sample_range}/{K_str}xK\"\n",
    "        if not model_path.exists():\n",
    "            print(f\"[WARN] Model path {model_path} does not exist for N_ctrl={N_ctrl}.\")\n",
    "            continue\n",
    "        \n",
    "        # Iterate over each Nr directory\n",
    "        for Nr in sorted(os.listdir(model_path)):\n",
    "            Nr_path = model_path / Nr\n",
    "            if not Nr_path.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Iterate over each trotter step directory\n",
    "            print(sorted(os.listdir(Nr_path)))\n",
    "            for trotter_step_dir in sorted(os.listdir(Nr_path)):\n",
    "                trotter_step_path = Nr_path / trotter_step_dir\n",
    "                if not trotter_step_path.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                data_file = trotter_step_path / f\"L_{datasize}/data.pickle\"\n",
    "                if not data_file.exists():\n",
    "                    continue\n",
    "                processed_files.append(data_file)\n",
    "                # Validate the pickle file without using cached results\n",
    "                if not is_valid_pickle_file(data_file):\n",
    "                    continue\n",
    "                \n",
    "                # Load the raw pickle data\n",
    "                raw_data = load_and_clean_pickle(data_file)\n",
    "                # Extract trotter step and reservoir count from the directory structure\n",
    "                try:\n",
    "                    trotter_step_num = extract_trotter_step(data_file)\n",
    "                    reservoir_count = extract_Nr(data_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Could not extract parameters from {data_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Process raw data using your expanded function (do not use cached expanded rows)\n",
    "                expanded_rows = process_data_dqfim(raw_data, threshold, by_test, N_ctrl, reservoir_count, trotter_step_num)\n",
    "                all_expanded_rows.extend(expanded_rows)\n",
    "\n",
    "    return pd.DataFrame(all_expanded_rows),processed_files\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "def compute_single_draw_stats(\n",
    "    eigvals,\n",
    "    full_qfim_mat,\n",
    "    threshold=1e-10,\n",
    "    spread_methods=(\"variance\", \"mad\"),\n",
    "    ddof=1,\n",
    "    scale=\"normal\",\n",
    "    gamma=1.0,\n",
    "    n=1,\n",
    "    V_theta=1.0,\n",
    "    n_ctrl=None,\n",
    "    n_reserv=None,\n",
    "    trotter_step=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute QFIM (or DQFIM) statistics for a SINGLE set of eigenvalues (one draw),\n",
    "    and also compute an effective dimension from the provided full QFIM matrix.\n",
    "    \n",
    "    Returned dictionary includes:\n",
    "      - \"draw_rank\"\n",
    "      - \"var_all_eigenvalues\"\n",
    "      - \"var_nonzero_eigenvalues\"\n",
    "      - \"trace_eigenvalues\"\n",
    "      - \"var_all_normalized_by_param_count\"\n",
    "      - \"trace_normalized_by_rank\"\n",
    "      - \"var_nonzero_log\"\n",
    "      - \"trace_normalized_by_param_count\"\n",
    "      - \"ipr_deff_raw\"        (raw IPR measure)\n",
    "      - \"ipr_deff_norm\"       (IPR computed on trace-normalized eigenvalues)\n",
    "      - \"abbas_deff_raw\"      (sum(log(1 + alpha*λ)) on raw eigenvalues)\n",
    "      - \"abbas_deff_norm\"     (sum(log(1 + alpha*λ)) on trace-normalized eigenvalues)\n",
    "      - \"effective_dimension\" (computed from the trace-normalized full QFIM)\n",
    "      - \"spread_metric_{method}\" for each method in spread_methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eigvals : array-like\n",
    "        Eigenvalues for this single QFIM (or DQFIM) draw.\n",
    "    full_qfim_mat : array-like (2D)\n",
    "        The full QFIM matrix for this draw.\n",
    "    threshold : float\n",
    "        Zero out eigenvalues below this threshold.\n",
    "    spread_methods : tuple of str\n",
    "        Methods for \"spread-of-log\" metrics.\n",
    "    ddof : int\n",
    "        Degrees of freedom for variance computations.\n",
    "    scale : str\n",
    "        Scale indicator for spread metrics.\n",
    "    gamma : float\n",
    "        Scaling parameter in the Abbas formula (typically in (0,1]).\n",
    "    n : int\n",
    "        Number of data samples used in the Abbas formula.\n",
    "    V_theta : float\n",
    "        Volume factor (typically 1.0).\n",
    "    n_ctrl, n_reserv, trotter_step : optional\n",
    "        Additional metadata.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stats_dict : dict\n",
    "        Dictionary of computed statistics.\n",
    "    \"\"\"\n",
    "    # Ensure eigvals is a 1D NumPy array.\n",
    "    arr = np.array(eigvals, dtype=float)\n",
    "    if arr.ndim != 1:\n",
    "        arr = arr.flatten()\n",
    "    # Zero out small eigenvalues.\n",
    "    arr = np.where(arr < threshold, 0.0, arr)\n",
    "    \n",
    "    # --- 1) Basic stats ---\n",
    "    draw_rank = np.count_nonzero(arr)\n",
    "    var_all_eigenvalues = np.var(arr, ddof=ddof)\n",
    "    # Variance on nonzero values using Boolean indexing.\n",
    "    nonzero = arr[arr > threshold]\n",
    "    var_nonzero_eigenvalues = np.var(nonzero, ddof=ddof) if nonzero.size > 1 else 0.0\n",
    "    var_nonzero_log = np.log(var_nonzero_eigenvalues) if var_nonzero_eigenvalues > 0 else -np.inf\n",
    "    trace_eigenvalues = np.sum(arr)\n",
    "    \n",
    "    var_normalized_by_param_count = var_all_eigenvalues / len(arr)\n",
    "    var_nonzero_normalized_by_rank = var_nonzero_eigenvalues / draw_rank\n",
    "    var_normalized_by_rank = var_all_eigenvalues / draw_rank\n",
    "    trace_normalized_by_rank = (trace_eigenvalues / draw_rank) if draw_rank > 0 else 0.0\n",
    "    trace_normalized_by_param_count = trace_eigenvalues / len(arr)\n",
    "    \n",
    "    # --- 2) IPR-based dimensions ---\n",
    "    # Raw IPR: (trace^2) / (sum of squares)\n",
    "    sum_of_squares = np.sum(arr**2)\n",
    "    ipr_deff_raw = (trace_eigenvalues**2) / sum_of_squares if sum_of_squares > 0 else 0.0\n",
    "    \n",
    "    # Normalized IPR: using normalized eigenvalues (p = λ/trace)\n",
    "    if trace_eigenvalues > 0:\n",
    "        arr_norm = arr / trace_eigenvalues\n",
    "        sum_norm_sq = np.sum(arr_norm**2)\n",
    "        ipr_deff_norm = 1.0 / sum_norm_sq if sum_norm_sq > 0 else 0.0\n",
    "    else:\n",
    "        arr_norm = None\n",
    "        ipr_deff_norm = 0.0\n",
    "    \n",
    "    # --- 3) Abbas-based dimensions ---\n",
    "    # Compute alpha = (gamma * n) / (2*log(n)) if n>1, else use limit.\n",
    "    if n > 1 and math.log(n) != 0.0:\n",
    "        alpha = (gamma * n) / (2.0 * math.log(n))\n",
    "    else:\n",
    "        alpha = 0.0\n",
    "    # Raw Abbas: computed on original eigenvalues.\n",
    "    abbas_deff_raw = np.sum(np.log(np.maximum(1.0 + alpha * arr, 1e-15)))\n",
    "    # Normalized Abbas: computed on trace-normalized eigenvalues.\n",
    "    if arr_norm is not None:\n",
    "        abbas_deff_norm = np.sum(np.log(np.maximum(1.0 + alpha * arr_norm, 1e-15)))\n",
    "    else:\n",
    "        abbas_deff_norm = 0.0\n",
    "    \n",
    "    # --- 4) Effective dimension from the full QFIM ---\n",
    "    # Normalize the full QFIM by its trace BEFORE diagonalizing.\n",
    "    F = np.array(full_qfim_mat, dtype=float)\n",
    "    trF = np.trace(F)\n",
    "    if trF > 0:\n",
    "        F_hat = F / trF\n",
    "        eigs_F = np.linalg.eigvalsh(F_hat)  # Eigenvalues of the normalized full QFIM.\n",
    "        eps = 1e-12\n",
    "        # Here, effective dimension is computed from the normalized spectrum p_i.\n",
    "        # If n > 1, use the standard formula; if n == 1, use the limit:\n",
    "        if n > 1 and math.log(n) != 0.0:\n",
    "            z = 0.5 * np.sum(np.log(1.0 + n * eigs_F + eps))\n",
    "            effective_dimension = (2.0 / np.log(n)) * z\n",
    "        else:\n",
    "            # For n == 1, define effective dimension as the sum_i p_i/(1+p_i)\n",
    "            effective_dimension = np.sum(eigs_F / (1.0 + eigs_F))\n",
    "    else:\n",
    "        effective_dimension = 0.0\n",
    "    \n",
    "    # --- 5) Spread-of-log metrics ---\n",
    "    # Reshape arr into a 1-row 2D array for external functions.\n",
    "    arr_2d = arr.reshape(1, -1)\n",
    "    spread_metrics = {}\n",
    "    for method in spread_methods:\n",
    "        per_draw = spread_per_sample_vectorized(arr_2d, method=method, threshold=threshold, ddof=ddof, scale=scale)\n",
    "        spread_metrics[f\"spread_metric_{method}\"] = per_draw[0] if per_draw.size > 0 else 0.0\n",
    "    \n",
    "    # --- 6) Build final dictionary ---\n",
    "    stats_dict = {\n",
    "        # Basic stats\n",
    "        \"draw_rank\": draw_rank,\n",
    "        \"var_all_eigenvalues\": var_all_eigenvalues,\n",
    "        \"var_nonzero_eigenvalues\": var_nonzero_eigenvalues,\n",
    "        \"trace_eigenvalues\": trace_eigenvalues,\n",
    "        \"var_all_normalized_by_param_count\": var_normalized_by_param_count,\n",
    "        \"var_all_normalized_by_rank\": var_normalized_by_rank,\n",
    "        \"var_nonzero_normalized_by_rank\":var_nonzero_normalized_by_rank,\n",
    "        \"trace_normalized_by_rank\": trace_normalized_by_rank,\n",
    "        \"trace_normalized_by_param_count\": trace_normalized_by_param_count,\n",
    "        \"var_nonzero_log\": var_nonzero_log,\n",
    "        \n",
    "        # IPR-based dimensions\n",
    "        \"ipr_deff_raw\": ipr_deff_raw,\n",
    "        \"ipr_deff_norm\": ipr_deff_norm,\n",
    "        \n",
    "        # Abbas-based dimensions\n",
    "        \"abbas_deff_raw\": abbas_deff_raw,\n",
    "        \"abbas_deff_norm\": abbas_deff_norm,\n",
    "        \n",
    "        # Effective dimension computed from the full QFIM (trace-normalized)\n",
    "        \"d_eff\": effective_dimension,\n",
    "    }\n",
    "    stats_dict.update(spread_metrics)\n",
    "    \n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_results shape: (120, 25)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display  # Ensure this is not overwritten elsewhere\n",
    "\n",
    "\n",
    "def clean_array(data):\n",
    "    \"\"\"Helper function to clean any deprecated JAX arrays.\"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return np.array(data)  # Ensure the array doesn't have deprecated attributes\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: clean_array(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [clean_array(v) for v in data]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "\n",
    "def get_dqfim_full_from_storage(num_L,N_R,N_C, T,test_key,fixed_param_dict_key=\"fixed_params0\"):\n",
    "\n",
    "    file_path = (\n",
    "        f\"/Users/sophieblock/QRCcapstone/parameter_analysis_directory/\"\n",
    "        f\"QFIM_global_results/gate_model_DQFIM/Nc_{N_C}/sample_pi/1bbxK/\"\n",
    "        f\"Nr_{N_R}/trotter_step_{T}/L_{num_L}/data.pickle\"\n",
    "    )\n",
    "    # print(file_path)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        all_tests_data = pickle.load(f)\n",
    "\n",
    "\n",
    "    results = all_tests_data[fixed_param_dict_key][test_key]\n",
    "    return results.get('qfim',None)\n",
    "\n",
    "def read_jax_file(file_path, gate_name,test_key):\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    df = clean_array(df)\n",
    "    # print(df.keys())\n",
    "    # print(df.get('controls', None), df.get('controls', None)[0])\n",
    "    try:\n",
    "        costs = np.asarray([float(i) for i in df['costs'][0]], dtype=np.float64)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading costs from {file_path}: {e}\")\n",
    "        costs = None\n",
    "        \n",
    "    try:\n",
    "        grads_per_epoch = [np.asarray(i, dtype=np.float64) for i in df['grads_per_epoch'][0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading grads_per_epoch from {file_path}: {e}\")\n",
    "        grads_per_epoch = None\n",
    "        \n",
    "    try:\n",
    "        fidelity = float(df['avg_fidelity'][0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading avg_fidelity from {file_path}: {e}\")\n",
    "        fidelity = None\n",
    "        \n",
    "    try:\n",
    "        num_params = 3 + int(df['controls'][0]) * int(df['reservoirs'][0]) * int(df['trotter_step'][0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing num_params from {file_path}: {e}\")\n",
    "        num_params = None\n",
    "        \n",
    "    try:\n",
    "        test_results = np.asarray(df['testing_results'][0], dtype=np.float64)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading testing_results from {file_path}: {e}\")\n",
    "        test_results = None\n",
    "        \n",
    "    # Read QFIM Results (GHZ initial state)\n",
    "    qfim_stats_dict_GHZ = df.get('QFIM Results', [None])[0]\n",
    "    if qfim_stats_dict_GHZ is None:\n",
    "        print(f\"Warning: 'QFIM Results' not found in {file_path}\")\n",
    "        qfim_eigvals_GHZ = qfim_full_GHZ = entropy_GHZ = None\n",
    "    else:\n",
    "        qfim_eigvals_GHZ = qfim_stats_dict_GHZ.get('qfim_eigvals', None)\n",
    "        if qfim_eigvals_GHZ is None:\n",
    "            print(f\"Warning: 'qfim_eigvals' not found in QFIM Results in {file_path}\")\n",
    "        qfim_full_GHZ = qfim_stats_dict_GHZ.get('qfim', None)\n",
    "        entropy_GHZ = qfim_stats_dict_GHZ.get('entropy', None)\n",
    "    \n",
    "    # Read QFIM Basis State stats\n",
    "    qfim_stats_dict_basis = df.get('QFIM_basis_state', [None])[0]\n",
    "    if qfim_stats_dict_basis is None:\n",
    "        print(f\"Warning: 'QFIM_basis_state' not found in {file_path}\")\n",
    "        qfim_eigvals = qfim_full = entropy = None\n",
    "    else:\n",
    "        qfim_eigvals = qfim_stats_dict_basis.get('qfim_eigvals', None)\n",
    "        if qfim_eigvals is None:\n",
    "            print(f\"Warning: 'qfim_eigvals' not found in QFIM_basis_state in {file_path}\")\n",
    "        qfim_full = qfim_stats_dict_basis.get('qfim', None)\n",
    "        entropy = qfim_stats_dict_basis.get('entropy', None)\n",
    "    \n",
    "    # Read DQFIM stats\n",
    "    dqfim_stats_dict = df.get('DQFIM_stats', [None])[0]\n",
    "    if dqfim_stats_dict is None:\n",
    "        print(f\"Warning: 'DQFIM_stats' not found in {file_path}\")\n",
    "        dqfim_eigvals = dqfim_full = dqfim_entropies = L_val = None\n",
    "    else:\n",
    "        dqfim_eigvals = dqfim_stats_dict.get('dqfim_eigvals', None)\n",
    "        if dqfim_eigvals is None:\n",
    "            print(f\"Warning: 'qfim_eigvals' not found in DQFIM_stats in {file_path}\")\n",
    "        dqfim_full = dqfim_stats_dict.get('dqfim', None)\n",
    "        L = dqfim_stats_dict.get('L', None)\n",
    "        if dqfim_full is None:\n",
    "            # extract from raw file\n",
    "            dqfim_full = get_dqfim_full_from_storage(num_L=len(L),N_R=df.get('reservoirs', None)[0],N_C= df.get('controls', None)[0], T = df.get('trotter_step', None)[0], test_key=test_key)\n",
    "        dqfim_entropies = dqfim_stats_dict.get('entropies', None)\n",
    "    \n",
    "\n",
    "\n",
    "    # Read target DQFIM stats\n",
    "    target_dqfim_stats_dict = df.get(\"target DQFIM stats\", [None])[0]\n",
    "    if target_dqfim_stats_dict is None:\n",
    "        print(f\"Warning: 'target DQFIM stats' not found in {file_path}\")\n",
    "        target_dqfim_eigvals = target_dqfim_full = target_dqfim_entropies = None\n",
    "    else:\n",
    "        target_dqfim_eigvals = target_dqfim_stats_dict.get('dqfim_eigvals', None)\n",
    "        target_dqfim_full = target_dqfim_stats_dict.get('DQFIM', None)\n",
    "        target_dqfim_entropies = target_dqfim_stats_dict.get('entropies', None)\n",
    "    \n",
    "    result = {\n",
    "        \"costs\": costs,\n",
    "        \"fidelity\": fidelity,\n",
    "        \"num_params\": num_params,\n",
    "        \"test_results\": test_results,\n",
    "        \"qfim_eigvals_GHZ\": qfim_eigvals_GHZ,\n",
    "        \"qfim_full_GHZ\": qfim_full_GHZ,\n",
    "        \"entropy_GHZ\": entropy_GHZ,\n",
    "        \"qfim_eigvals\": qfim_eigvals,\n",
    "        \"qfim_full\": qfim_full,\n",
    "        \"entropy\": entropy,\n",
    "        \"dqfim_eigvals\": dqfim_eigvals,\n",
    "        \"dqfim_full\": dqfim_full,\n",
    "        \"dqfim_entropies\": dqfim_entropies,\n",
    "        \"L\": L,\n",
    "        \"num_sampled_states\": len(L) if L is not None else 0,\n",
    "        \"target_dqfim_eigvals\": target_dqfim_eigvals,\n",
    "        \"target_dqfim_full\": target_dqfim_full,\n",
    "        \"target_dqfim_entropies\": target_dqfim_entropies,\n",
    "        \"N_ctrl\": df.get('controls', None)[0],\n",
    "        \"Trotter_Step\": df.get('trotter_step', None)[0],\n",
    "        \"N_R\": df.get('reservoirs', None)[0],\n",
    "        \"gate\": gate_name\n",
    "    }\n",
    "    return result\n",
    "def build_df_results(fixed_param_folder, base_folder,N_C = 2):\n",
    "    \"\"\"\n",
    "    Build a DataFrame by reading pickle files stored in the following hierarchy:\n",
    "    \n",
    "    base_folder/\n",
    "        trainsize_{train_size}/\n",
    "            sample_pi/\n",
    "                {fixed_param_folder}/\n",
    "                    {test_key}/\n",
    "                        {gate_folder}/\n",
    "                            data_run_0.pickle\n",
    "\n",
    "    For each pickle file, this function calls read_jax_file to extract relevant fields,\n",
    "    and adds additional columns for 'test_key' and 'gate_folder'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fixed_param_folder : str\n",
    "        The folder name for fixed parameters (e.g. 'fixed_params0').\n",
    "    base_folder : str\n",
    "        The base directory for the experiment results. For example:\n",
    "        \"/Users/sophieblock/QRCCapstone/parameter_analysis_directory/param_initialization_final/digital_results/Nc_2/reservoirs_1/trotter_8/trainsize_10/sample_pi\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame where each row corresponds to one pickle file's extracted data,\n",
    "        augmented with the test key and gate folder.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    full_path = os.path.join(base_folder, fixed_param_folder)\n",
    "    # print(f\"Scanning results in: {full_path}\")\n",
    "    \n",
    "    # Loop over test key folders.\n",
    "    for test_key in os.listdir(full_path):\n",
    "        test_key_path = os.path.join(full_path, test_key)\n",
    "        if not os.path.isdir(test_key_path):\n",
    "            print(f\"Skipping non-directory: {test_key_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Loop over gate folders inside each test key folder.\n",
    "        for gate_folder in os.listdir(test_key_path):\n",
    "            # Enforce expected naming.\n",
    "            if not gate_folder.startswith(f\"U{N_C}_\"):\n",
    "                # print(f\"Skipping gate folder {gate_folder} as it does not match expected pattern.\")\n",
    "                continue\n",
    "            gate_folder_path = os.path.join(test_key_path, gate_folder)\n",
    "            if not os.path.isdir(gate_folder_path):\n",
    "                print(f\"Skipping non-directory: {gate_folder_path}\")\n",
    "                continue\n",
    "            pickle_file = os.path.join(gate_folder_path, \"data_run_0.pickle\")\n",
    "            # print(f\"Looking for pickle file: {pickle_file}\")\n",
    "            if os.path.isfile(pickle_file):\n",
    "                try:\n",
    "                    # Use read_jax_file to extract the data.\n",
    "                    data = read_jax_file(pickle_file, gate_folder,test_key)\n",
    "                    # Add metadata.\n",
    "                    data[\"test_key\"] = test_key\n",
    "                    data[\"gate_folder\"] = gate_folder\n",
    "                    data[\"file_path\"] = pickle_file\n",
    "                    rows.append(data)\n",
    "                    # print(f\"Loaded data from {pickle_file}\")\n",
    "                except Exception as ex:\n",
    "                    print(f\"Error processing {pickle_file}: {ex}\")\n",
    "            else:\n",
    "                print(f\"Pickle file does not exist: {pickle_file}\")\n",
    "    \n",
    "    df_results = pd.DataFrame(rows)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "trotter_Step = 14\n",
    "fixed_param_folder = \"fixed_params0\"\n",
    "num_epochs = 1500\n",
    "train_size = 10\n",
    "base_folder = f\"/Users/sophieblock/QRCCapstone/parameter_analysis_directory/param_initialization_final/digital_results_res2/Nc_2//epochs_{num_epochs}/reservoirs_1/trotter_{trotter_Step}/trainsize_{train_size}/sample_pi\"\n",
    "df_results = build_df_results(fixed_param_folder, base_folder)\n",
    "\n",
    "print(\"df_results shape:\", df_results.shape)\n",
    "\n",
    "#  /Users/sophieblock/QRCCapstone/parameter_analysis_directory/param_initialization_final/digital_results/Nc_2/epochs_1000/reservoirs_1/trotter_12/trainsize_20/sample_positive_pi/fixed_params0/test0\n",
    "# IMPORTANT: If you get an error saying \"TypeError: 'list' object is not callable\" when calling display(df_results.head()),\n",
    "# ensure that you haven't overwritten the built-in 'display' function (e.g., by assigning a list to display).\n",
    "# display(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated DataFrame shape: (120, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['test_key', 'gate', 'fidelities_list', 'avg_fidelity', 'error',\n",
       "       'qfim_eigvals_GHZ', 'qfim_full_GHZ', 'entropy_GHZ', 'qfim_eigvals',\n",
       "       'qfim_full', 'entropy', 'dqfim_eigvals', 'dqfim_full',\n",
       "       'dqfim_entropies', 'target_dqfim_eigvals', 'target_dqfim_full',\n",
       "       'target_dqfim_entropies', 'N_ctrl', 'N_R', 'Trotter_Step', 'L',\n",
       "       'num_sampled_states'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/w2tzbky134bg3g8mct87xb8c0000gn/T/ipykernel_86799/3626108875.py:209: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  F = np.array(full_qfim_mat, dtype=float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['test_key', 'gate', 'fidelities_list', 'avg_fidelity', 'error',\n",
       "       'qfim_eigvals_GHZ', 'qfim_full_GHZ', 'entropy_GHZ', 'qfim_eigvals',\n",
       "       'qfim_full', 'entropy', 'dqfim_eigvals', 'dqfim_full',\n",
       "       'dqfim_entropies', 'target_dqfim_eigvals', 'target_dqfim_full',\n",
       "       'target_dqfim_entropies', 'N_ctrl', 'N_R', 'Trotter_Step', 'L',\n",
       "       'num_sampled_states', 'GHZ_draw_rank', 'GHZ_var_all_eigenvalues',\n",
       "       'GHZ_var_nonzero_eigenvalues', 'GHZ_trace_eigenvalues',\n",
       "       'GHZ_var_all_normalized_by_param_count',\n",
       "       'GHZ_var_nonzero_normalized_by_rank', 'GHZ_trace_normalized_by_rank',\n",
       "       'GHZ_trace_normalized_by_param_count', 'GHZ_var_nonzero_log',\n",
       "       'GHZ_ipr_deff_raw', 'GHZ_ipr_deff_norm', 'GHZ_abbas_deff_raw',\n",
       "       'GHZ_abbas_deff_norm', 'GHZ_d_eff', 'GHZ_spread_metric_variance',\n",
       "       'GHZ_spread_metric_mad', 'basis_draw_rank', 'basis_var_all_eigenvalues',\n",
       "       'basis_var_nonzero_eigenvalues', 'basis_trace_eigenvalues',\n",
       "       'basis_var_all_normalized_by_param_count',\n",
       "       'basis_var_nonzero_normalized_by_rank',\n",
       "       'basis_trace_normalized_by_rank',\n",
       "       'basis_trace_normalized_by_param_count', 'basis_var_nonzero_log',\n",
       "       'basis_ipr_deff_raw', 'basis_ipr_deff_norm', 'basis_abbas_deff_raw',\n",
       "       'basis_abbas_deff_norm', 'basis_d_eff', 'basis_spread_metric_variance',\n",
       "       'basis_spread_metric_mad', 'dqfim_draw_rank',\n",
       "       'dqfim_var_all_eigenvalues', 'dqfim_var_nonzero_eigenvalues',\n",
       "       'dqfim_trace_eigenvalues', 'dqfim_var_all_normalized_by_param_count',\n",
       "       'dqfim_var_nonzero_normalized_by_rank',\n",
       "       'dqfim_trace_normalized_by_rank',\n",
       "       'dqfim_trace_normalized_by_param_count', 'dqfim_var_nonzero_log',\n",
       "       'dqfim_ipr_deff_raw', 'dqfim_ipr_deff_norm', 'dqfim_abbas_deff_raw',\n",
       "       'dqfim_abbas_deff_norm', 'dqfim_d_eff', 'dqfim_spread_metric_variance',\n",
       "       'dqfim_spread_metric_mad', 'target_draw_rank',\n",
       "       'target_var_all_eigenvalues', 'target_var_nonzero_eigenvalues',\n",
       "       'target_trace_eigenvalues', 'target_var_all_normalized_by_param_count',\n",
       "       'target_var_nonzero_normalized_by_rank',\n",
       "       'target_trace_normalized_by_rank',\n",
       "       'target_trace_normalized_by_param_count', 'target_var_nonzero_log',\n",
       "       'target_ipr_deff_raw', 'target_ipr_deff_norm', 'target_abbas_deff_raw',\n",
       "       'target_abbas_deff_norm', 'target_d_eff',\n",
       "       'target_spread_metric_variance', 'target_spread_metric_mad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def aggregate_results(df):\n",
    "    \"\"\"\n",
    "    Aggregate the DataFrame by test_key and gate.\n",
    "    For each test_key/gate combination, aggregate key columns.\n",
    "    \"\"\"\n",
    "    aggregated = df.groupby([\"test_key\", \"gate\"]).agg(\n",
    "        fidelities_list=(\"fidelity\", list),\n",
    "        avg_fidelity=(\"fidelity\", \"mean\"),\n",
    "        error=(\"fidelity\", lambda x: np.mean(np.log(1 - x))),\n",
    "        qfim_eigvals_GHZ=(\"qfim_eigvals_GHZ\", \"first\"),\n",
    "        qfim_full_GHZ=(\"qfim_full_GHZ\", \"first\"),\n",
    "        entropy_GHZ=(\"entropy_GHZ\", \"first\"),\n",
    "        qfim_eigvals=(\"qfim_eigvals\", \"first\"),\n",
    "        qfim_full=(\"qfim_full\", \"first\"),\n",
    "        entropy=(\"entropy\", \"first\"),\n",
    "        dqfim_eigvals=(\"dqfim_eigvals\", \"first\"),\n",
    "        dqfim_full=(\"dqfim_full\", \"first\"),\n",
    "        dqfim_entropies=(\"dqfim_entropies\", \"first\"),\n",
    "        target_dqfim_eigvals=(\"target_dqfim_eigvals\", \"first\"),\n",
    "        target_dqfim_full=(\"target_dqfim_full\", \"first\"),\n",
    "        target_dqfim_entropies=(\"target_dqfim_entropies\", \"first\"),\n",
    "        N_ctrl=(\"N_ctrl\", \"first\"),\n",
    "        N_R=(\"N_R\", \"first\"),\n",
    "        Trotter_Step=(\"Trotter_Step\", \"first\"),\n",
    "        L=(\"L\", \"first\"),\n",
    "        num_sampled_states=(\"num_sampled_states\", \"first\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def update_with_all_qfim_metrics(df, threshold=1e-12, spread_methods=(\"variance\", \"mad\"),\n",
    "                                 ddof=1, scale=\"normal\", gamma=0.1, n=1, V_theta=1.0):\n",
    "    \"\"\"\n",
    "    For each row (one test_key/gate combination), compute derived metrics for all four QFIM variants:\n",
    "      - GHZ QFIM (from \"qfim_eigvals_GHZ\" and \"qfim_full_GHZ\")\n",
    "      - Basis QFIM (from \"qfim_eigvals\" and \"qfim_full\")\n",
    "      - DQFIM (from \"dqfim_eigvals\"; note: if you don’t store a full DQFIM matrix, you may need to pass a dummy matrix)\n",
    "      - Target DQFIM (from \"target_dqfim_eigvals\" and \"target_dqfim_full\")\n",
    "    \n",
    "    Returns a new DataFrame with the computed stats added with suffixes.\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # GHZ QFIM metrics:\n",
    "        ghz_stats = compute_single_draw_stats(\n",
    "            row[\"qfim_eigvals_GHZ\"],\n",
    "            row[\"qfim_full_GHZ\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=n,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        # Basis QFIM metrics:\n",
    "        basis_stats = compute_single_draw_stats(\n",
    "            row[\"qfim_eigvals\"],\n",
    "            row[\"qfim_full\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=n,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        # DQFIM metrics:\n",
    "     \n",
    "        assert isinstance(row['num_sampled_states'],int)\n",
    "        dqfim_stats = compute_single_draw_stats(\n",
    "            row[\"dqfim_eigvals\"],\n",
    "            row[\"dqfim_full\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=row['num_sampled_states'],\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "       \n",
    "        # Target DQFIM metrics:\n",
    "        target_stats = compute_single_draw_stats(\n",
    "            row[\"target_dqfim_eigvals\"],\n",
    "            row[\"target_dqfim_full\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=row['num_sampled_states'],\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        \n",
    "        updated_row = row.to_dict()\n",
    "        # Add the computed stats with suffixes to distinguish them.\n",
    "        updated_row.update({f\"GHZ_{k}\": v for k, v in ghz_stats.items()})\n",
    "        updated_row.update({f\"basis_{k}\": v for k, v in basis_stats.items()})\n",
    "        updated_row.update({f\"dqfim_{k}\": v for k, v in dqfim_stats.items()})\n",
    "        updated_row.update({f\"target_{k}\": v for k, v in target_stats.items()})\n",
    "        new_rows.append(updated_row)\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "df_agg = aggregate_results(df_results)\n",
    "print(\"Aggregated DataFrame shape:\", df_agg.shape)\n",
    "display(df_agg.keys())\n",
    "\n",
    "\n",
    "df_final = update_with_all_qfim_metrics(df_agg)\n",
    "df_final.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "def analyze_correlations(df_merged, x_metric, metrics_of_interest, corr_threshold=0.2, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Analyze pairwise correlations between a given x_metric and each metric in metrics_of_interest.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_merged : pd.DataFrame\n",
    "        The merged DataFrame containing the columns of interest.\n",
    "    x_metric : str\n",
    "        The column name for the independent variable (e.g., \"avg_fidelity\").\n",
    "    metrics_of_interest : list of str\n",
    "        List of column names whose correlations with x_metric will be computed.\n",
    "    corr_threshold : float, optional\n",
    "        Minimum absolute correlation coefficient to report (default 0.2).\n",
    "    p_threshold : float, optional\n",
    "        Maximum p-value threshold to report (default 0.05).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pearson_results : dict\n",
    "        Dictionary mapping each metric to its Pearson correlation coefficient and p-value.\n",
    "    spearman_results : dict\n",
    "        Dictionary mapping each metric to its Spearman correlation coefficient and p-value.\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with the columns of interest and drop rows with NaN values.\n",
    "    df_corr = df_merged[[x_metric] + metrics_of_interest].copy()\n",
    "    df_corr = df_corr.dropna(subset=[x_metric] + metrics_of_interest)\n",
    "    # print(\"Columns in df_corr:\", df_corr.columns)\n",
    "    # print(\"Number of rows after dropna:\", len(df_corr))\n",
    "    \n",
    "    # Ensure that each metric column contains float values (e.g., convert from JAX arrays if needed)\n",
    "    for col in metrics_of_interest:\n",
    "        df_corr[col] = df_corr[col].apply(\n",
    "            lambda val: float(val.item()) if hasattr(val, \"item\") else float(val)\n",
    "        )\n",
    "    \n",
    "    # Compute Pearson correlations for each metric.\n",
    "    pearson_results = {}\n",
    "    for col in metrics_of_interest:\n",
    "        res_df = pg.corr(x=df_corr[x_metric], y=df_corr[col], method=\"pearson\")\n",
    "        r_val = res_df[\"r\"].iloc[0]\n",
    "        p_val = res_df[\"p-val\"].iloc[0]\n",
    "        pearson_results[col] = {\"pearson_r\": r_val, \"p_value\": p_val}\n",
    "    \n",
    "    # Compute Spearman correlations for each metric.\n",
    "    spearman_results = {}\n",
    "    for col in metrics_of_interest:\n",
    "        sp_df = pg.corr(x=df_corr[x_metric], y=df_corr[col], method=\"spearman\")\n",
    "        rho_val = sp_df[\"r\"].iloc[0]\n",
    "        p_val = sp_df[\"p-val\"].iloc[0]\n",
    "        spearman_results[col] = {\"spearman_rho\": rho_val, \"p_value\": p_val}\n",
    "    \n",
    "    # Print out the correlations that meet the filtering thresholds.\n",
    "    print(f\"\\nPairwise correlations vs. {x_metric} (Pearson):\")\n",
    "    for metric, vals in pearson_results.items():\n",
    "        if abs(vals[\"pearson_r\"]) > corr_threshold and vals[\"p_value\"] < p_threshold:\n",
    "            print(f\"{metric}: r={vals['pearson_r']:.3f}, p={vals['p_value']:.3g}\")\n",
    "    \n",
    "    print(f\"\\nPairwise correlations vs. {x_metric} (Spearman):\")\n",
    "    for metric, vals in spearman_results.items():\n",
    "        if abs(vals[\"spearman_rho\"]) > corr_threshold and vals[\"p_value\"] < p_threshold:\n",
    "            print(f\"{metric}: rho={vals['spearman_rho']:.3f}, p={vals['p_value']:.3g}\")\n",
    "    \n",
    "    return pearson_results, spearman_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['GHZ_var_all_normalized_by_rank'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m metrics_of_interest_ghz \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m   \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGHZ_spread_metric_variance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGHZ_spread_metric_mad\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Now you can use these common thresholds in your calls:\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pearson_corrs_ghz, spearman_corrs_ghz \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_correlations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_of_interest_ghz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorr_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCORR_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP_THRESHOLD\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m metrics_of_interest_basis \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasis_spread_metric_mad\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     31\u001b[0m ]\n\u001b[1;32m     32\u001b[0m pearson_corrs_basis, spearman_corrs_basis \u001b[38;5;241m=\u001b[39m analyze_correlations(\n\u001b[1;32m     33\u001b[0m     df_final, x_metric, metrics_of_interest_basis,\n\u001b[1;32m     34\u001b[0m     corr_threshold\u001b[38;5;241m=\u001b[39mCORR_THRESHOLD, p_threshold\u001b[38;5;241m=\u001b[39mP_THRESHOLD\n\u001b[1;32m     35\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36manalyze_correlations\u001b[0;34m(df_merged, x_metric, metrics_of_interest, corr_threshold, p_threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mAnalyze pairwise correlations between a given x_metric and each metric in metrics_of_interest.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Dictionary mapping each metric to its Spearman correlation coefficient and p-value.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with the columns of interest and drop rows with NaN values.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m df_corr \u001b[38;5;241m=\u001b[39m \u001b[43mdf_merged\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_metric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetrics_of_interest\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     29\u001b[0m df_corr \u001b[38;5;241m=\u001b[39m df_corr\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[x_metric] \u001b[38;5;241m+\u001b[39m metrics_of_interest)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(\"Columns in df_corr:\", df_corr.columns)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# print(\"Number of rows after dropna:\", len(df_corr))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Ensure that each metric column contains float values (e.g., convert from JAX arrays if needed)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_env/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['GHZ_var_all_normalized_by_rank'] not in index\""
     ]
    }
   ],
   "source": [
    "CORR_THRESHOLD = 0.2\n",
    "P_THRESHOLD = 0.05\n",
    "x_metric = \"avg_fidelity\"\n",
    "metrics_of_interest_ghz = [\n",
    "\n",
    "  \n",
    "       'GHZ_var_all_normalized_by_rank', 'GHZ_var_nonzero_normalized_by_rank',\n",
    "       'GHZ_trace_normalized_by_rank', \n",
    "       'GHZ_var_nonzero_log',  'GHZ_ipr_deff_norm',\n",
    "       'GHZ_abbas_deff_raw', 'GHZ_abbas_deff_norm', 'GHZ_d_eff',\n",
    "       'GHZ_spread_metric_variance', 'GHZ_spread_metric_mad',\n",
    "]\n",
    "# Now you can use these common thresholds in your calls:\n",
    "pearson_corrs_ghz, spearman_corrs_ghz = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_ghz,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD\n",
    ")\n",
    "\n",
    "\n",
    "metrics_of_interest_basis = [\n",
    "\n",
    "\n",
    "  \n",
    "       'basis_var_all_normalized_by_rank',\n",
    "       'basis_var_nonzero_normalized_by_rank',\n",
    "       'basis_trace_normalized_by_rank',\n",
    "        'basis_var_nonzero_log',\n",
    "      'basis_ipr_deff_norm', 'basis_abbas_deff_raw',\n",
    "       'basis_abbas_deff_norm', 'basis_d_eff', 'basis_spread_metric_variance',\n",
    "       'basis_spread_metric_mad'\n",
    "]\n",
    "pearson_corrs_basis, spearman_corrs_basis = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_basis,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD\n",
    ")\n",
    "\n",
    "\n",
    "metrics_of_interest_dqfim = [\n",
    "\n",
    "     \n",
    "       'dqfim_var_all_normalized_by_rank',\n",
    "       'dqfim_var_nonzero_normalized_by_rank',\n",
    "       'dqfim_trace_normalized_by_rank',\n",
    "       'dqfim_var_nonzero_log',\n",
    "     'dqfim_ipr_deff_norm', 'dqfim_abbas_deff_raw',\n",
    "       'dqfim_abbas_deff_norm', 'dqfim_d_eff', 'dqfim_spread_metric_variance',\n",
    "       'dqfim_spread_metric_mad'\n",
    "]\n",
    "pearson_corrs_dqfim, spearman_corrs_dqfim = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_dqfim,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD\n",
    ")\n",
    "\n",
    "\n",
    "metrics_of_interest_targ_dqfim = [\n",
    "    \n",
    "       'target_var_all_normalized_by_rank',\n",
    "       'target_var_nonzero_normalized_by_rank',\n",
    "       'target_trace_normalized_by_rank',\n",
    "       'target_var_nonzero_log',\n",
    "      'target_ipr_deff_norm', 'target_abbas_deff_raw',\n",
    "       'target_abbas_deff_norm', 'target_d_eff',\n",
    "       'target_spread_metric_variance', 'target_spread_metric_mad']\n",
    "pearson_corrs_targ, spearman_corrs_targ = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_targ_dqfim,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD\n",
    ")\n",
    "# print(f\"\\nPairwise correlations vs. {x_metric} (Pearson):\")\n",
    "# for metric, vals in pearson_corrs.items():\n",
    "    \n",
    "#     print(f\"{metric}: r={vals['pearson_r']:.3f}, p={vals['p_value']:.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def collect_dqfim_stats(all_tests_data, fixed_params_key, threshold=1e-12, gamma=1.0, n=100):\n",
    "    \"\"\"\n",
    "    Build a single dictionary-of-dicts for the DQFIM stats keyed by test_key.\n",
    "\n",
    "    Returns a dict:\n",
    "\n",
    "        {\n",
    "           test_key1: {\n",
    "               \"draw_rank\": ...,\n",
    "               \"trace_eigenvalues\": ...,\n",
    "               \"ipr_deff_raw_dqfim\": ...,\n",
    "               ... all metrics ...\n",
    "           },\n",
    "           test_key2: {...},\n",
    "           ...\n",
    "        }\n",
    "\n",
    "    Then we can convert to a DataFrame and merge.\n",
    "    \"\"\"\n",
    "    dqfim_stats_dict = {}\n",
    "    test_data = all_tests_data[fixed_params_key]\n",
    "\n",
    "    for test_key, results in test_data.items():\n",
    "        qfi_eigvals = results[\"qfim_eigvals\"]\n",
    "        qfim_mat = results[\"qfim\"]\n",
    "        # call your single-draw function\n",
    "        stats = compute_single_draw_stats(\n",
    "            qfi_eigvals,\n",
    "            qfim_mat,\n",
    "            threshold=threshold,\n",
    "            gamma=gamma,\n",
    "            n=n,           # number of data-based states\n",
    "        )\n",
    "      \n",
    "        # rename or store with distinct suffix (e.g. \"_dqfim\") if you want\n",
    "        # so they don’t collide with existing columns\n",
    "        dqfim_stats_dict[test_key] = {\n",
    "            \"draw_rank_dqfim\": stats[\"draw_rank\"],\n",
    "            \"trace_dqfim\": stats[\"trace_eigenvalues\"],\n",
    "            \"trace_normalized_by_rank_dqfim\": stats[\"trace_normalized_by_rank\"],\n",
    "            \"trace_normalized_by_param_count_dqfim\": stats[\"trace_normalized_by_param_count\"],\n",
    "            \"var_all_eigenvalues_dqfim\": stats[\"var_all_eigenvalues\"],\n",
    "            \"var_nonzero_eigenvalues_dqfim\": stats[\"var_nonzero_eigenvalues\"],\n",
    "            \"var_all_normalized_by_param_count_dqfim\": stats[\"var_all_normalized_by_param_count\"],\n",
    "            \"var_nonzero_normalized_by_rank_dqfim\": stats[\"var_nonzero_normalized_by_rank\"],\n",
    "             \"var_nonzero_log_dqfim\": stats[\"var_nonzero_log\"],\n",
    "            \"ipr_deff_raw_dqfim\": stats[\"ipr_deff_raw\"],\n",
    "            \"ipr_deff_norm_dqfim\": stats[\"ipr_deff_norm\"],\n",
    "            \"abbas_deff_raw_dqfim\": stats[\"abbas_deff_raw\"],\n",
    "            \"abbas_deff_norm_dqfim\": stats[\"abbas_deff_norm\"],\n",
    "            # And the spread-of-log results:\n",
    "            \"spread_metric_variance_dqfim\": stats.get(\"spread_metric_variance\", 0.0),\n",
    "            \"spread_metric_mad_dqfim\": stats.get(\"spread_metric_mad\", 0.0),\n",
    "            \"d_eff_dqfim\": stats.get(\"d_eff\",0.0)\n",
    "        }\n",
    "    return dqfim_stats_dict\n",
    "\n",
    "\n",
    "L = 20\n",
    "T = 14\n",
    "file_path = (\n",
    "    f\"/Users/sophieblock/QRCcapstone/parameter_analysis_directory/\"\n",
    "    f\"QFIM_global_results/gate_model_DQFIM/Nc_2/sample_pi/1xK/\"\n",
    "    f\"Nr_1/trotter_step_{T}/L_{L}/data.pickle\"\n",
    ")\n",
    "print(file_path)\n",
    "with open(file_path, 'rb') as f:\n",
    "    all_tests_data = pickle.load(f)\n",
    "\n",
    "fixed_params_key = \"fixed_params0\"\n",
    "dqfim_stats_dict = collect_dqfim_stats(\n",
    "    all_tests_data,\n",
    "    fixed_params_key,\n",
    "    threshold=1e-12,\n",
    "    gamma=1.0,\n",
    "    n=L\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the dictionary-of-dicts to a DataFrame, keyed by test_key\n",
    "df_dqfim = pd.DataFrame.from_dict(dqfim_stats_dict, orient=\"index\")\n",
    "df_dqfim.reset_index(inplace=True)\n",
    "df_dqfim.rename(columns={\"index\": \"test_key\"}, inplace=True)\n",
    "\n",
    "# Now df_dqfim has columns like:\n",
    "#   test_key, trace_dqfim, var_all_eigenvalues_dqfim, ...\n",
    "# Merge onto df_final on 'test_key'\n",
    "df_merged = pd.merge(df_final, df_dqfim, on=\"test_key\", how=\"left\")\n",
    "\n",
    "# print(\"df_merged columns:\", df_merged.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "import warnings\n",
    "x_metric = \"avg_fidelity\"\n",
    "metrics_of_interest = [\n",
    "    \"entropy\",\"var_all_eigenvalues\", \"var_nonzero_eigenvalues\", \"trace_eigenvalues\",\n",
    "    \"var_nonzero_log\", \"var_nonzero_normalized_by_rank\",\n",
    "    \"trace_normalized_by_rank\",\n",
    "    \"ipr_deff_raw\", \"ipr_deff_norm\",\n",
    "    \"spread_metric_variance\", \"spread_metric_mad\",\n",
    "    \"trace_dqfim\",\"trace_normalized_by_param_count_dqfim\",\n",
    "   \"var_all_eigenvalues_dqfim\",\"var_nonzero_log_dqfim\",\n",
    "    \"var_nonzero_eigenvalues_dqfim\", \"var_all_normalized_by_param_count_dqfim\",\n",
    "    \"var_nonzero_normalized_by_rank_dqfim\", \"ipr_deff_raw_dqfim\",\n",
    "    \"ipr_deff_norm_dqfim\", \"abbas_deff_raw_dqfim\", \"abbas_deff_norm_dqfim\",\n",
    "    \"spread_metric_variance_dqfim\", \"spread_metric_mad_dqfim\",\n",
    "    \"d_eff\", \"d_eff_dqfim\"\n",
    "]\n",
    "\n",
    "# Build a new DataFrame focusing on these columns plus 'fidelity' \n",
    "# (or 'avg_fidelity' if you only have aggregated fidelity).\n",
    "df_corr = df_merged[[x_metric] + metrics_of_interest].copy()\n",
    "\n",
    "df_corr = df_corr.dropna(subset=[x_metric] + metrics_of_interest)\n",
    "print(\"Columns in df_corr:\", df_corr.columns)\n",
    "print(\"Number of rows after dropna:\", len(df_corr))\n",
    "# For each column that might contain JAX arrays:\n",
    "# cols_to_fix = [\"entropy\"] + metrics_of_interest\n",
    "\n",
    "# for c in cols_to_fix:\n",
    "#     df_corr[c] = df_corr[c].apply(\n",
    "#         lambda val: float(val.item()) if hasattr(val, \"item\") else float(val)\n",
    "#     )\n",
    "# 1) Pearson\n",
    "pearson_results = {}\n",
    "for col in metrics_of_interest:\n",
    "    df_corr[col] = df_corr[col].apply(\n",
    "        lambda val: float(val.item()) if hasattr(val, \"item\") else float(val)\n",
    "    )\n",
    "    res_df = pg.corr(x=df_corr[x_metric], y=df_corr[col], method=\"pearson\")\n",
    "    r_val = res_df[\"r\"].iloc[0]\n",
    "    p_val = res_df[\"p-val\"].iloc[0]\n",
    "    pearson_results[col] = {\n",
    "        \"pearson_r\": r_val,\n",
    "        \"p_value\": p_val\n",
    "    }\n",
    "\n",
    "# Print them out\n",
    "# print(\"\\nPairwise correlations vs. avg_fidelity (Pearson):\")\n",
    "# for k, v in pearson_results.items():\n",
    "#     print(f\"{k}: r={v['pearson_r']:.3f}, p={v['p_value']:.3g}\")\n",
    "\n",
    "# 2) Spearman\n",
    "spearman_results = {}\n",
    "for col in metrics_of_interest:\n",
    "    sp_df = pg.corr(x=df_corr[x_metric], y=df_corr[col], method=\"spearman\")\n",
    "    rho_val = sp_df[\"r\"].iloc[0]\n",
    "    p_val = sp_df[\"p-val\"].iloc[0]\n",
    "    spearman_results[col] = {\n",
    "        \"spearman_rho\": rho_val,\n",
    "        \"p_value\": p_val\n",
    "    }\n",
    "\n",
    "# 3) Possibly filter by thresholds:\n",
    "CORR_THRESHOLD = 0.2   \n",
    "P_THRESHOLD    = 0.05 \n",
    "\n",
    "print(f\"\\nPairwise correlations vs. {x_metric} (Pearson):\")\n",
    "for metric, vals in pearson_results.items():\n",
    "    if abs(vals[\"pearson_r\"]) > CORR_THRESHOLD and vals[\"p_value\"] < P_THRESHOLD:\n",
    "        print(f\"{metric}: r={vals['pearson_r']:.3f}, p={vals['p_value']:.3g}\")\n",
    "\n",
    "print(f\"\\nPairwise correlations vs. {x_metric} (Spearman):\")\n",
    "for metric, vals in spearman_results.items():\n",
    "    if abs(vals[\"spearman_rho\"]) > CORR_THRESHOLD and vals[\"p_value\"] < P_THRESHOLD:\n",
    "        print(f\"{metric}: rho={vals['spearman_rho']:.3f}, p={vals['p_value']:.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "predictor_metrics = [\n",
    "    \n",
    "    'var_nonzero_eigenvalues_dqfim',\n",
    "    # 'error',\n",
    "    # 'spread_metric_mad',\n",
    "    \n",
    "    'trace_normalized_by_rank',\n",
    "    # 'd_eff',\n",
    "\n",
    "    'd_eff_dqfim'\n",
    "    # 'spread_metric_variance',\n",
    "    # 'spread_metric_mad',\n",
    "    # 'spread_metric_variance_dqfim',\n",
    "    # \"spread_metric_variance_dqfim\"\n",
    "    # \"trace_normalized_by_rank\"\n",
    "    # 'spread_metric_variance_dqfim',\n",
    "    # \"abbas_deff_raw_dqfim\",\n",
    "    # \"ipr_deff_raw_dqfim\"}\n",
    "]\n",
    "metrics_to_visualize = [\n",
    "    \"test_key\",\n",
    "    'avg_fidelity',\n",
    "    'error',\n",
    "    'trace_eigenvalues',\n",
    "    'var_nonzero_eigenvalues_dqfim',\n",
    "    'trace_normalized_by_rank',\n",
    "    'var_nonzero_normalized_by_rank',\n",
    "    'var_nonzero_log',\n",
    "    'var_nonzero_normalized_by_rank_dqfim',\n",
    "    'spread_metric_mad',\n",
    "    'spread_metric_variance',\n",
    "    'spread_metric_variance_dqfim',\n",
    "    \"spread_metric_mad_dqfim\",\n",
    "    \"abbas_deff_raw_dqfim\",\n",
    "    \"abbas_deff_norm_dqfim\",\n",
    "    \"d_eff\",\n",
    "    \"d_eff_dqfim\"\n",
    "]\n",
    "\n",
    "df_viz = df_merged[metrics_to_visualize].dropna()\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df_viz,\n",
    "    dimensions=predictor_metrics,\n",
    "    color=\"avg_fidelity\",        # Use fidelity as the color dimension\n",
    "    hover_data=[\"test_key\"],      # Show test_key on hover\n",
    "    title=\"Scatter Matrix of Predictors with Fidelity as Color\"\n",
    ")\n",
    "fig.update_layout(width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "predictor_metrics = [\n",
    "    \n",
    "    # 'trace_normalized_by_rank',\n",
    "    # 'error',\n",
    "    'var_nonzero_eigenvalues_dqfim',\n",
    "    \n",
    "    # 'trace_normalized_by_rank',\n",
    "    # 'spread_metric_variance',\n",
    "    # 'spread_metric_mad',\n",
    "    # 'spread_metric_variance_dqfim',\n",
    "    # \"spread_metric_variance_dqfim\"\n",
    "    # \"trace_normalized_by_rank\"\n",
    "    'abbas_deff_norm_dqfim',\n",
    "    \"abbas_deff_raw_dqfim\",\n",
    "    # \"ipr_deff_raw_dqfim\"\n",
    "]\n",
    "metrics_to_visualize = [\n",
    "    \"test_key\",\n",
    "    'avg_fidelity',\n",
    "    'error',\n",
    "    'trace_eigenvalues',\n",
    "    'trace_normalized_by_rank',\n",
    "    'var_nonzero_eigenvalues_dqfim',\n",
    "    'var_nonzero_normalized_by_rank',\n",
    "    'var_nonzero_log',\n",
    "    'var_nonzero_normalized_by_rank_dqfim',\n",
    "    'spread_metric_mad',\n",
    "    'spread_metric_variance',\n",
    "    'spread_metric_variance_dqfim',\n",
    "    \"spread_metric_mad_dqfim\",\n",
    "    \"abbas_deff_raw_dqfim\",\n",
    "    \"abbas_deff_norm_dqfim\",\n",
    "    \"ipr_deff_raw_dqfim\"\n",
    "]\n",
    "\n",
    "df_viz = df_merged[metrics_to_visualize].dropna()\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df_viz,\n",
    "    dimensions=predictor_metrics,\n",
    "    color=\"avg_fidelity\",        # Use fidelity as the color dimension\n",
    "    hover_data=[\"test_key\"],      # Show test_key on hover\n",
    "    title=\"Scatter Matrix of Predictors with Fidelity as Color\"\n",
    ")\n",
    "fig.update_layout(width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "predictor_metrics = [\n",
    "    \n",
    "    # 'trace_normalized_by_rank',\n",
    "    'error',\n",
    "    'var_nonzero_normalized_by_rank',\n",
    "    \n",
    "    'trace_normalized_by_rank',\n",
    "    # 'spread_metric_variance',\n",
    "    # 'spread_metric_mad',\n",
    "    # 'spread_metric_variance_dqfim',\n",
    "    # \"spread_metric_variance_dqfim\"\n",
    "    # \"trace_normalized_by_rank\"\n",
    "    # 'spread_metric_variance_dqfim',\n",
    "    # \"abbas_deff_raw_dqfim\",\n",
    "    # \"ipr_deff_raw_dqfim\"\n",
    "]\n",
    "metrics_to_visualize = [\n",
    "    \"test_key\",\n",
    "    'avg_fidelity',\n",
    "    'error',\n",
    "    'trace_eigenvalues',\n",
    "    'trace_normalized_by_rank',\n",
    "    'var_nonzero_normalized_by_rank',\n",
    "    'var_nonzero_log',\n",
    "    'var_nonzero_normalized_by_rank_dqfim',\n",
    "    'spread_metric_mad',\n",
    "    'spread_metric_variance',\n",
    "    'spread_metric_variance_dqfim',\n",
    "    \"spread_metric_mad_dqfim\",\n",
    "    \"abbas_deff_raw_dqfim\",\n",
    "    \"abbas_deff_norm_dqfim\",\n",
    "    \"ipr_deff_raw_dqfim\"\n",
    "]\n",
    "\n",
    "df_viz = df_merged[metrics_to_visualize].dropna()\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df_viz,\n",
    "    dimensions=predictor_metrics,\n",
    "    color=\"avg_fidelity\",        # Use fidelity as the color dimension\n",
    "    hover_data=[\"test_key\"],      # Show test_key on hover\n",
    "    title=\"Scatter Matrix of Predictors with Fidelity as Color\"\n",
    ")\n",
    "fig.update_layout(width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch env (v3.11)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
