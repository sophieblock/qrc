{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import statistics\n",
    "import numpy as np\n",
    "import sympy\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import pickle\n",
    "from sympy import symbols, MatrixSymbol, lambdify, Matrix, pprint\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sympy import symbols, MatrixSymbol, lambdify\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import scipy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "from qiskit.circuit.library import *\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import *\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import statistics\n",
    "from pennylane.wires import Wires\n",
    "import matplotlib.cm as cm\n",
    "import base64\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ANALYSIS_SPECIFIC_CONFIG import is_valid_pickle_file,spread_per_sample_vectorized,spread_pooling_vectorized, load_and_clean_pickle, extract_Nr, extract_trotter_step\n",
    "\n",
    "def compute_single_draw_stats(\n",
    "    eigvals,\n",
    "    full_qfim_mat,\n",
    "    threshold=1e-10,\n",
    "    spread_methods=(\"variance\", \"mad\"),\n",
    "    ddof=1,\n",
    "    scale=\"normal\",\n",
    "    gamma=1.0,\n",
    "    n=1,\n",
    "    V_theta=1.0,\n",
    "    n_ctrl=None,\n",
    "    n_reserv=None,\n",
    "    trotter_step=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute QFIM (or DQFIM) statistics for a SINGLE set of eigenvalues (one draw),\n",
    "    and also compute an effective dimension from the provided full QFIM matrix.\n",
    "    \n",
    "    Returned dictionary includes:\n",
    "      - \"draw_rank\"\n",
    "      - \"var_all_eigenvalues\"\n",
    "      - \"var_nonzero_eigenvalues\"\n",
    "      - \"trace_eigenvalues\"\n",
    "      - \"var_all_normalized_by_param_count\"\n",
    "      - \"trace_normalized_by_rank\"\n",
    "      - \"var_nonzero_log\"\n",
    "      - \"trace_normalized_by_param_count\"\n",
    "      - \"ipr_deff_raw\"        (raw IPR measure)\n",
    "      - \"ipr_deff_norm\"       (IPR computed on trace-normalized eigenvalues)\n",
    "      - \"abbas_deff_raw\"      (sum(log(1 + alpha*λ)) on raw eigenvalues)\n",
    "      - \"abbas_deff_norm\"     (sum(log(1 + alpha*λ)) on trace-normalized eigenvalues)\n",
    "      - \"effective_dimension\" (computed from the trace-normalized full QFIM)\n",
    "      - \"spread_metric_{method}\" for each method in spread_methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eigvals : array-like\n",
    "        Eigenvalues for this single QFIM (or DQFIM) draw.\n",
    "    full_qfim_mat : array-like (2D)\n",
    "        The full QFIM matrix for this draw.\n",
    "    threshold : float\n",
    "        Zero out eigenvalues below this threshold.\n",
    "    spread_methods : tuple of str\n",
    "        Methods for \"spread-of-log\" metrics.\n",
    "    ddof : int\n",
    "        Degrees of freedom for variance computations.\n",
    "    scale : str\n",
    "        Scale indicator for spread metrics.\n",
    "    gamma : float\n",
    "        Scaling parameter in the Abbas formula (typically in (0,1]).\n",
    "    n : int\n",
    "        Number of data samples used in the Abbas formula.\n",
    "    V_theta : float\n",
    "        Volume factor (typically 1.0).\n",
    "    n_ctrl, n_reserv, trotter_step : optional\n",
    "        Additional metadata.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stats_dict : dict\n",
    "        Dictionary of computed statistics.\n",
    "    \"\"\"\n",
    "    # Ensure eigvals is a 1D NumPy array.\n",
    "    arr = np.array(eigvals, dtype=float)\n",
    "    if arr.ndim != 1:\n",
    "        arr = arr.flatten()\n",
    "    # Zero out small eigenvalues.\n",
    "    arr = np.where(arr < threshold, 0.0, arr)\n",
    "    \n",
    "    # --- 1) Basic stats ---\n",
    "    draw_rank = np.count_nonzero(arr)\n",
    "    var_all_eigenvalues = np.var(arr, ddof=ddof)\n",
    "    # Variance on nonzero values using Boolean indexing.\n",
    "    nonzero = arr[arr > threshold]\n",
    "    var_nonzero_eigenvalues = np.var(nonzero, ddof=ddof) if nonzero.size > 1 else 0.0\n",
    "    var_nonzero_log = np.log(var_nonzero_eigenvalues) if var_nonzero_eigenvalues > 0 else -np.inf\n",
    "    trace_eigenvalues = np.sum(arr)\n",
    "    min_nonzero_eigenvalue = np.min(nonzero) \n",
    "    \n",
    "    var_normalized_by_param_count = var_all_eigenvalues / len(arr)\n",
    "    var_nonzero_normalized_by_rank = var_nonzero_eigenvalues / draw_rank\n",
    "    var_normalized_by_rank = var_all_eigenvalues / draw_rank\n",
    "    trace_normalized_by_rank = (trace_eigenvalues / draw_rank) if draw_rank > 0 else 0.0\n",
    "    trace_normalized_by_param_count = trace_eigenvalues / len(arr)\n",
    "    \n",
    "    # --- 2) IPR-based dimensions ---\n",
    "    # Raw IPR: (trace^2) / (sum of squares)\n",
    "    sum_of_squares = np.sum(arr**2)\n",
    "    ipr_deff_raw = (trace_eigenvalues**2) / sum_of_squares if sum_of_squares > 0 else 0.0\n",
    "    \n",
    "    # Normalized IPR: using normalized eigenvalues (p = λ/trace)\n",
    "    if trace_eigenvalues > 0:\n",
    "        arr_norm = arr / trace_eigenvalues\n",
    "        sum_norm_sq = np.sum(arr_norm**2)\n",
    "        ipr_deff_norm = 1.0 / sum_norm_sq if sum_norm_sq > 0 else 0.0\n",
    "    else:\n",
    "        arr_norm = None\n",
    "        ipr_deff_norm = 0.0\n",
    "    \n",
    "    # --- 3) Abbas-based dimensions ---\n",
    "    # Compute alpha = (gamma * n) / (2*log(n)) if n>1, else use limit.\n",
    "    if n > 1 and math.log(n) != 0.0:\n",
    "        alpha = (gamma * n) / (2.0 * math.log(n))\n",
    "    else:\n",
    "        alpha = 0.0\n",
    "    # Raw Abbas: computed on original eigenvalues.\n",
    "    abbas_deff_raw = np.sum(np.log(np.maximum(1.0 + alpha * arr, 1e-15)))\n",
    "    # Normalized Abbas: computed on trace-normalized eigenvalues.\n",
    "    if arr_norm is not None:\n",
    "        abbas_deff_norm = np.sum(np.log(np.maximum(1.0 + alpha * arr_norm, 1e-15)))\n",
    "    else:\n",
    "        abbas_deff_norm = 0.0\n",
    "    \n",
    "    # --- 4) Effective dimension from the full QFIM ---\n",
    "    # Normalize the full QFIM by its trace BEFORE diagonalizing.\n",
    "    F = np.array(full_qfim_mat, dtype=complex)\n",
    "    trF = np.trace(F)\n",
    "    if trF > 0:\n",
    "        F_hat = F / trF\n",
    "        eigs_F = np.linalg.eigvalsh(F_hat)  # Eigenvalues of the normalized full QFIM.\n",
    "        eps = 1e-12\n",
    "        # Here, effective dimension is computed from the normalized spectrum p_i.\n",
    "        # If n > 1, use the standard formula; if n == 1, use the limit:\n",
    "        if n > 1 and math.log(n) != 0.0:\n",
    "            z = 0.5 * np.sum(np.log(1.0 + n * eigs_F + eps))\n",
    "            effective_dimension = (2.0 / np.log(n)) * z\n",
    "        else:\n",
    "            # For n == 1, define effective dimension as the sum_i p_i/(1+p_i)\n",
    "            effective_dimension = np.sum(eigs_F / (1.0 + eigs_F))\n",
    "    else:\n",
    "        effective_dimension = 0.0\n",
    "    \n",
    "    # --- 5) Spread-of-log metrics ---\n",
    "    # Reshape arr into a 1-row 2D array for external functions.\n",
    "    arr_2d = arr.reshape(1, -1)\n",
    "    spread_metrics = {}\n",
    "    for method in spread_methods:\n",
    "        per_draw = spread_per_sample_vectorized(arr_2d, method=method, threshold=threshold, ddof=ddof, scale=scale)\n",
    "        spread_metrics[f\"spread_metric_{method}\"] = per_draw[0] if per_draw.size > 0 else 0.0\n",
    "    \n",
    "    # --- 6) Build final dictionary ---\n",
    "    stats_dict = {\n",
    "        # Basic stats\n",
    "        \"draw_rank\": draw_rank,\n",
    "        \"var_all_eigenvalues\": var_all_eigenvalues,\n",
    "        \"var_nonzero_eigenvalues\": var_nonzero_eigenvalues,\n",
    "        \"trace_eigenvalues\": trace_eigenvalues,\n",
    "        \"var_all_normalized_by_param_count\": var_normalized_by_param_count,\n",
    "        \"var_all_normalized_by_rank\": var_normalized_by_rank,\n",
    "        \"var_nonzero_normalized_by_rank\":var_nonzero_normalized_by_rank,\n",
    "        \"trace_normalized_by_rank\": trace_normalized_by_rank,\n",
    "        \"trace_normalized_by_param_count\": trace_normalized_by_param_count,\n",
    "        \"var_nonzero_log\": var_nonzero_log,\n",
    "        \n",
    "        # IPR-based dimensions\n",
    "        \"ipr_deff_raw\": ipr_deff_raw,\n",
    "        \"ipr_deff_norm\": ipr_deff_norm,\n",
    "        \n",
    "        # Abbas-based dimensions\n",
    "        \"abbas_deff_raw\": abbas_deff_raw,\n",
    "        \"abbas_deff_norm\": abbas_deff_norm,\n",
    "        \n",
    "        # Effective dimension computed from the full QFIM (trace-normalized)\n",
    "        \"d_eff\": effective_dimension,\n",
    "        # Minimum nonzero eigenvalue (above threshold)\n",
    "        \"min_nonzero_eigenvalue\": min_nonzero_eigenvalue,\n",
    "    }\n",
    "    stats_dict.update(spread_metrics)\n",
    "    \n",
    "    return stats_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>QFIM</b>, T = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2\n",
    "sample_range_label ='normal_.25pi'\n",
    "Nc = 1\n",
    "Nr = 1\n",
    "# file_path = f'/Users/sophieblock/QRCCapstone/parameter_analysis_directory/QFIM_results/analog/Nc_{Nc}/sample_{sample_range_label}/1xK/Nr_{Nr}/trotter_step_{T}/data.pickle'\n",
    "file_path = f'/Users/sophieblock/QRCCapstone/parameter_analysis_directory/QFIM_global_results/analog_model_DQFIM/Nc_{Nc}/sample_{sample_range_label}/1xK/Nr_{Nr}/trotter_step_{T}/L_10/data.pickle'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    all_tests_data = pickle.load(f)\n",
    "fixed_params_dict = 'fixed_params0'\n",
    "test_data = all_tests_data[fixed_params_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_qfim_trace: \n",
      " - min: [('test32', '6.775e-05'), ('test120', '1.337e-03'), ('test143', '8.949e-03')]\n",
      " - max: [('test9', '6.25'), ('test48', '5.86'), ('test15', '5.43')]\n",
      "\n",
      "sorted_qfim_var: \n",
      " - min: [('test32', '6.470e-10'), ('test120', '2.460e-07'), ('test143', '1.142e-05')]\n",
      " - max: [('test9', '1.833e+00'), ('test48', '1.597e+00'), ('test15', '1.398e+00')]\n",
      "\n",
      "sorted_qfim_spread_var: \n",
      " - min: [('test193', '1.353e+00'), ('test1', '1.607e+00'), ('test58', '2.28')]\n",
      " - max: [('test197', '33.52'), ('test129', '29.84'), ('test123', '26.47')]\n",
      "\n",
      "sorted_qfim_deff: \n",
      " - min: [('test197', '5.005e-01'), ('test81', '5.005e-01'), ('test112', '5.005e-01')]\n",
      " - max: [('test193', '6.483e-01'), ('test1', '6.229e-01'), ('test121', '6.047e-01')]\n",
      "\n",
      "sorted_qfim_mineig:\n",
      " - min: \n",
      "  test32: rank = 2, min eigenvalue = 3.8748316910641734e-07\n",
      "  test143: rank = 3, min eigenvalue = 8.285804824481602e-07\n",
      "  test120: rank = 3, min eigenvalue = 2.601603910079575e-06\n",
      " - max: \n",
      "  test100: rank = 3, min eigenvalue = 0.04975012317299843\n",
      "  test35: rank = 3, min eigenvalue = 0.0380588136613369\n",
      "  test25: rank = 3, min eigenvalue = 0.0372161790728569\n"
     ]
    }
   ],
   "source": [
    "threshold=1e-8\n",
    "# Existing dictionaries\n",
    "params_sorted_by_qfim_trace = {}\n",
    "params_sorted_by_qfim_eigvals = {}\n",
    "params_sorted_by_qfim_var, params_sorted_by_qfim_nonzero_var = {}, {}\n",
    "params_sorted_by_qfim_mineig = {}\n",
    "params_sorted_by_qfim_entropy = {}\n",
    "key_and_params = {}\n",
    "\n",
    "# NEW dictionaries for IPR/Abbas or single-draw stats\n",
    "params_sorted_by_ipr_raw = {}\n",
    "params_sorted_by_ipr_norm = {}\n",
    "params_sorted_by_abbas_raw = {}\n",
    "params_sorted_by_abbas_norm = {}\n",
    "params_sorted_rank = {}\n",
    "params_sorted_spread_var, params_sorted_spread_mad = {}, {}\n",
    "\n",
    "# NEW dictionaries for the additional normalized metrics:\n",
    "params_sorted_by_var_all_normalized_by_param_count,params_sorted_by_var_all_normalized_by_rank = {},{}\n",
    "params_sorted_by_var_nonzero_normalized_by_rank = {}\n",
    "params_sorted_by_var_nonzero_log = {}\n",
    "params_sorted_by_trace_normalized_by_param_count = {}\n",
    "\n",
    "params_sorted_rank, params_sorted_deff= {},{}\n",
    "\n",
    "for trainable_params_dict in test_data.keys():\n",
    "    results = test_data[trainable_params_dict]\n",
    "    qfi_eigvals = results['qfim_eigvals']\n",
    "    stats = compute_single_draw_stats(qfi_eigvals,results['qfim'],  threshold=threshold, gamma=1.0, n=1)\n",
    "    \n",
    "    # Extract metrics from stats\n",
    "    rank = stats[\"draw_rank\"]\n",
    "    deff = stats['d_eff']\n",
    "    # print(deff)\n",
    "    trace = stats[\"trace_eigenvalues\"]\n",
    "    variance = stats[\"var_all_eigenvalues\"]\n",
    "    variance_nonzero = stats[\"var_nonzero_eigenvalues\"]\n",
    "    ipr_raw = stats[\"ipr_deff_raw\"]\n",
    "    ipr_norm = stats[\"ipr_deff_norm\"]\n",
    "    abbas_raw = stats[\"abbas_deff_raw\"]\n",
    "    abbas_norm = stats[\"abbas_deff_norm\"]\n",
    "    spread_mad = stats[\"spread_metric_mad\"]\n",
    "    spread_var = stats[\"spread_metric_variance\"]\n",
    "    \n",
    "    #  normalized metrics \n",
    "    var_all_norm = stats[\"var_all_normalized_by_param_count\"]\n",
    "    var_all_norm_rank = stats[\"var_all_normalized_by_rank\"]\n",
    "    var_nonzero_norm = stats[\"var_nonzero_normalized_by_rank\"]\n",
    "    var_nonzero_log = stats[\"var_nonzero_log\"]\n",
    "    trace_norm_param = stats[\"trace_normalized_by_param_count\"]\n",
    "    \n",
    "    # Store existing metrics\n",
    "    params_sorted_deff[trainable_params_dict] = deff\n",
    "    params_sorted_by_qfim_trace[trainable_params_dict] = float(trace)\n",
    "    params_sorted_by_qfim_var[trainable_params_dict] = float(variance)\n",
    "    params_sorted_by_qfim_nonzero_var[trainable_params_dict] = float(variance_nonzero)\n",
    "   \n",
    "    params_sorted_by_qfim_mineig[trainable_params_dict] = (rank, float(stats[\"min_nonzero_eigenvalue\"]))\n",
    "\n",
    "    params_sorted_spread_mad[trainable_params_dict] = spread_mad\n",
    "    params_sorted_spread_var[trainable_params_dict] = spread_var\n",
    "    \n",
    "    params_sorted_by_ipr_raw[trainable_params_dict] = ipr_raw\n",
    "    params_sorted_by_ipr_norm[trainable_params_dict] = ipr_norm\n",
    "    params_sorted_by_abbas_raw[trainable_params_dict] = abbas_raw\n",
    "    params_sorted_by_abbas_norm[trainable_params_dict] = abbas_norm\n",
    "    params_sorted_rank[trainable_params_dict] = rank\n",
    "    \n",
    "    try:\n",
    "        ent = float(np.mean(results['entropies']))\n",
    "    except KeyError:\n",
    "        ent = float(results['entropy'])\n",
    "    params_sorted_by_qfim_entropy[trainable_params_dict] = ent\n",
    "    \n",
    "    key_and_params[trainable_params_dict] = results['trainable_params']\n",
    "    \n",
    "    # Store the new normalized metrics\n",
    "    params_sorted_by_var_all_normalized_by_param_count[trainable_params_dict] = float(var_all_norm)\n",
    "    params_sorted_by_var_all_normalized_by_rank[trainable_params_dict] = float(var_all_norm_rank)\n",
    "    params_sorted_by_var_nonzero_normalized_by_rank[trainable_params_dict] = float(var_nonzero_norm)\n",
    "    params_sorted_by_var_nonzero_log[trainable_params_dict] = float(var_nonzero_log)\n",
    "    params_sorted_by_trace_normalized_by_param_count[trainable_params_dict] = float(trace_norm_param)\n",
    "\n",
    "# print(params_sorted_by_qfim_entropy)\n",
    "\n",
    "# print(key_and_params.keys())\n",
    "number_of_tests = len(test_data)\n",
    "\n",
    "sorted_qfim_entropy = sorted(params_sorted_by_qfim_entropy.items(), key=lambda x: round(float(x[1]), 6))\n",
    "sorted_qfim_var = sorted(params_sorted_by_qfim_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_var_normalized_by_rank = sorted(params_sorted_by_var_all_normalized_by_rank.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_var_nonzero = sorted(params_sorted_by_qfim_nonzero_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_trace = sorted(params_sorted_by_qfim_trace.items(), key=lambda x: float(x[1]))\n",
    "# params_sorted_by_abbas_raw\n",
    "sorted_qfim_deff = sorted(params_sorted_deff.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_abbas_d_eff = sorted(params_sorted_by_abbas_raw.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_ipr_d_eff = sorted(params_sorted_by_ipr_raw.items(), key=lambda x: float(x[1]))\n",
    "# Calculate the number of parameters to select\n",
    "number_of_tests = len(test_data)\n",
    "sorted_qfim_spread_var = sorted(params_sorted_spread_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_mineig = sorted(\n",
    "    params_sorted_by_qfim_mineig.items(), \n",
    "    key=lambda x: (x[1][0], x[1][1])\n",
    ")\n",
    "def round_tuples2(lst, ndigits=2):\n",
    "    \"\"\"Convert each (key, value) in lst to (key, formatted_value).\n",
    "    \n",
    "    If the rounded value is less than or equal to ndigits, it is formatted in scientific notation (e).\n",
    "    Otherwise, it is formatted in fixed-point notation (f).\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for k, v in lst:\n",
    "        val = round(float(v), ndigits)\n",
    "        if val <= ndigits:\n",
    "            formatted_value = f\"{v:.{ndigits+1}e}\"\n",
    "        else:\n",
    "            formatted_value = f\"{val:.{ndigits}f}\"\n",
    "        formatted.append((k, formatted_value))\n",
    "    return formatted\n",
    "def format_metric(val, ndigits=2):\n",
    "    \"\"\"\n",
    "    Format a numeric value such that if its rounded value is less than or equal to ndigits,\n",
    "    it is printed in scientific notation; otherwise in fixed-point notation.\n",
    "    \"\"\"\n",
    "    # Convert to float in case it's not already.\n",
    "    val = float(val)\n",
    "    rounded_val = round(val, ndigits)\n",
    "    if rounded_val <= ndigits:\n",
    "        return f\"{val:.{ndigits+1}e}\"\n",
    "    else:\n",
    "        return f\"{val:.{ndigits}f}\"\n",
    "num_print = 3\n",
    "print(f\"sorted_qfim_trace: \\n - min: {round_tuples2(sorted_qfim_trace)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_trace[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_var: \\n - min: {round_tuples2(sorted_qfim_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_var_normalized_by_rank[::-1])[:num_print]}\\n\")\n",
    "\n",
    "# print(f\"sorted_qfim_mineig: \\n - min: {round_tuples2(sorted_qfim_mineig)[:num_print]}\")\n",
    "# print(f\" - max: {round_tuples2(sorted_qfim_mineig[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_spread_var: \\n - min: {round_tuples2(sorted_qfim_spread_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_spread_var[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_deff: \\n - min: {round_tuples2(sorted_qfim_deff)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_deff[::-1])[:num_print]}\\n\")\n",
    "print(\"sorted_qfim_mineig:\\n - min: \")\n",
    "for key, (rnk, mineig) in sorted_qfim_mineig[:num_print]:\n",
    "    print(f\"  {key}: rank = {rnk}, min eigenvalue = {mineig}\")\n",
    "print(f\" - max: \")\n",
    "for key, (rnk, mineig) in sorted_qfim_mineig[::-1][:num_print]:\n",
    "    print(f\"  {key}: rank = {rnk}, min eigenvalue = {mineig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_qfim_trace: \n",
      " - min: [('test32', '6.775e-05'), ('test120', '1.337e-03'), ('test143', '8.949e-03')]\n",
      " - max: [('test9', '6.25'), ('test48', '5.86'), ('test15', '5.43')]\n",
      "\n",
      "sorted_qfim_var: \n",
      " - min: [('test32', '6.470e-10'), ('test120', '2.460e-07'), ('test143', '1.142e-05')]\n",
      " - max: [('test9', '1.833e+00'), ('test48', '1.597e+00'), ('test15', '1.398e+00')]\n",
      "\n",
      "sorted_qfim_spread_var: \n",
      " - min: [('test193', '1.353e+00'), ('test1', '1.607e+00'), ('test58', '2.28')]\n",
      " - max: [('test197', '33.52'), ('test129', '29.84'), ('test123', '26.47')]\n",
      "\n",
      "sorted_qfim_deff: \n",
      " - min: [('test197', '5.005e-01'), ('test81', '5.005e-01'), ('test112', '5.005e-01')]\n",
      " - max: [('test193', '6.483e-01'), ('test1', '6.229e-01'), ('test121', '6.047e-01')]\n",
      "\n",
      "sorted_qfim_mineig:\n",
      " - min: \n",
      "  test32: rank = 2, min eigenvalue = 3.8748316910641734e-07\n",
      "  test143: rank = 3, min eigenvalue = 8.285804824481602e-07\n",
      "  test120: rank = 3, min eigenvalue = 2.601603910079575e-06\n",
      " - max: \n",
      "  test100: rank = 3, min eigenvalue = 0.04975012317299843\n",
      "  test35: rank = 3, min eigenvalue = 0.0380588136613369\n",
      "  test25: rank = 3, min eigenvalue = 0.0372161790728569\n"
     ]
    }
   ],
   "source": [
    "threshold=1e-8\n",
    "# Existing dictionaries\n",
    "params_sorted_by_qfim_trace = {}\n",
    "params_sorted_by_qfim_eigvals = {}\n",
    "params_sorted_by_qfim_var, params_sorted_by_qfim_nonzero_var = {}, {}\n",
    "params_sorted_by_qfim_mineig = {}\n",
    "params_sorted_by_qfim_entropy = {}\n",
    "key_and_params = {}\n",
    "\n",
    "# NEW dictionaries for IPR/Abbas or single-draw stats\n",
    "params_sorted_by_ipr_raw = {}\n",
    "params_sorted_by_ipr_norm = {}\n",
    "params_sorted_by_abbas_raw = {}\n",
    "params_sorted_by_abbas_norm = {}\n",
    "params_sorted_rank = {}\n",
    "params_sorted_spread_var, params_sorted_spread_mad = {}, {}\n",
    "\n",
    "# NEW dictionaries for the additional normalized metrics:\n",
    "params_sorted_by_var_all_normalized_by_param_count,params_sorted_by_var_all_normalized_by_rank = {},{}\n",
    "params_sorted_by_var_nonzero_normalized_by_rank = {}\n",
    "params_sorted_by_var_nonzero_log = {}\n",
    "params_sorted_by_trace_normalized_by_param_count = {}\n",
    "\n",
    "params_sorted_rank, params_sorted_deff= {},{}\n",
    "\n",
    "for trainable_params_dict in test_data.keys():\n",
    "    results = test_data[trainable_params_dict]\n",
    "    qfi_eigvals = results['qfim_eigvals']\n",
    "    stats = compute_single_draw_stats(qfi_eigvals,results['qfim'],  threshold=threshold, gamma=1.0, n=1)\n",
    "    \n",
    "    # Extract metrics from stats\n",
    "    rank = stats[\"draw_rank\"]\n",
    "    deff = stats['d_eff']\n",
    "    # print(deff)\n",
    "    trace = stats[\"trace_eigenvalues\"]\n",
    "    variance = stats[\"var_all_eigenvalues\"]\n",
    "    variance_nonzero = stats[\"var_nonzero_eigenvalues\"]\n",
    "    ipr_raw = stats[\"ipr_deff_raw\"]\n",
    "    ipr_norm = stats[\"ipr_deff_norm\"]\n",
    "    abbas_raw = stats[\"abbas_deff_raw\"]\n",
    "    abbas_norm = stats[\"abbas_deff_norm\"]\n",
    "    spread_mad = stats[\"spread_metric_mad\"]\n",
    "    spread_var = stats[\"spread_metric_variance\"]\n",
    "    \n",
    "    #  normalized metrics \n",
    "    var_all_norm = stats[\"var_all_normalized_by_param_count\"]\n",
    "    var_all_norm_rank = stats[\"var_all_normalized_by_rank\"]\n",
    "    var_nonzero_norm = stats[\"var_nonzero_normalized_by_rank\"]\n",
    "    var_nonzero_log = stats[\"var_nonzero_log\"]\n",
    "    trace_norm_param = stats[\"trace_normalized_by_param_count\"]\n",
    "    \n",
    "    # Store existing metrics\n",
    "    params_sorted_deff[trainable_params_dict] = deff\n",
    "    params_sorted_by_qfim_trace[trainable_params_dict] = float(trace)\n",
    "    params_sorted_by_qfim_var[trainable_params_dict] = float(variance)\n",
    "    params_sorted_by_qfim_nonzero_var[trainable_params_dict] = float(variance_nonzero)\n",
    "   \n",
    "    params_sorted_by_qfim_mineig[trainable_params_dict] = (rank, float(stats[\"min_nonzero_eigenvalue\"]))\n",
    "\n",
    "    params_sorted_spread_mad[trainable_params_dict] = spread_mad\n",
    "    params_sorted_spread_var[trainable_params_dict] = spread_var\n",
    "    \n",
    "    params_sorted_by_ipr_raw[trainable_params_dict] = ipr_raw\n",
    "    params_sorted_by_ipr_norm[trainable_params_dict] = ipr_norm\n",
    "    params_sorted_by_abbas_raw[trainable_params_dict] = abbas_raw\n",
    "    params_sorted_by_abbas_norm[trainable_params_dict] = abbas_norm\n",
    "    params_sorted_rank[trainable_params_dict] = rank\n",
    "    \n",
    "    try:\n",
    "        ent = float(np.mean(results['entropies']))\n",
    "    except KeyError:\n",
    "        ent = float(results['entropy'])\n",
    "    params_sorted_by_qfim_entropy[trainable_params_dict] = ent\n",
    "    \n",
    "    key_and_params[trainable_params_dict] = results['trainable_params']\n",
    "    \n",
    "    # Store the new normalized metrics\n",
    "    params_sorted_by_var_all_normalized_by_param_count[trainable_params_dict] = float(var_all_norm)\n",
    "    params_sorted_by_var_all_normalized_by_rank[trainable_params_dict] = float(var_all_norm_rank)\n",
    "    params_sorted_by_var_nonzero_normalized_by_rank[trainable_params_dict] = float(var_nonzero_norm)\n",
    "    params_sorted_by_var_nonzero_log[trainable_params_dict] = float(var_nonzero_log)\n",
    "    params_sorted_by_trace_normalized_by_param_count[trainable_params_dict] = float(trace_norm_param)\n",
    "\n",
    "# print(params_sorted_by_qfim_entropy)\n",
    "\n",
    "# print(key_and_params.keys())\n",
    "number_of_tests = len(test_data)\n",
    "\n",
    "sorted_qfim_entropy = sorted(params_sorted_by_qfim_entropy.items(), key=lambda x: round(float(x[1]), 6))\n",
    "sorted_qfim_var = sorted(params_sorted_by_qfim_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_var_normalized_by_rank = sorted(params_sorted_by_var_all_normalized_by_rank.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_var_nonzero = sorted(params_sorted_by_qfim_nonzero_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_trace = sorted(params_sorted_by_qfim_trace.items(), key=lambda x: float(x[1]))\n",
    "# params_sorted_by_abbas_raw\n",
    "sorted_qfim_deff = sorted(params_sorted_deff.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_abbas_d_eff = sorted(params_sorted_by_abbas_raw.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_ipr_d_eff = sorted(params_sorted_by_ipr_raw.items(), key=lambda x: float(x[1]))\n",
    "# Calculate the number of parameters to select\n",
    "number_of_tests = len(test_data)\n",
    "sorted_qfim_spread_var = sorted(params_sorted_spread_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_mineig = sorted(\n",
    "    params_sorted_by_qfim_mineig.items(), \n",
    "    key=lambda x: (x[1][0], x[1][1])\n",
    ")\n",
    "def round_tuples2(lst, ndigits=2):\n",
    "    \"\"\"Convert each (key, value) in lst to (key, formatted_value).\n",
    "    \n",
    "    If the rounded value is less than or equal to ndigits, it is formatted in scientific notation (e).\n",
    "    Otherwise, it is formatted in fixed-point notation (f).\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for k, v in lst:\n",
    "        val = round(float(v), ndigits)\n",
    "        if val <= ndigits:\n",
    "            formatted_value = f\"{v:.{ndigits+1}e}\"\n",
    "        else:\n",
    "            formatted_value = f\"{val:.{ndigits}f}\"\n",
    "        formatted.append((k, formatted_value))\n",
    "    return formatted\n",
    "def format_metric(val, ndigits=2):\n",
    "    \"\"\"\n",
    "    Format a numeric value such that if its rounded value is less than or equal to ndigits,\n",
    "    it is printed in scientific notation; otherwise in fixed-point notation.\n",
    "    \"\"\"\n",
    "    # Convert to float in case it's not already.\n",
    "    val = float(val)\n",
    "    rounded_val = round(val, ndigits)\n",
    "    if rounded_val <= ndigits:\n",
    "        return f\"{val:.{ndigits+1}e}\"\n",
    "    else:\n",
    "        return f\"{val:.{ndigits}f}\"\n",
    "num_print = 3\n",
    "print(f\"sorted_qfim_trace: \\n - min: {round_tuples2(sorted_qfim_trace)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_trace[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_var: \\n - min: {round_tuples2(sorted_qfim_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_var_normalized_by_rank[::-1])[:num_print]}\\n\")\n",
    "\n",
    "# print(f\"sorted_qfim_mineig: \\n - min: {round_tuples2(sorted_qfim_mineig)[:num_print]}\")\n",
    "# print(f\" - max: {round_tuples2(sorted_qfim_mineig[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_spread_var: \\n - min: {round_tuples2(sorted_qfim_spread_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_spread_var[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_deff: \\n - min: {round_tuples2(sorted_qfim_deff)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_deff[::-1])[:num_print]}\\n\")\n",
    "print(\"sorted_qfim_mineig:\\n - min: \")\n",
    "for key, (rnk, mineig) in sorted_qfim_mineig[:num_print]:\n",
    "    print(f\"  {key}: rank = {rnk}, min eigenvalue = {mineig}\")\n",
    "print(f\" - max: \")\n",
    "for key, (rnk, mineig) in sorted_qfim_mineig[::-1][:num_print]:\n",
    "    print(f\"  {key}: rank = {rnk}, min eigenvalue = {mineig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_qfim_mineig:\n",
      " - min: \n",
      "  test32: rank = 2, min eigenvalue = 1.784063169907313e-05\n",
      "  test143: rank = 3, min eigenvalue = 3.345400756415984e-08\n",
      "  test120: rank = 3, min eigenvalue = 7.25559004877141e-07\n",
      " - max: \n",
      "  test170: rank = 3, min eigenvalue = 1.8117555379867554\n",
      "  test48: rank = 3, min eigenvalue = 1.3264001607894897\n",
      "  test100: rank = 3, min eigenvalue = 1.075181484222412\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted_qfim_mineig:\\n - min: \")\n",
    "for key, (rnk, mineig) in sorted_qfim_mineig[:num_print]:\n",
    "    print(f\"  {key}: rank = {rnk}, min eigenvalue = {mineig}\")\n",
    "print(f\" - max: \")\n",
    "for key, (rnk, mineig) in sorted_qfim_mineig[::-1][:num_print]:\n",
    "    print(f\"  {key}: rank = {rnk}, min eigenvalue = {mineig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['test170','test49','test48','test100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_qfim_trace: \n",
      " - min: [('test32', '1.353e-01'), ('test120', '4.669e-01'), ('test75', '5.984e-01')]\n",
      " - max: [('test48', '23.44'), ('test49', '23.39'), ('test170', '20.74')]\n",
      "\n",
      "sorted_qfim_var: \n",
      " - min: [('test32', '2.615e-03'), ('test120', '2.847e-02'), ('test58', '4.871e-02')]\n",
      " - max: [('test49', '19.23'), ('test48', '16.74'), ('test50', '12.14')]\n",
      "\n",
      "sorted_qfim_mineig: \n",
      " - min: [('test143', '3.345e-08'), ('test120', '7.256e-07'), ('test3', '1.414e-06')]\n",
      " - max: [('test170', '1.812e+00'), ('test48', '1.326e+00'), ('test100', '1.075e+00')]\n",
      "\n",
      "sorted_qfim_spread_var: \n",
      " - min: [('test100', '9.063e-01'), ('test17', '1.078e+00'), ('test79', '1.228e+00')]\n",
      " - max: [('test143', '75.75'), ('test13', '51.38'), ('test120', '48.41')]\n",
      "\n",
      "sorted_qfim_deff: \n",
      " - min: [('test143', '5.001e-01'), ('test32', '5.001e-01'), ('test93', '5.007e-01')]\n",
      " - max: [('test17', '6.977e-01'), ('test79', '6.912e-01'), ('test63', '6.817e-01')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Existing dictionaries\n",
    "params_sorted_by_qfim_trace = {}\n",
    "params_sorted_by_qfim_eigvals = {}\n",
    "params_sorted_by_qfim_var, params_sorted_by_qfim_nonzero_var = {}, {}\n",
    "params_sorted_by_qfim_mineig = {}\n",
    "params_sorted_by_qfim_entropy = {}\n",
    "key_and_params = {}\n",
    "\n",
    "# NEW dictionaries for IPR/Abbas or single-draw stats\n",
    "params_sorted_by_ipr_raw = {}\n",
    "params_sorted_by_ipr_norm = {}\n",
    "params_sorted_by_abbas_raw = {}\n",
    "params_sorted_by_abbas_norm = {}\n",
    "params_sorted_rank = {}\n",
    "params_sorted_spread_var, params_sorted_spread_mad = {}, {}\n",
    "\n",
    "# NEW dictionaries for the additional normalized metrics:\n",
    "params_sorted_by_var_all_normalized_by_param_count,params_sorted_by_var_all_normalized_by_rank = {},{}\n",
    "params_sorted_by_var_nonzero_normalized_by_rank = {}\n",
    "params_sorted_by_var_nonzero_log = {}\n",
    "params_sorted_by_trace_normalized_by_param_count = {}\n",
    "\n",
    "params_sorted_rank, params_sorted_deff= {},{}\n",
    "\n",
    "for trainable_params_dict in test_data.keys():\n",
    "    results = test_data[trainable_params_dict]\n",
    "    qfi_eigvals = results['qfim_eigvals']\n",
    "    stats = compute_single_draw_stats(qfi_eigvals,results['qfim'],  threshold=threshold, gamma=1.0, n=1)\n",
    "    \n",
    "    # Extract metrics from stats\n",
    "    rank = stats[\"draw_rank\"]\n",
    "    deff = stats['d_eff']\n",
    "    # print(deff)\n",
    "    trace = stats[\"trace_eigenvalues\"]\n",
    "    variance = stats[\"var_all_eigenvalues\"]\n",
    "    variance_nonzero = stats[\"var_nonzero_eigenvalues\"]\n",
    "    ipr_raw = stats[\"ipr_deff_raw\"]\n",
    "    ipr_norm = stats[\"ipr_deff_norm\"]\n",
    "    abbas_raw = stats[\"abbas_deff_raw\"]\n",
    "    abbas_norm = stats[\"abbas_deff_norm\"]\n",
    "    spread_mad = stats[\"spread_metric_mad\"]\n",
    "    spread_var = stats[\"spread_metric_variance\"]\n",
    "    \n",
    "    #  normalized metrics \n",
    "    var_all_norm = stats[\"var_all_normalized_by_param_count\"]\n",
    "    var_all_norm_rank = stats[\"var_all_normalized_by_rank\"]\n",
    "    var_nonzero_norm = stats[\"var_nonzero_normalized_by_rank\"]\n",
    "    var_nonzero_log = stats[\"var_nonzero_log\"]\n",
    "    trace_norm_param = stats[\"trace_normalized_by_param_count\"]\n",
    "    \n",
    "    # Store existing metrics\n",
    "    params_sorted_deff[trainable_params_dict] = deff\n",
    "    params_sorted_by_qfim_trace[trainable_params_dict] = float(trace)\n",
    "    params_sorted_by_qfim_var[trainable_params_dict] = float(variance)\n",
    "    params_sorted_by_qfim_nonzero_var[trainable_params_dict] = float(variance_nonzero)\n",
    "    params_sorted_by_qfim_mineig[trainable_params_dict] = float(stats[\"min_nonzero_eigenvalue\"])\n",
    "    params_sorted_spread_mad[trainable_params_dict] = spread_mad\n",
    "    params_sorted_spread_var[trainable_params_dict] = spread_var\n",
    "    \n",
    "    params_sorted_by_ipr_raw[trainable_params_dict] = ipr_raw\n",
    "    params_sorted_by_ipr_norm[trainable_params_dict] = ipr_norm\n",
    "    params_sorted_by_abbas_raw[trainable_params_dict] = abbas_raw\n",
    "    params_sorted_by_abbas_norm[trainable_params_dict] = abbas_norm\n",
    "    params_sorted_rank[trainable_params_dict] = rank\n",
    "    \n",
    "    try:\n",
    "        ent = float(np.mean(results['entropies']))\n",
    "    except KeyError:\n",
    "        ent = float(results['entropy'])\n",
    "    params_sorted_by_qfim_entropy[trainable_params_dict] = ent\n",
    "    \n",
    "    key_and_params[trainable_params_dict] = results['trainable_params']\n",
    "    \n",
    "    # Store the new normalized metrics\n",
    "    params_sorted_by_var_all_normalized_by_param_count[trainable_params_dict] = float(var_all_norm)\n",
    "    params_sorted_by_var_all_normalized_by_rank[trainable_params_dict] = float(var_all_norm_rank)\n",
    "    params_sorted_by_var_nonzero_normalized_by_rank[trainable_params_dict] = float(var_nonzero_norm)\n",
    "    params_sorted_by_var_nonzero_log[trainable_params_dict] = float(var_nonzero_log)\n",
    "    params_sorted_by_trace_normalized_by_param_count[trainable_params_dict] = float(trace_norm_param)\n",
    "\n",
    "# print(params_sorted_by_qfim_entropy)\n",
    "\n",
    "# print(key_and_params.keys())\n",
    "number_of_tests = len(test_data)\n",
    "\n",
    "sorted_qfim_entropy = sorted(params_sorted_by_qfim_entropy.items(), key=lambda x: round(float(x[1]), 6))\n",
    "sorted_qfim_var = sorted(params_sorted_by_qfim_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_var_normalized_by_rank = sorted(params_sorted_by_var_all_normalized_by_rank.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_var_nonzero = sorted(params_sorted_by_qfim_nonzero_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_trace = sorted(params_sorted_by_qfim_trace.items(), key=lambda x: float(x[1]))\n",
    "# params_sorted_by_abbas_raw\n",
    "sorted_qfim_deff = sorted(params_sorted_deff.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_abbas_d_eff = sorted(params_sorted_by_abbas_raw.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_ipr_d_eff = sorted(params_sorted_by_ipr_raw.items(), key=lambda x: float(x[1]))\n",
    "# Calculate the number of parameters to select\n",
    "number_of_tests = len(test_data)\n",
    "sorted_qfim_spread_var = sorted(params_sorted_spread_var.items(), key=lambda x: float(x[1]))\n",
    "sorted_qfim_mineig = sorted(params_sorted_by_qfim_mineig.items(), key=lambda x: float(x[1]))\n",
    "def round_tuples2(lst, ndigits=2):\n",
    "    \"\"\"Convert each (key, value) in lst to (key, formatted_value).\n",
    "    \n",
    "    If the rounded value is less than or equal to ndigits, it is formatted in scientific notation (e).\n",
    "    Otherwise, it is formatted in fixed-point notation (f).\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for k, v in lst:\n",
    "        val = round(float(v), ndigits)\n",
    "        if val <= ndigits:\n",
    "            formatted_value = f\"{v:.{ndigits+1}e}\"\n",
    "        else:\n",
    "            formatted_value = f\"{val:.{ndigits}f}\"\n",
    "        formatted.append((k, formatted_value))\n",
    "    return formatted\n",
    "\n",
    "num_print = 3\n",
    "print(f\"sorted_qfim_trace: \\n - min: {round_tuples2(sorted_qfim_trace)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_trace[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_var: \\n - min: {round_tuples2(sorted_qfim_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_var_normalized_by_rank[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_mineig: \\n - min: {round_tuples2(sorted_qfim_mineig)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_mineig[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_spread_var: \\n - min: {round_tuples2(sorted_qfim_spread_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_spread_var[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_deff: \\n - min: {round_tuples2(sorted_qfim_deff)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_deff[::-1])[:num_print]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_qfim_trace: \n",
      " - min: [('test32', '1.353e-01'), ('test120', '4.669e-01'), ('test75', '5.984e-01')]\n",
      " - max: [('test48', '23.44'), ('test49', '23.39'), ('test170', '20.74')]\n",
      "\n",
      "sorted_qfim_var: \n",
      " - min: [('test32', '1.308e-03'), ('test120', '9.491e-03'), ('test58', '1.624e-02')]\n",
      " - max: [('test49', '19.23'), ('test48', '16.74'), ('test50', '12.14')]\n",
      "\n",
      "sorted_qfim_ipr_d_eff: \n",
      " - min: [('test143', '1.000e+00'), ('test32', '1.000e+00'), ('test93', '1.002e+00')]\n",
      " - max: [('test17', '2.23'), ('test79', '2.15'), ('test58', '2.06')]\n",
      "\n",
      "sorted_qfim_spread_var: \n",
      " - min: [('test100', '9.063e-01'), ('test17', '1.078e+00'), ('test79', '1.228e+00')]\n",
      " - max: [('test143', '75.75'), ('test13', '51.38'), ('test120', '48.41')]\n",
      "\n",
      "sorted_qfim_deff: \n",
      " - min: [('test143', '5.001e-01'), ('test32', '5.001e-01'), ('test93', '5.007e-01')]\n",
      " - max: [('test17', '6.977e-01'), ('test79', '6.912e-01'), ('test63', '6.817e-01')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"sorted_qfim_trace: \\n - min: {round_tuples2(sorted_qfim_trace)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_trace[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_var: \\n - min: {round_tuples2(sorted_qfim_var_normalized_by_rank)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_var_normalized_by_rank[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_ipr_d_eff: \\n - min: {round_tuples2(sorted_qfim_ipr_d_eff)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_ipr_d_eff[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_spread_var: \\n - min: {round_tuples2(sorted_qfim_spread_var)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_spread_var[::-1])[:num_print]}\\n\")\n",
    "\n",
    "print(f\"sorted_qfim_deff: \\n - min: {round_tuples2(sorted_qfim_deff)[:num_print]}\")\n",
    "print(f\" - max: {round_tuples2(sorted_qfim_deff[::-1])[:num_print]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "12\n",
      "trace: test9\n",
      "var: test9\n",
      "var: test48\n",
      "var: test15\n",
      "var: test79\n",
      "ipr_d_eff: test193\n",
      "ipr_d_eff: test1\n",
      "Selected keys from min, max, and middle of each metric: {'test3', 'test49', 'test9', 'test1', 'test93', 'test100', 'test170', 'test193', 'test32', 'test120', 'test79', 'test58', 'test48', 'test15', 'test143'}\n",
      "Total number of selected keys: 15\n"
     ]
    }
   ],
   "source": [
    "metrics_sorted = {\n",
    "    \"trace\": sorted_qfim_trace,\n",
    "    \"var\": sorted_qfim_var_normalized_by_rank,\n",
    "    # \"spread_var\": sorted_qfim_spread_var,\n",
    "   \n",
    "    # \"d_eff\": sorted_qfim_deff,\n",
    "    \n",
    "    \"ipr_d_eff\":sorted_qfim_ipr_d_eff,\n",
    "    # \"entropy\":sorted_qfim_entropy\n",
    "}\n",
    "num_print = 1  # number of unique keys to select from each end for each metric\n",
    "# ['test170','test49','test48','test100']\n",
    "# Create an empty set to collect keys\n",
    "completed =  ['test32','test143','test120','test58','test3','test93','test170','test49','test48','test100','test170', 'test193', 'test32', 'test120', 'test58', 'test48', 'test15', 'test143']\n",
    "print(len(completed))\n",
    "# completed = ['test170','test49','test48','test100']\n",
    "# , 'test170', 'test193', 'test32', 'test120', 'test58', 'test48', 'test15', 'test143'\n",
    "selected_from_all = set(completed)\n",
    "print(len(selected_from_all))\n",
    "\n",
    "for metric_name, sorted_list in metrics_sorted.items():\n",
    "    if not sorted_list:\n",
    "        print(f\"No data for metric {metric_name}\")\n",
    "        continue  # Skip if the list is empty\n",
    "\n",
    "    # --- Select lower end keys ---\n",
    "    # lower_count = 0\n",
    "    # i = 0\n",
    "    # while lower_count < num_print and i < len(sorted_list):\n",
    "    #     key_candidate = sorted_list[i][0]\n",
    "    #     print(f\"{metric_name}: {key_candidate}\")\n",
    "    #     if key_candidate not in selected_from_all:\n",
    "    #         selected_from_all.add(key_candidate)\n",
    "    #         lower_count += 1\n",
    "    #     i += 1\n",
    "\n",
    "    # --- Select upper end keys ---\n",
    "    upper_count = 0\n",
    "    j = len(sorted_list) - 1\n",
    "    while upper_count < num_print and j >= 0:\n",
    "        key_candidate = sorted_list[j][0]\n",
    "        print(f\"{metric_name}: {key_candidate}\")\n",
    "        if key_candidate not in selected_from_all:\n",
    "            selected_from_all.add(key_candidate)\n",
    "            upper_count += 1\n",
    "        j -= 1\n",
    "\n",
    " \n",
    "\n",
    "print(\"Selected keys from min, max, and middle of each metric:\", selected_from_all)\n",
    "print(\"Total number of selected keys:\", len(selected_from_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test32 [rank: 2]:\n",
      "Trace: 0.135, min(lam): 1.784e-05\n",
      "Var: 0.001, Var (nonzero included): 0.005\n",
      "d_eff: 1.000, log(var): 39.906, \n",
      "\n",
      "test143 [rank: 3]:\n",
      "Trace: 1.190, min(lam): 3.345e-08\n",
      "Var: 0.067, Var (nonzero included): 0.157\n",
      "d_eff: 1.000, log(var): 75.755, \n",
      "\n",
      "test120 [rank: 3]:\n",
      "Trace: 0.467, min(lam): 7.256e-07\n",
      "Var: 0.009, Var (nonzero included): 0.022\n",
      "d_eff: 1.079, log(var): 48.411, \n",
      "\n",
      "test58 [rank: 3]:\n",
      "Trace: 0.923, min(lam): 1.509e-02\n",
      "Var: 0.016, Var (nonzero included): 0.022\n",
      "d_eff: 2.058, log(var): 3.862, \n",
      "\n",
      "test3 [rank: 3]:\n",
      "Trace: 0.935, min(lam): 1.414e-06\n",
      "Var: 0.040, Var (nonzero included): 0.093\n",
      "d_eff: 1.027, log(var): 46.689, \n",
      "\n",
      "test93 [rank: 3]:\n",
      "Trace: 0.991, min(lam): 1.734e-06\n",
      "Var: 0.047, Var (nonzero included): 0.109\n",
      "d_eff: 1.002, log(var): 43.974, \n"
     ]
    }
   ],
   "source": [
    "a = ['test32','test143','test120','test58','test3','test93']\n",
    "\n",
    "for test in a:\n",
    "    print(f\"\\n{test} [rank: {params_sorted_rank[test]}]:\")\n",
    "    \n",
    "    print(f\"Trace: {params_sorted_by_qfim_trace[test]:.3f}, min(lam): {params_sorted_by_qfim_mineig[test][1]:.3e}\")\n",
    "    print(f\"Var: {params_sorted_by_var_all_normalized_by_rank[test]:.3f}, Var (nonzero included): {params_sorted_by_var_nonzero_normalized_by_rank[test]:.3f}\")\n",
    "    print(f\"d_eff: {params_sorted_by_ipr_raw[test]:.3f}, log(var): {params_sorted_spread_var[test]:.3f}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test170 [rank: 3]:\n",
      "Trace: 20.742, min(lam): 1.812e+00\n",
      "Var: 11.651, Var (nonzero included): 21.295\n",
      "d_eff: 1.586, log(var): 1.338, \n",
      "\n",
      "test49 [rank: 3]:\n",
      "Trace: 23.395, min(lam): 9.898e-01\n",
      "Var: 19.234, Var (nonzero included): 40.326\n",
      "d_eff: 1.290, log(var): 2.538, \n",
      "\n",
      "test48 [rank: 3]:\n",
      "Trace: 23.439, min(lam): 1.326e+00\n",
      "Var: 16.735, Var (nonzero included): 32.765\n",
      "d_eff: 1.447, log(var): 1.891, \n",
      "\n",
      "test100 [rank: 3]:\n",
      "Trace: 10.505, min(lam): 1.075e+00\n",
      "Var: 2.307, Var (nonzero included): 3.417\n",
      "d_eff: 1.926, log(var): 0.906, \n"
     ]
    }
   ],
   "source": [
    "b = ['test170','test49','test48','test100']\n",
    "\n",
    "for test in b:\n",
    "    print(f\"\\n{test} [rank: {params_sorted_rank[test]}]:\")\n",
    "    \n",
    "    print(f\"Trace: {params_sorted_by_qfim_trace[test]:.3f}, min(lam): {params_sorted_by_qfim_mineig[test][1]:.3e}\")\n",
    "    print(f\"Var: {params_sorted_by_var_all_normalized_by_rank[test]:.3f}, Var (nonzero included): {params_sorted_by_var_nonzero_normalized_by_rank[test]:.3f}\")\n",
    "    print(f\"d_eff: {params_sorted_by_ipr_raw[test]:.3f}, log(var): {params_sorted_spread_var[test]:.3f}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test49 [rank: 3]:\n",
      "Trace: 23.395, min(lam): 9.898e-01\n",
      "Var: 19.234, Var (nonzero included): 40.326\n",
      "d_eff: 1.290, log(var): 2.538, \n",
      "\n",
      "test48 [rank: 3]:\n",
      "Trace: 23.439, min(lam): 1.326e+00\n",
      "Var: 16.735, Var (nonzero included): 32.765\n",
      "d_eff: 1.447, log(var): 1.891, \n",
      "\n",
      "test16 [rank: 3]:\n",
      "Trace: 9.629, min(lam): 5.346e-01\n",
      "Var: 2.094, Var (nonzero included): 3.339\n",
      "d_eff: 1.820, log(var): 1.617, \n"
     ]
    }
   ],
   "source": [
    "b = ['test49','test48','test16']\n",
    "\n",
    "for test in b:\n",
    "    print(f\"\\n{test} [rank: {params_sorted_rank[test]}]:\")\n",
    "    \n",
    "    print(f\"Trace: {params_sorted_by_qfim_trace[test]:.3f}, min(lam): {params_sorted_by_qfim_mineig[test][1]:.3e}\")\n",
    "    print(f\"Var: {params_sorted_by_var_all_normalized_by_rank[test]:.3f}, Var (nonzero included): {params_sorted_by_var_nonzero_normalized_by_rank[test]:.3f}\")\n",
    "    print(f\"d_eff: {params_sorted_by_ipr_raw[test]:.3f}, log(var): {params_sorted_spread_var[test]:.3f}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test32 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test32 gate U1_0 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test32 gate U1_1 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test143 gate U1_3 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test143 gate U1_4 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test143 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test143 gate U1_0 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test143 gate U1_1 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test93 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test93 gate U1_0 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test93 gate U1_1 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test58 gate U1_3 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test58 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test58 gate U1_0 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test58 gate U1_1 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test120 gate U1_3 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test120 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test120 gate U1_0 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test120 gate U1_1 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test48 gate U1_0 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test48 gate U1_1 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test49 gate U1_0 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test3 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test3 gate U1_0 has 3 data runs stored. Picking latest one: data_run_2.pickle\n",
      "test3 gate U1_1 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test170 gate U1_2 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test170 gate U1_0 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "test170 gate U1_1 has 2 data runs stored. Picking latest one: data_run_1.pickle\n",
      "df_results shape: (240, 27)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Compute single-draw QFIM/DQFIM statistics (unchanged)\n",
    "# ----------------------------------------------------------------------\n",
    "def compute_single_draw_stats(\n",
    "    eigvals,\n",
    "    full_qfim_mat,\n",
    "    threshold=1e-10,\n",
    "    spread_methods=(\"variance\", \"mad\"),\n",
    "    ddof=1,\n",
    "    scale=\"normal\",\n",
    "    gamma=1.0,\n",
    "    n=1,\n",
    "    V_theta=1.0,\n",
    "    n_ctrl=None,\n",
    "    n_reserv=None,\n",
    "    trotter_step=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute QFIM (or DQFIM) statistics for a SINGLE set of eigenvalues.\n",
    "    (This function is unchanged.)\n",
    "    \"\"\"\n",
    "    arr = np.array(eigvals)\n",
    "    if arr.ndim != 1:\n",
    "        arr = arr.flatten()\n",
    "    arr = np.where(arr < threshold, 0.0, arr)\n",
    "    \n",
    "    draw_rank = np.count_nonzero(arr)\n",
    "    var_all_eigenvalues = np.var(arr, ddof=ddof)\n",
    "    nonzero = arr[arr > threshold]\n",
    "    var_nonzero_eigenvalues = np.var(nonzero, ddof=ddof) if nonzero.size > 1 else 0.0\n",
    "    var_nonzero_log = np.log(var_nonzero_eigenvalues) if var_nonzero_eigenvalues > 0 else -np.inf\n",
    "    trace_eigenvalues = np.sum(arr)\n",
    "    min_nonzero_eigenvalue = np.min(nonzero)\n",
    "    \n",
    "    var_normalized_by_param_count = var_all_eigenvalues / len(arr)\n",
    "    var_nonzero_normalized_by_rank = var_nonzero_eigenvalues / draw_rank if draw_rank > 0 else 0.0\n",
    "    var_normalized_by_rank = var_all_eigenvalues / draw_rank if draw_rank > 0 else 0.0\n",
    "    trace_normalized_by_rank = (trace_eigenvalues / draw_rank) if draw_rank > 0 else 0.0\n",
    "    trace_normalized_by_param_count = trace_eigenvalues / len(arr)\n",
    "    \n",
    "    sum_of_squares = np.sum(arr**2)\n",
    "    ipr_deff_raw = (trace_eigenvalues**2) / sum_of_squares if sum_of_squares > 0 else 0.0\n",
    "    \n",
    "    if trace_eigenvalues > 0:\n",
    "        arr_norm = arr / trace_eigenvalues\n",
    "        sum_norm_sq = np.sum(arr_norm**2)\n",
    "        ipr_deff_norm = 1.0 / sum_norm_sq if sum_norm_sq > 0 else 0.0\n",
    "    else:\n",
    "        arr_norm = None\n",
    "        ipr_deff_norm = 0.0\n",
    "\n",
    "    if n > 1 and math.log(n) != 0.0:\n",
    "        alpha = (gamma * n) / (2.0 * math.log(n))\n",
    "    else:\n",
    "        alpha = 0.0\n",
    "    abbas_deff_raw = np.sum(np.log(np.maximum(1.0 + alpha * arr, 1e-15)))\n",
    "    if arr_norm is not None:\n",
    "        abbas_deff_norm = np.sum(np.log(np.maximum(1.0 + alpha * arr_norm, 1e-15)))\n",
    "    else:\n",
    "        abbas_deff_norm = 0.0\n",
    "\n",
    "    F = np.array(full_qfim_mat, dtype=complex)\n",
    "    trF = np.trace(F)\n",
    "    if trF > 0:\n",
    "        F_hat = F / trF\n",
    "        eigs_F = np.linalg.eigvalsh(F_hat)\n",
    "        eps = 1e-12\n",
    "        if n > 1 and math.log(n) != 0.0:\n",
    "            z = 0.5 * np.sum(np.log(1.0 + n * eigs_F + eps))\n",
    "            effective_dimension = (2.0 / np.log(n)) * z\n",
    "        else:\n",
    "            effective_dimension = np.sum(eigs_F / (1.0 + eigs_F))\n",
    "    else:\n",
    "        effective_dimension = 0.0\n",
    "\n",
    "    arr_2d = arr.reshape(1, -1)\n",
    "    spread_metrics = {}\n",
    "    # (Assuming spread_per_sample_vectorized is defined elsewhere.)\n",
    "    for method in spread_methods:\n",
    "        per_draw = spread_per_sample_vectorized(arr_2d, method=method, threshold=threshold, ddof=ddof, scale=scale)\n",
    "        spread_metrics[f\"spread_metric_{method}\"] = per_draw[0] if per_draw.size > 0 else 0.0\n",
    "    \n",
    "    stats_dict = {\n",
    "        \"draw_rank\": draw_rank,\n",
    "        \"var_all_eigenvalues\": var_all_eigenvalues,\n",
    "        \"var_nonzero_eigenvalues\": var_nonzero_eigenvalues,\n",
    "        \"trace_eigenvalues\": trace_eigenvalues,\n",
    "        \"var_all_normalized_by_param_count\": var_normalized_by_param_count,\n",
    "        \"var_all_normalized_by_rank\": var_normalized_by_rank,\n",
    "        \"var_nonzero_normalized_by_rank\": var_nonzero_normalized_by_rank,\n",
    "        \"trace_normalized_by_rank\": trace_normalized_by_rank,\n",
    "        \"trace_normalized_by_param_count\": trace_normalized_by_param_count,\n",
    "        \"var_nonzero_log\": var_nonzero_log,\n",
    "        \"ipr_deff_raw\": ipr_deff_raw,\n",
    "        \"ipr_deff_norm\": ipr_deff_norm,\n",
    "        \"abbas_deff_raw\": abbas_deff_raw,\n",
    "        \"abbas_deff_norm\": abbas_deff_norm,\n",
    "        \"d_eff\": effective_dimension,\n",
    "        # Minimum nonzero eigenvalue (above threshold)\n",
    "        \"min_nonzero_eigenvalue\": min_nonzero_eigenvalue,\n",
    "    }\n",
    "    stats_dict.update(spread_metrics)\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. clean_array (unchanged)\n",
    "# ----------------------------------------------------------------------\n",
    "def clean_array(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return np.array(data)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: clean_array(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [clean_array(v) for v in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Updated read_jax_file\n",
    "# ----------------------------------------------------------------------\n",
    "def read_jax_file(file_path, gate_name, test_key,num_L):\n",
    "    \"\"\"\n",
    "    Read the pickle file and extract fields using the updated keys.\n",
    "    Now extracts:\n",
    "      - QFIM Results (GHZ)\n",
    "      - QFIM_basis_state\n",
    "      - Computed training DQFIM from \"DQFIM_stats_local\"\n",
    "      - Original (file-stored) DQFIM from \"DQFIM_stats_{NUM_L}_L_states\"\n",
    "      - Computed target DQFIM from \"target DQFIM stats\"\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    df = clean_array(df)\n",
    "    \n",
    "    try:\n",
    "        costs = np.asarray([float(i) for i in df['costs'][0]], dtype=np.float64)\n",
    "        N_train = float(df['N_train'][0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading costs/N_train from {file_path}: {e}\")\n",
    "        costs = None\n",
    "        N_train= None\n",
    "        \n",
    "    try:\n",
    "        grads_per_epoch = [np.asarray(i, dtype=np.float64) for i in df['grads_per_epoch'][0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading grads_per_epoch from {file_path}: {e}\")\n",
    "        grads_per_epoch = None\n",
    "        \n",
    "    try:\n",
    "        fidelity = float(df['avg_fidelity'][0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading avg_fidelity from {file_path}: {e}\")\n",
    "        fidelity = None\n",
    "        \n",
    "    try:\n",
    "        num_params = 3 + int(df['controls'][0]) * int(df['reservoirs'][0]) * int(df['trotter_step'][0]) + int(df['trotter_step'][0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing num_params from {file_path}: {e}\")\n",
    "        num_params = None\n",
    "        \n",
    "    try:\n",
    "        test_results = np.asarray(df['testing_results'][0], dtype=np.float64)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading testing_results from {file_path}: {e}\")\n",
    "        test_results = None\n",
    "        \n",
    "    # QFIM Results (GHZ)\n",
    "    qfim_stats_dict_GHZ = df.get('QFIM Results', [None])[0]\n",
    "    if qfim_stats_dict_GHZ is None:\n",
    "        print(f\"Warning: 'QFIM Results' not found in {file_path}\")\n",
    "        qfim_eigvals_GHZ = qfim_full_GHZ = entropy_GHZ = None\n",
    "    else:\n",
    "        qfim_eigvals_GHZ = qfim_stats_dict_GHZ.get('qfim_eigvals', None)\n",
    "        qfim_full_GHZ = qfim_stats_dict_GHZ.get('qfim', None)\n",
    "        entropy_GHZ = qfim_stats_dict_GHZ.get('entropy', None)\n",
    "    \n",
    "    # QFIM Results (GHZ)\n",
    "    qfim_stats_dict_GHZ = df['QFIM Results'][0]\n",
    "    qfim_eigvals_GHZ = qfim_stats_dict_GHZ['qfim_eigvals']\n",
    "    qfim_full_GHZ = qfim_stats_dict_GHZ['qfim']\n",
    "    entropy_GHZ = qfim_stats_dict_GHZ['entropy']\n",
    "    \n",
    "    # QFIM Basis State\n",
    "    qfim_stats_dict_basis = df['QFIM_basis_state'][0]\n",
    "    qfim_eigvals_basis = qfim_stats_dict_basis['qfim_eigvals']\n",
    "    qfim_full_basis = qfim_stats_dict_basis['qfim']\n",
    "    entropy_basis = qfim_stats_dict_basis['entropy']\n",
    "    \n",
    "    # Computed Training DQFIM stats (new key)\n",
    "    dqfim_stats_local = df['DQFIM_stats_local'][0]\n",
    "    dqfim_eigvals_train = dqfim_stats_local['eigvals_train']\n",
    "    dqfim_train = dqfim_stats_local['DQFIM_train']\n",
    "    dqfim_entropies = dqfim_stats_local['entropies_train']\n",
    "    \n",
    "    # Original (file-stored) DQFIM stats (new key) – stored under key like \"DQFIM_stats_{NUM_L}_L_states\"\n",
    "    key_name = f'DQFIM_stats_{num_L}_L_states'\n",
    "    dqfim_file_stats = df[key_name][0]\n",
    "    dqfim_eigvals_file = dqfim_file_stats['dqfim_eigvals']\n",
    "    dqfim_full_file = dqfim_file_stats['dqfim']\n",
    "   \n",
    "    # Computed Target DQFIM stats (new key) target_dqfim_entropies\n",
    "    computed_target = df[\"target DQFIM stats\"][0]\n",
    "    target_dqfim_eigvals = computed_target['eigvals_target']\n",
    "    target_dqfim_full = computed_target['DQFIM_target']\n",
    "    target_dqfim_entropies = computed_target['entropies_target']\n",
    "                                             \n",
    "    readin_test_key = df.get(\"test_key\", [None])[0]\n",
    "    assert readin_test_key == test_key, f'Got: {readin_test_key}. Expected: {test_key}'\n",
    "    result = {\n",
    "        \"costs\": costs,\n",
    "        \"fidelity\": fidelity,\n",
    "        \"num_params\": num_params,\n",
    "        \"test_results\": test_results,\n",
    "        \"qfim_eigvals_GHZ\": qfim_eigvals_GHZ,\n",
    "        \"qfim_full_GHZ\": qfim_full_GHZ,\n",
    "        \"entropy_GHZ\": entropy_GHZ,\n",
    "        \"qfim_eigvals_basis\": qfim_eigvals_basis,\n",
    "        \"qfim_full_basis\": qfim_full_basis,\n",
    "        \"entropy_basis\": entropy_basis,\n",
    "        \"dqfim_eigvals_train\": dqfim_eigvals_train,\n",
    "        \"dqfim_train\": dqfim_train,\n",
    "        \"dqfim_entropies\": dqfim_entropies,\n",
    "        \"dqfim_eigvals_file\": dqfim_eigvals_file,\n",
    "        \"dqfim_full_file\": dqfim_full_file,\n",
    "        \"num_sampled_states\": num_L,\n",
    "        \"target_dqfim_eigvals\": target_dqfim_eigvals,\n",
    "        \"target_dqfim_full\": target_dqfim_full,\n",
    "        \"target_dqfim_entropies\": target_dqfim_entropies,\n",
    "        \"N_ctrl\": df.get('controls', [None])[0],\n",
    "        \"Trotter_Step\": df.get('trotter_step', [None])[0],\n",
    "        \"N_R\": df.get('reservoirs', [None])[0],\n",
    "        \"gate\": gate_name,\n",
    "        'N_train':N_train,\n",
    "        \"test_key\": readin_test_key\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Build and aggregate DataFrame results (updated)\n",
    "# ----------------------------------------------------------------------\n",
    "def build_df_results(fixed_param_folder, base_folder, N_C, N_R, T,num_L):\n",
    "    \"\"\"\n",
    "    Build a DataFrame by scanning the results folder.\n",
    "    Updated to include new keys (e.g., 'DQFIM_stats_local' and original file DQFIM stats).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    full_path = os.path.join(base_folder, fixed_param_folder)\n",
    "    for test_key in os.listdir(full_path):\n",
    "        test_key_path = os.path.join(full_path, test_key)\n",
    "        if not os.path.isdir(test_key_path):\n",
    "            continue\n",
    "        for gate_folder in os.listdir(test_key_path):\n",
    "            if not gate_folder.startswith(f\"U{N_C}_\"):\n",
    "                continue\n",
    "            gate_folder_path = os.path.join(test_key_path, gate_folder)\n",
    "            if not os.path.isdir(gate_folder_path):\n",
    "                continue\n",
    "            datarun_files_in_folder = os.listdir(gate_folder_path)\n",
    "            if len(datarun_files_in_folder) > 1:\n",
    "                file_name = f\"data_run_{len(datarun_files_in_folder)-1}.pickle\"\n",
    "                print(f'{test_key} gate {gate_folder} has {len(datarun_files_in_folder)} data runs stored. Picking latest one: {file_name}')\n",
    "            else:\n",
    "                file_name = \"data_run_0.pickle\"\n",
    "            pickle_file = os.path.join(gate_folder_path, file_name)\n",
    "            if os.path.isfile(pickle_file):\n",
    "                try:\n",
    "                    data = read_jax_file(pickle_file, gate_folder, test_key, num_L)\n",
    "                    data[\"test_key\"] = test_key\n",
    "                    data[\"gate_folder\"] = gate_folder\n",
    "                    data[\"file_path\"] = pickle_file\n",
    "                    rows.append(data)\n",
    "                except Exception as ex:\n",
    "                    print(f\"Error processing {pickle_file}: {ex}\")\n",
    "                    raise  # Break out immediately on error.\n",
    "            else:\n",
    "                print(f\"Pickle file does not exist: {pickle_file}\")\n",
    "    df_results = pd.DataFrame(rows)\n",
    "    return df_results\n",
    "\n",
    "# Example usage in post-processing:\n",
    "# df_results = build_df_results(fixed_param_folder, base_folder, N_C=2)\n",
    "# df_agg = aggregate_results(df_results)\n",
    "# df_final = update_with_all_qfim_metrics(df_agg)\n",
    "# print(df_final.shape)\n",
    "\n",
    "trotter_Step = 2\n",
    "N_ctrl = 1\n",
    "num_L = 10\n",
    "N_reserv = 1\n",
    "fixed_param_folder = \"fixed_params0\"\n",
    "num_epochs = 1000\n",
    "train_size = 10\n",
    "sample_range_label = 'normal_.25pi'\n",
    "base_folder = f\"/Users/sophieblock/QRCCapstone/parameter_analysis_directory/param_initialization_final/analog_results/Nc_{N_ctrl}/epochs_{num_epochs}/reservoirs_{N_reserv}/trotter_{trotter_Step}/trainsize_{train_size}/sample_{sample_range_label}\"\n",
    "df_results = build_df_results(fixed_param_folder, base_folder, N_C=N_ctrl, N_R=N_reserv, T=trotter_Step,num_L=num_L)\n",
    "\n",
    "print(\"df_results shape:\", df_results.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_results(df):\n",
    "    \"\"\"\n",
    "    Aggregate the DataFrame by test_key and gate.\n",
    "    Now also includes the original DQFIM file stats.\n",
    "    \"\"\"\n",
    "    aggregated = df.groupby([\"test_key\", \"gate\"]).agg(\n",
    "        fidelities_list=(\"fidelity\", list),\n",
    "        avg_fidelity=(\"fidelity\", \"mean\"),\n",
    "        error=(\"fidelity\", lambda x: np.mean(np.log(1 - x))),\n",
    "        avg_infidelity=(\"fidelity\", lambda x: np.mean(1 - x)),\n",
    "        qfim_eigvals_GHZ=(\"qfim_eigvals_GHZ\", \"first\"),\n",
    "        qfim_full_GHZ=(\"qfim_full_GHZ\", \"first\"),\n",
    "        entropy_GHZ=(\"entropy_GHZ\", \"first\"),\n",
    "        qfim_eigvals_basis=(\"qfim_eigvals_basis\", \"first\"),\n",
    "        qfim_full_basis=(\"qfim_full_basis\", \"first\"),\n",
    "        entropy_basis=(\"entropy_basis\", \"first\"),\n",
    "        dqfim_eigvals_train=(\"dqfim_eigvals_train\", \"first\"),\n",
    "        dqfim_train=(\"dqfim_train\", \"first\"),\n",
    "        dqfim_eigvals_file=(\"dqfim_eigvals_file\", \"first\"),\n",
    "        dqfim_full_file=(\"dqfim_full_file\", \"first\"),\n",
    "        target_dqfim_eigvals=(\"target_dqfim_eigvals\", \"first\"),\n",
    "        target_dqfim_full=(\"target_dqfim_full\", \"first\"),\n",
    "        target_dqfim_entropies=(\"target_dqfim_entropies\", \"first\"),\n",
    "        N_ctrl=(\"N_ctrl\", \"first\"),\n",
    "        N_R=(\"N_R\", \"first\"),\n",
    "        Trotter_Step=(\"Trotter_Step\", \"first\"),\n",
    "        num_sampled_states=(\"num_sampled_states\", \"first\"),\n",
    "        num_train = (\"N_train\",\"first\")\n",
    "    ).reset_index()\n",
    "    return aggregated\n",
    "\n",
    "def update_with_all_qfim_metrics(df, threshold=1e-10, spread_methods=(\"variance\", \"mad\"),\n",
    "                                 ddof=1, scale=\"normal\", gamma=0.1, n=1, V_theta=1.0):\n",
    "    \"\"\"\n",
    "    For each row, compute derived metrics for each QFIM variant:\n",
    "      - GHZ QFIM (from \"qfim_eigvals_GHZ\" and \"qfim_full_GHZ\")\n",
    "      - Basis QFIM (from \"qfim_eigvals\" and \"qfim_full\")\n",
    "      - Computed Training DQFIM (from \"dqfim_eigvals_train\" and \"dqfim_train\")\n",
    "      - Original file-stored DQFIM (from \"dqfim_eigvals_file\" and \"dqfim_full_file\")\n",
    "      - Computed Target DQFIM (from \"target_dqfim_eigvals\" and \"target_dqfim_full\")\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        ghz_stats = compute_single_draw_stats(\n",
    "            row[\"qfim_eigvals_GHZ\"],\n",
    "            row[\"qfim_full_GHZ\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=1,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        basis_stats = compute_single_draw_stats(\n",
    "            row[\"qfim_eigvals_basis\"],\n",
    "            row[\"qfim_full_basis\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=1,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        dqfim_stats_train = compute_single_draw_stats(\n",
    "            row[\"dqfim_eigvals_train\"],\n",
    "            row[\"dqfim_train\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=row['num_train'] if 'num_train' in row else 1,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        file_dqfim_stats = compute_single_draw_stats(\n",
    "            row[\"dqfim_eigvals_file\"],\n",
    "            row[\"dqfim_full_file\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=row['num_sampled_states'] if 'num_sampled_states' in row else 1,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        dqfim_stats_targ = compute_single_draw_stats(\n",
    "            row[\"target_dqfim_eigvals\"],\n",
    "            row[\"target_dqfim_full\"],\n",
    "            threshold=threshold,\n",
    "            spread_methods=spread_methods,\n",
    "            ddof=ddof,\n",
    "            scale=scale,\n",
    "            gamma=gamma,\n",
    "            n=row['num_train'] if 'num_train' in row else 1,\n",
    "            V_theta=V_theta,\n",
    "            n_ctrl=row[\"N_ctrl\"],\n",
    "            n_reserv=row[\"N_R\"],\n",
    "            trotter_step=row[\"Trotter_Step\"]\n",
    "        )\n",
    "        updated_row = row.to_dict()\n",
    "        updated_row.update({f\"GHZ_{k}\": v for k, v in ghz_stats.items()})\n",
    "        updated_row.update({f\"basis_{k}\": v for k, v in basis_stats.items()})\n",
    "        updated_row.update({f\"dqfim_{k}\": v for k, v in dqfim_stats_train.items()})\n",
    "        updated_row.update({f\"random_dample_dqfim_{k}\": v for k, v in file_dqfim_stats.items()})\n",
    "        updated_row.update({f\"tdqfim_{k}\": v for k, v in dqfim_stats_targ.items()})\n",
    "        new_rows.append(updated_row)\n",
    "    return pd.DataFrame(new_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated DataFrame shape: (240, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['test_key', 'gate', 'fidelities_list', 'avg_fidelity', 'error',\n",
       "       'avg_infidelity', 'qfim_eigvals_GHZ', 'qfim_full_GHZ', 'entropy_GHZ',\n",
       "       'qfim_eigvals_basis', 'qfim_full_basis', 'entropy_basis',\n",
       "       'dqfim_eigvals_train', 'dqfim_train', 'dqfim_eigvals_file',\n",
       "       'dqfim_full_file', 'target_dqfim_eigvals', 'target_dqfim_full',\n",
       "       'target_dqfim_entropies', 'N_ctrl', 'N_R', 'Trotter_Step',\n",
       "       'num_sampled_states', 'num_train'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['test_key', 'gate', 'fidelities_list', 'avg_fidelity', 'error',\n",
       "       'avg_infidelity', 'qfim_eigvals_GHZ', 'qfim_full_GHZ', 'entropy_GHZ',\n",
       "       'qfim_eigvals_basis',\n",
       "       ...\n",
       "       'tdqfim_trace_normalized_by_param_count', 'tdqfim_var_nonzero_log',\n",
       "       'tdqfim_ipr_deff_raw', 'tdqfim_ipr_deff_norm', 'tdqfim_abbas_deff_raw',\n",
       "       'tdqfim_abbas_deff_norm', 'tdqfim_d_eff',\n",
       "       'tdqfim_min_nonzero_eigenvalue', 'tdqfim_spread_metric_variance',\n",
       "       'tdqfim_spread_metric_mad'],\n",
       "      dtype='object', length=114)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_agg = aggregate_results(df_results)\n",
    "print(\"Aggregated DataFrame shape:\", df_agg.shape)\n",
    "display(df_agg.keys())\n",
    "\n",
    "\n",
    "df_final = update_with_all_qfim_metrics(df_agg)\n",
    "df_final.shape\n",
    "display(df_final.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise correlations vs. avg_fidelity (Pearson):\n",
      "GHZ_var_all_normalized_by_rank: r = -0.158, p = 0.0142\n",
      "GHZ_var_nonzero_normalized_by_rank: r = -0.154, p = 0.0171\n",
      "GHZ_trace_normalized_by_rank: r = -0.145, p = 0.0252\n",
      "\n",
      "Pairwise correlations vs. avg_fidelity (Spearman):\n",
      "GHZ_trace_normalized_by_rank: rho = -0.140, p = 0.0304\n"
     ]
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "import warnings\n",
    "from scipy.stats import ConstantInputWarning\n",
    "def analyze_correlations(df_merged, x_metric, metrics_of_interest, corr_threshold=0.2, p_threshold=0.05,\n",
    "                         print_all_pearson=False, print_all_spearman=False):\n",
    "    \"\"\"\n",
    "    Analyze pairwise correlations between a given x_metric and each metric in metrics_of_interest.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_merged : pd.DataFrame\n",
    "        The merged DataFrame containing the columns of interest.\n",
    "    x_metric : str\n",
    "        The column name for the independent variable (e.g., \"avg_fidelity\").\n",
    "    metrics_of_interest : list of str\n",
    "        List of column names whose correlations with x_metric will be computed.\n",
    "    corr_threshold : float, optional\n",
    "        Minimum absolute correlation coefficient to report (default 0.2).\n",
    "    p_threshold : float, optional\n",
    "        Maximum p-value threshold to report (default 0.05).\n",
    "    print_all_pearson : bool, optional\n",
    "        If True, print Pearson correlation results for every metric.\n",
    "    print_all_spearman : bool, optional\n",
    "        If True, print Spearman correlation results for every metric.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pearson_results : dict\n",
    "        Dictionary mapping each metric to its Pearson correlation coefficient and p-value.\n",
    "    spearman_results : dict\n",
    "        Dictionary mapping each metric to its Spearman correlation coefficient and p-value.\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with the columns of interest and drop rows with NaN values.\n",
    "    df_corr = df_merged[[x_metric] + metrics_of_interest].copy()\n",
    "    df_corr = df_corr.dropna(subset=[x_metric] + metrics_of_interest)\n",
    "    \n",
    "    # Ensure that each metric column contains float values.\n",
    "    for col in metrics_of_interest:\n",
    "        df_corr[col] = df_corr[col].apply(\n",
    "            lambda val: float(val.item()) if hasattr(val, \"item\") else float(val)\n",
    "        )\n",
    "    \n",
    "    # Compute Pearson correlations.\n",
    "    pearson_results = {}\n",
    "    for col in metrics_of_interest:\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"error\", category=ConstantInputWarning)\n",
    "                res_df = pg.corr(x=df_corr[x_metric], y=df_corr[col], method=\"pearson\")\n",
    "            r_val = res_df[\"r\"].iloc[0]\n",
    "            p_val = res_df[\"p-val\"].iloc[0]\n",
    "            pearson_results[col] = {\"pearson_r\": r_val, \"p_value\": p_val}\n",
    "        except ConstantInputWarning as cie:\n",
    "            print(f\"ConstantInputWarning for {col} (Pearson): {cie}\")\n",
    "            pearson_results[col] = {\"pearson_r\": None, \"p_value\": None}\n",
    "    \n",
    "    # Compute Spearman correlations.\n",
    "    spearman_results = {}\n",
    "    for col in metrics_of_interest:\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"error\", category=ConstantInputWarning)\n",
    "                sp_df = pg.corr(x=df_corr[x_metric], y=df_corr[col], method=\"spearman\")\n",
    "            rho_val = sp_df[\"r\"].iloc[0]\n",
    "            p_val = sp_df[\"p-val\"].iloc[0]\n",
    "            spearman_results[col] = {\"spearman_rho\": rho_val, \"p_value\": p_val}\n",
    "        except ConstantInputWarning as cie:\n",
    "            print(f\"ConstantInputWarning for {col} (Spearman): {cie}\")\n",
    "            spearman_results[col] = {\"spearman_rho\": None, \"p_value\": None}\n",
    "    \n",
    "    # Print out the Pearson correlations.\n",
    "    print(f\"\\nPairwise correlations vs. {x_metric} (Pearson):\")\n",
    "    for metric, vals in pearson_results.items():\n",
    "        if print_all_pearson or (vals[\"pearson_r\"] is not None and abs(vals[\"pearson_r\"]) > corr_threshold and vals[\"p_value\"] < p_threshold):\n",
    "            if vals[\"pearson_r\"] is not None:\n",
    "                print(f\"{metric}: r = {vals['pearson_r']:.3f}, p = {vals['p_value']:.3g}\")\n",
    "            else:\n",
    "                print(f\"{metric}: r = None, p = None\")\n",
    "    \n",
    "    # Print out the Spearman correlations.\n",
    "    print(f\"\\nPairwise correlations vs. {x_metric} (Spearman):\")\n",
    "    for metric, vals in spearman_results.items():\n",
    "        if print_all_spearman or (vals[\"spearman_rho\"] is not None and abs(vals[\"spearman_rho\"]) > corr_threshold and vals[\"p_value\"] < p_threshold):\n",
    "            if vals[\"spearman_rho\"] is not None:\n",
    "                print(f\"{metric}: rho = {vals['spearman_rho']:.3f}, p = {vals['p_value']:.3g}\")\n",
    "            else:\n",
    "                print(f\"{metric}: rho = None, p = None\")\n",
    "    \n",
    "    return pearson_results, spearman_results\n",
    "\n",
    "\n",
    "\n",
    "CORR_THRESHOLD = 0.1\n",
    "P_THRESHOLD = 0.05\n",
    "x_metric = \"avg_fidelity\"\n",
    "metrics_of_interest_ghz = [\n",
    "\n",
    "  \n",
    "       'GHZ_var_all_normalized_by_rank', 'GHZ_var_nonzero_normalized_by_rank',\n",
    "       'GHZ_trace_normalized_by_rank', \n",
    "       'GHZ_var_nonzero_log',  'GHZ_ipr_deff_norm', 'GHZ_d_eff',\n",
    "       'GHZ_spread_metric_variance', 'GHZ_spread_metric_mad',\n",
    "]\n",
    "# Now you can use these common thresholds in your calls:\n",
    "pearson_corrs_ghz, spearman_corrs_ghz = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_ghz,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD\n",
    ")\n",
    "\n",
    "# for metric, vals in pearson_corrss\n",
    "# on_r']:.3f}, p={vals['p_value']:.3g}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dQFIM metric keys: ['dqfim_eigvals_train', 'dqfim_train', 'dqfim_eigvals_file', 'dqfim_full_file', 'dqfim_draw_rank', 'dqfim_var_all_eigenvalues', 'dqfim_var_nonzero_eigenvalues', 'dqfim_trace_eigenvalues', 'dqfim_var_all_normalized_by_param_count', 'dqfim_var_all_normalized_by_rank', 'dqfim_var_nonzero_normalized_by_rank', 'dqfim_trace_normalized_by_rank', 'dqfim_trace_normalized_by_param_count', 'dqfim_var_nonzero_log', 'dqfim_ipr_deff_raw', 'dqfim_ipr_deff_norm', 'dqfim_abbas_deff_raw', 'dqfim_abbas_deff_norm', 'dqfim_d_eff', 'dqfim_min_nonzero_eigenvalue', 'dqfim_spread_metric_variance', 'dqfim_spread_metric_mad']\n"
     ]
    }
   ],
   "source": [
    "tdqfim_metrics = [key for key in df_final.keys() if key.startswith('dqfim_')]\n",
    "print(\"Target dQFIM metric keys:\", tdqfim_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated_row.update({f\"GHZ_{k}\": v for k, v in ghz_stats.items()})\n",
    "updated_row.update({f\"basis_{k}\": v for k, v in basis_stats.items()})\n",
    "updated_row.update({f\"dqfim_{k}\": v for k, v in dqfim_stats_train.items()})\n",
    "updated_row.update({f\"random_dample_dqfim_{k}\": v for k, v in file_dqfim_stats.items()})\n",
    "updated_row.update({f\"tdqfim_{k}\": v for k, v in dqfim_stats_targ.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise correlations vs. avg_fidelity (Pearson):\n",
      "tdqfim_var_all_eigenvalues: r = -0.125, p = 0.0536\n",
      "tdqfim_var_nonzero_eigenvalues: r = -0.117, p = 0.0692\n",
      "tdqfim_var_all_normalized_by_rank: r = -0.125, p = 0.0536\n",
      "tdqfim_var_nonzero_log: r = -0.112, p = 0.0827\n",
      "tdqfim_ipr_deff_raw: r = 0.068, p = 0.293\n",
      "tdqfim_abbas_deff_raw: r = -0.111, p = 0.0855\n",
      "tdqfim_abbas_deff_norm: r = 0.036, p = 0.579\n",
      "tdqfim_d_eff: r = 0.011, p = 0.866\n",
      "tdqfim_min_nonzero_eigenvalue: r = -0.114, p = 0.0771\n",
      "tdqfim_spread_metric_variance: r = -0.003, p = 0.966\n",
      "tdqfim_spread_metric_mad: r = 0.007, p = 0.911\n",
      "\n",
      "Pairwise correlations vs. avg_fidelity (Spearman):\n"
     ]
    }
   ],
   "source": [
    "x_metric = 'avg_fidelity'\n",
    "metrics_of_interest_targ_dqfim = [\n",
    "    \n",
    "       'tdqfim_var_all_eigenvalues',\n",
    "       'tdqfim_var_nonzero_eigenvalues',\n",
    "       'tdqfim_var_all_normalized_by_rank',\n",
    "       'tdqfim_var_nonzero_log',\n",
    "      'tdqfim_ipr_deff_raw', 'tdqfim_abbas_deff_raw', \n",
    "      'tdqfim_abbas_deff_norm', 'tdqfim_d_eff', 'tdqfim_min_nonzero_eigenvalue',\n",
    "      'tdqfim_spread_metric_variance', 'tdqfim_spread_metric_mad']\n",
    "pearson_corrs_targ, spearman_corrs_targ = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_targ_dqfim,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD, print_all_pearson=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise correlations vs. avg_fidelity (Pearson):\n",
      "dqfim_var_all_normalized_by_rank: r = -0.028, p = 0.671\n",
      "dqfim_var_nonzero_normalized_by_rank: r = -0.023, p = 0.722\n",
      "dqfim_trace_normalized_by_rank: r = -0.064, p = 0.327\n",
      "dqfim_var_nonzero_log: r = -0.095, p = 0.144\n",
      "dqfim_ipr_deff_norm: r = -0.052, p = 0.426\n",
      "dqfim_abbas_deff_raw: r = -0.074, p = 0.256\n",
      "dqfim_abbas_deff_norm: r = -0.062, p = 0.341\n",
      "dqfim_d_eff: r = -0.061, p = 0.346\n",
      "dqfim_spread_metric_variance: r = 0.062, p = 0.34\n",
      "dqfim_spread_metric_mad: r = 0.057, p = 0.38\n",
      "\n",
      "Pairwise correlations vs. avg_fidelity (Spearman):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_of_interest_dqfim = [\n",
    "\n",
    "     \n",
    "       'dqfim_var_all_normalized_by_rank',\n",
    "       'dqfim_var_nonzero_normalized_by_rank',\n",
    "       'dqfim_trace_normalized_by_rank',\n",
    "       'dqfim_var_nonzero_log',\n",
    "     'dqfim_ipr_deff_norm', 'dqfim_abbas_deff_raw',\n",
    "       'dqfim_abbas_deff_norm', 'dqfim_d_eff', 'dqfim_spread_metric_variance',\n",
    "       'dqfim_spread_metric_mad'\n",
    "]\n",
    "pearson_corrs_dqfim, spearman_corrs_dqfim = analyze_correlations(\n",
    "    df_final, x_metric, metrics_of_interest_dqfim,\n",
    "    corr_threshold=CORR_THRESHOLD, p_threshold=P_THRESHOLD, print_all_pearson=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch env (v3.11.10)",
   "language": "python",
   "name": "torch_en"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
